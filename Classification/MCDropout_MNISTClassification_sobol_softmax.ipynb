{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNetwork(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,proba_drop):\n",
    "        #assign module after called Module.__init__() \n",
    "        super(DropoutNetwork, self).__init__()\n",
    "        #basic parameters\n",
    "        self.input_dim=input_dim\n",
    "        self.output_dim=output_dim\n",
    "        self.proba_drop=proba_drop\n",
    "        self.hidden_dim=int((input_dim+output_dim)/2)\n",
    "        #construction of layers\n",
    "        self.layer1=nn.Linear(input_dim,self.hidden_dim)\n",
    "        self.layer2=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.layer3=nn.Linear(self.hidden_dim,self.output_dim)\n",
    "        #nn.init.normal_(self.layer1.weight)\n",
    "        #nn.init.normal_(self.layer2.weight)\n",
    "        #nn.init.normal_(self.layer3.weight)\n",
    "        \n",
    "    def forward(self, xdata,drop_inplace=False):\n",
    "        #inplace=False means input vector will not be changed\n",
    "        \n",
    "        input=xdata.view(-1,self.input_dim)\n",
    "        input=F.dropout(input,p=self.proba_drop,training=self.training,inplace=drop_inplace)\n",
    "          \n",
    "        #Pass layer 1\n",
    "        input=self.layer1(input)\n",
    "        #Remark: training = True may be change to self.training to compare pred MCdropout and pred naive.\n",
    "        input=F.dropout(input,p=self.proba_drop,training=self.training,inplace=drop_inplace)\n",
    "        input=F.relu(input)\n",
    "        #Pass layer 2\n",
    "        input=self.layer2(input)\n",
    "        input=F.dropout(input,p=self.proba_drop,training=self.training,inplace=drop_inplace)\n",
    "        input=F.relu(input)\n",
    "        #Pass layer 3 to get the output\n",
    "        output=self.layer3(input)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "learning_rate=1e-3 #may be changed to adaptive rate\n",
    "cuda=False\n",
    "device = torch.device(\"cpu\") #\"cuda\" if args.cuda else \n",
    "input_dim=28*28\n",
    "nclasses=10\n",
    "p_drop=0.5\n",
    "n_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DropoutNetwork(input_dim,nclasses,p_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=1, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs=50,display_step=10,criterion = nn.CrossEntropyLoss()):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss=0\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate/(epoch+1))\n",
    "        \n",
    "        for batch_idx, (xdata,ydata) in enumerate(train_loader):\n",
    "            #if we have cuda\n",
    "            xdata=xdata.to(device)\n",
    "            ydata=ydata.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output=model(xdata)\n",
    "            \n",
    "            loss=criterion(output,ydata)\n",
    "            epoch_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % display_step==0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)] lr: {}\\tLoss: {:.6f}'\n",
    "                  .format(epoch+1, batch_idx * len(ydata),\n",
    "                          len(train_loader.dataset),\n",
    "                          100. * batch_idx / len(train_loader),\n",
    "                          learning_rate/(epoch+1), loss.item()))\n",
    "        print(\"Finish {} epoch(s). Epoch loss: {}.\".format(epoch+1,epoch_loss/len(train_loader)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)] lr: 0.001\tLoss: 2.303787\n",
      "Train Epoch: 1 [2560/60000 (4%)] lr: 0.001\tLoss: 1.899794\n",
      "Train Epoch: 1 [5120/60000 (9%)] lr: 0.001\tLoss: 1.036946\n",
      "Train Epoch: 1 [7680/60000 (13%)] lr: 0.001\tLoss: 0.843606\n",
      "Train Epoch: 1 [10240/60000 (17%)] lr: 0.001\tLoss: 0.701042\n",
      "Train Epoch: 1 [12800/60000 (21%)] lr: 0.001\tLoss: 0.739169\n",
      "Train Epoch: 1 [15360/60000 (26%)] lr: 0.001\tLoss: 0.618845\n",
      "Train Epoch: 1 [17920/60000 (30%)] lr: 0.001\tLoss: 0.699894\n",
      "Train Epoch: 1 [20480/60000 (34%)] lr: 0.001\tLoss: 0.566059\n",
      "Train Epoch: 1 [23040/60000 (38%)] lr: 0.001\tLoss: 0.469241\n",
      "Train Epoch: 1 [25600/60000 (43%)] lr: 0.001\tLoss: 0.622306\n",
      "Train Epoch: 1 [28160/60000 (47%)] lr: 0.001\tLoss: 0.501206\n",
      "Train Epoch: 1 [30720/60000 (51%)] lr: 0.001\tLoss: 0.503086\n",
      "Train Epoch: 1 [33280/60000 (55%)] lr: 0.001\tLoss: 0.455076\n",
      "Train Epoch: 1 [35840/60000 (60%)] lr: 0.001\tLoss: 0.496750\n",
      "Train Epoch: 1 [38400/60000 (64%)] lr: 0.001\tLoss: 0.426650\n",
      "Train Epoch: 1 [40960/60000 (68%)] lr: 0.001\tLoss: 0.439807\n",
      "Train Epoch: 1 [43520/60000 (72%)] lr: 0.001\tLoss: 0.469195\n",
      "Train Epoch: 1 [46080/60000 (77%)] lr: 0.001\tLoss: 0.437004\n",
      "Train Epoch: 1 [48640/60000 (81%)] lr: 0.001\tLoss: 0.382467\n",
      "Train Epoch: 1 [51200/60000 (85%)] lr: 0.001\tLoss: 0.378397\n",
      "Train Epoch: 1 [53760/60000 (89%)] lr: 0.001\tLoss: 0.355848\n",
      "Train Epoch: 1 [56320/60000 (94%)] lr: 0.001\tLoss: 0.377911\n",
      "Train Epoch: 1 [58880/60000 (98%)] lr: 0.001\tLoss: 0.303021\n",
      "Finish 1 epoch(s). Epoch loss: 0.6507670964966429.\n",
      "Train Epoch: 2 [0/60000 (0%)] lr: 0.0005\tLoss: 0.426978\n",
      "Train Epoch: 2 [2560/60000 (4%)] lr: 0.0005\tLoss: 0.438580\n",
      "Train Epoch: 2 [5120/60000 (9%)] lr: 0.0005\tLoss: 0.324752\n",
      "Train Epoch: 2 [7680/60000 (13%)] lr: 0.0005\tLoss: 0.339764\n",
      "Train Epoch: 2 [10240/60000 (17%)] lr: 0.0005\tLoss: 0.303209\n",
      "Train Epoch: 2 [12800/60000 (21%)] lr: 0.0005\tLoss: 0.354955\n",
      "Train Epoch: 2 [15360/60000 (26%)] lr: 0.0005\tLoss: 0.319977\n",
      "Train Epoch: 2 [17920/60000 (30%)] lr: 0.0005\tLoss: 0.346154\n",
      "Train Epoch: 2 [20480/60000 (34%)] lr: 0.0005\tLoss: 0.364587\n",
      "Train Epoch: 2 [23040/60000 (38%)] lr: 0.0005\tLoss: 0.260792\n",
      "Train Epoch: 2 [25600/60000 (43%)] lr: 0.0005\tLoss: 0.311672\n",
      "Train Epoch: 2 [28160/60000 (47%)] lr: 0.0005\tLoss: 0.340542\n",
      "Train Epoch: 2 [30720/60000 (51%)] lr: 0.0005\tLoss: 0.275826\n",
      "Train Epoch: 2 [33280/60000 (55%)] lr: 0.0005\tLoss: 0.315148\n",
      "Train Epoch: 2 [35840/60000 (60%)] lr: 0.0005\tLoss: 0.267682\n",
      "Train Epoch: 2 [38400/60000 (64%)] lr: 0.0005\tLoss: 0.315473\n",
      "Train Epoch: 2 [40960/60000 (68%)] lr: 0.0005\tLoss: 0.303300\n",
      "Train Epoch: 2 [43520/60000 (72%)] lr: 0.0005\tLoss: 0.328201\n",
      "Train Epoch: 2 [46080/60000 (77%)] lr: 0.0005\tLoss: 0.253628\n",
      "Train Epoch: 2 [48640/60000 (81%)] lr: 0.0005\tLoss: 0.406233\n",
      "Train Epoch: 2 [51200/60000 (85%)] lr: 0.0005\tLoss: 0.374169\n",
      "Train Epoch: 2 [53760/60000 (89%)] lr: 0.0005\tLoss: 0.286313\n",
      "Train Epoch: 2 [56320/60000 (94%)] lr: 0.0005\tLoss: 0.282952\n",
      "Train Epoch: 2 [58880/60000 (98%)] lr: 0.0005\tLoss: 0.314930\n",
      "Finish 2 epoch(s). Epoch loss: 0.3353523996282131.\n",
      "Train Epoch: 3 [0/60000 (0%)] lr: 0.0003333333333333333\tLoss: 0.279769\n",
      "Train Epoch: 3 [2560/60000 (4%)] lr: 0.0003333333333333333\tLoss: 0.274895\n",
      "Train Epoch: 3 [5120/60000 (9%)] lr: 0.0003333333333333333\tLoss: 0.218469\n",
      "Train Epoch: 3 [7680/60000 (13%)] lr: 0.0003333333333333333\tLoss: 0.279229\n",
      "Train Epoch: 3 [10240/60000 (17%)] lr: 0.0003333333333333333\tLoss: 0.251722\n",
      "Train Epoch: 3 [12800/60000 (21%)] lr: 0.0003333333333333333\tLoss: 0.232052\n",
      "Train Epoch: 3 [15360/60000 (26%)] lr: 0.0003333333333333333\tLoss: 0.330673\n",
      "Train Epoch: 3 [17920/60000 (30%)] lr: 0.0003333333333333333\tLoss: 0.279034\n",
      "Train Epoch: 3 [20480/60000 (34%)] lr: 0.0003333333333333333\tLoss: 0.271662\n",
      "Train Epoch: 3 [23040/60000 (38%)] lr: 0.0003333333333333333\tLoss: 0.267353\n",
      "Train Epoch: 3 [25600/60000 (43%)] lr: 0.0003333333333333333\tLoss: 0.229049\n",
      "Train Epoch: 3 [28160/60000 (47%)] lr: 0.0003333333333333333\tLoss: 0.230379\n",
      "Train Epoch: 3 [30720/60000 (51%)] lr: 0.0003333333333333333\tLoss: 0.314309\n",
      "Train Epoch: 3 [33280/60000 (55%)] lr: 0.0003333333333333333\tLoss: 0.275602\n",
      "Train Epoch: 3 [35840/60000 (60%)] lr: 0.0003333333333333333\tLoss: 0.299119\n",
      "Train Epoch: 3 [38400/60000 (64%)] lr: 0.0003333333333333333\tLoss: 0.290578\n",
      "Train Epoch: 3 [40960/60000 (68%)] lr: 0.0003333333333333333\tLoss: 0.265964\n",
      "Train Epoch: 3 [43520/60000 (72%)] lr: 0.0003333333333333333\tLoss: 0.194869\n",
      "Train Epoch: 3 [46080/60000 (77%)] lr: 0.0003333333333333333\tLoss: 0.391496\n",
      "Train Epoch: 3 [48640/60000 (81%)] lr: 0.0003333333333333333\tLoss: 0.318649\n",
      "Train Epoch: 3 [51200/60000 (85%)] lr: 0.0003333333333333333\tLoss: 0.242363\n",
      "Train Epoch: 3 [53760/60000 (89%)] lr: 0.0003333333333333333\tLoss: 0.293226\n",
      "Train Epoch: 3 [56320/60000 (94%)] lr: 0.0003333333333333333\tLoss: 0.267545\n",
      "Train Epoch: 3 [58880/60000 (98%)] lr: 0.0003333333333333333\tLoss: 0.278069\n",
      "Finish 3 epoch(s). Epoch loss: 0.28255790231075695.\n",
      "Train Epoch: 4 [0/60000 (0%)] lr: 0.00025\tLoss: 0.238796\n",
      "Train Epoch: 4 [2560/60000 (4%)] lr: 0.00025\tLoss: 0.279268\n",
      "Train Epoch: 4 [5120/60000 (9%)] lr: 0.00025\tLoss: 0.237511\n",
      "Train Epoch: 4 [7680/60000 (13%)] lr: 0.00025\tLoss: 0.190715\n",
      "Train Epoch: 4 [10240/60000 (17%)] lr: 0.00025\tLoss: 0.272696\n",
      "Train Epoch: 4 [12800/60000 (21%)] lr: 0.00025\tLoss: 0.258941\n",
      "Train Epoch: 4 [15360/60000 (26%)] lr: 0.00025\tLoss: 0.249210\n",
      "Train Epoch: 4 [17920/60000 (30%)] lr: 0.00025\tLoss: 0.219153\n",
      "Train Epoch: 4 [20480/60000 (34%)] lr: 0.00025\tLoss: 0.239100\n",
      "Train Epoch: 4 [23040/60000 (38%)] lr: 0.00025\tLoss: 0.365052\n",
      "Train Epoch: 4 [25600/60000 (43%)] lr: 0.00025\tLoss: 0.312609\n",
      "Train Epoch: 4 [28160/60000 (47%)] lr: 0.00025\tLoss: 0.220173\n",
      "Train Epoch: 4 [30720/60000 (51%)] lr: 0.00025\tLoss: 0.157547\n",
      "Train Epoch: 4 [33280/60000 (55%)] lr: 0.00025\tLoss: 0.286167\n",
      "Train Epoch: 4 [35840/60000 (60%)] lr: 0.00025\tLoss: 0.243307\n",
      "Train Epoch: 4 [38400/60000 (64%)] lr: 0.00025\tLoss: 0.274767\n",
      "Train Epoch: 4 [40960/60000 (68%)] lr: 0.00025\tLoss: 0.241047\n",
      "Train Epoch: 4 [43520/60000 (72%)] lr: 0.00025\tLoss: 0.211153\n",
      "Train Epoch: 4 [46080/60000 (77%)] lr: 0.00025\tLoss: 0.291056\n",
      "Train Epoch: 4 [48640/60000 (81%)] lr: 0.00025\tLoss: 0.276573\n",
      "Train Epoch: 4 [51200/60000 (85%)] lr: 0.00025\tLoss: 0.261692\n",
      "Train Epoch: 4 [53760/60000 (89%)] lr: 0.00025\tLoss: 0.225209\n",
      "Train Epoch: 4 [56320/60000 (94%)] lr: 0.00025\tLoss: 0.238053\n",
      "Train Epoch: 4 [58880/60000 (98%)] lr: 0.00025\tLoss: 0.303751\n",
      "Finish 4 epoch(s). Epoch loss: 0.2626358656173057.\n",
      "Train Epoch: 5 [0/60000 (0%)] lr: 0.0002\tLoss: 0.270518\n",
      "Train Epoch: 5 [2560/60000 (4%)] lr: 0.0002\tLoss: 0.232479\n",
      "Train Epoch: 5 [5120/60000 (9%)] lr: 0.0002\tLoss: 0.253394\n",
      "Train Epoch: 5 [7680/60000 (13%)] lr: 0.0002\tLoss: 0.224159\n",
      "Train Epoch: 5 [10240/60000 (17%)] lr: 0.0002\tLoss: 0.158082\n",
      "Train Epoch: 5 [12800/60000 (21%)] lr: 0.0002\tLoss: 0.255651\n",
      "Train Epoch: 5 [15360/60000 (26%)] lr: 0.0002\tLoss: 0.202593\n",
      "Train Epoch: 5 [17920/60000 (30%)] lr: 0.0002\tLoss: 0.309315\n",
      "Train Epoch: 5 [20480/60000 (34%)] lr: 0.0002\tLoss: 0.181219\n",
      "Train Epoch: 5 [23040/60000 (38%)] lr: 0.0002\tLoss: 0.184171\n",
      "Train Epoch: 5 [25600/60000 (43%)] lr: 0.0002\tLoss: 0.309008\n",
      "Train Epoch: 5 [28160/60000 (47%)] lr: 0.0002\tLoss: 0.356781\n",
      "Train Epoch: 5 [30720/60000 (51%)] lr: 0.0002\tLoss: 0.310921\n",
      "Train Epoch: 5 [33280/60000 (55%)] lr: 0.0002\tLoss: 0.302219\n",
      "Train Epoch: 5 [35840/60000 (60%)] lr: 0.0002\tLoss: 0.250046\n",
      "Train Epoch: 5 [38400/60000 (64%)] lr: 0.0002\tLoss: 0.312187\n",
      "Train Epoch: 5 [40960/60000 (68%)] lr: 0.0002\tLoss: 0.250186\n",
      "Train Epoch: 5 [43520/60000 (72%)] lr: 0.0002\tLoss: 0.195358\n",
      "Train Epoch: 5 [46080/60000 (77%)] lr: 0.0002\tLoss: 0.230602\n",
      "Train Epoch: 5 [48640/60000 (81%)] lr: 0.0002\tLoss: 0.316027\n",
      "Train Epoch: 5 [51200/60000 (85%)] lr: 0.0002\tLoss: 0.186525\n",
      "Train Epoch: 5 [53760/60000 (89%)] lr: 0.0002\tLoss: 0.249708\n",
      "Train Epoch: 5 [56320/60000 (94%)] lr: 0.0002\tLoss: 0.303889\n",
      "Train Epoch: 5 [58880/60000 (98%)] lr: 0.0002\tLoss: 0.192636\n",
      "Finish 5 epoch(s). Epoch loss: 0.24037913179143947.\n",
      "Train Epoch: 6 [0/60000 (0%)] lr: 0.00016666666666666666\tLoss: 0.192198\n",
      "Train Epoch: 6 [2560/60000 (4%)] lr: 0.00016666666666666666\tLoss: 0.193582\n",
      "Train Epoch: 6 [5120/60000 (9%)] lr: 0.00016666666666666666\tLoss: 0.157428\n",
      "Train Epoch: 6 [7680/60000 (13%)] lr: 0.00016666666666666666\tLoss: 0.224348\n",
      "Train Epoch: 6 [10240/60000 (17%)] lr: 0.00016666666666666666\tLoss: 0.298578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12800/60000 (21%)] lr: 0.00016666666666666666\tLoss: 0.237910\n",
      "Train Epoch: 6 [15360/60000 (26%)] lr: 0.00016666666666666666\tLoss: 0.200762\n",
      "Train Epoch: 6 [17920/60000 (30%)] lr: 0.00016666666666666666\tLoss: 0.223635\n",
      "Train Epoch: 6 [20480/60000 (34%)] lr: 0.00016666666666666666\tLoss: 0.215047\n",
      "Train Epoch: 6 [23040/60000 (38%)] lr: 0.00016666666666666666\tLoss: 0.270272\n",
      "Train Epoch: 6 [25600/60000 (43%)] lr: 0.00016666666666666666\tLoss: 0.255577\n",
      "Train Epoch: 6 [28160/60000 (47%)] lr: 0.00016666666666666666\tLoss: 0.298723\n",
      "Train Epoch: 6 [30720/60000 (51%)] lr: 0.00016666666666666666\tLoss: 0.206436\n",
      "Train Epoch: 6 [33280/60000 (55%)] lr: 0.00016666666666666666\tLoss: 0.253493\n",
      "Train Epoch: 6 [35840/60000 (60%)] lr: 0.00016666666666666666\tLoss: 0.235781\n",
      "Train Epoch: 6 [38400/60000 (64%)] lr: 0.00016666666666666666\tLoss: 0.166703\n",
      "Train Epoch: 6 [40960/60000 (68%)] lr: 0.00016666666666666666\tLoss: 0.177651\n",
      "Train Epoch: 6 [43520/60000 (72%)] lr: 0.00016666666666666666\tLoss: 0.210721\n",
      "Train Epoch: 6 [46080/60000 (77%)] lr: 0.00016666666666666666\tLoss: 0.213650\n",
      "Train Epoch: 6 [48640/60000 (81%)] lr: 0.00016666666666666666\tLoss: 0.178383\n",
      "Train Epoch: 6 [51200/60000 (85%)] lr: 0.00016666666666666666\tLoss: 0.292025\n",
      "Train Epoch: 6 [53760/60000 (89%)] lr: 0.00016666666666666666\tLoss: 0.177691\n",
      "Train Epoch: 6 [56320/60000 (94%)] lr: 0.00016666666666666666\tLoss: 0.158978\n",
      "Train Epoch: 6 [58880/60000 (98%)] lr: 0.00016666666666666666\tLoss: 0.282045\n",
      "Finish 6 epoch(s). Epoch loss: 0.23310707528540428.\n",
      "Train Epoch: 7 [0/60000 (0%)] lr: 0.00014285714285714287\tLoss: 0.180342\n",
      "Train Epoch: 7 [2560/60000 (4%)] lr: 0.00014285714285714287\tLoss: 0.129691\n",
      "Train Epoch: 7 [5120/60000 (9%)] lr: 0.00014285714285714287\tLoss: 0.246053\n",
      "Train Epoch: 7 [7680/60000 (13%)] lr: 0.00014285714285714287\tLoss: 0.253685\n",
      "Train Epoch: 7 [10240/60000 (17%)] lr: 0.00014285714285714287\tLoss: 0.134032\n",
      "Train Epoch: 7 [12800/60000 (21%)] lr: 0.00014285714285714287\tLoss: 0.167338\n",
      "Train Epoch: 7 [15360/60000 (26%)] lr: 0.00014285714285714287\tLoss: 0.221380\n",
      "Train Epoch: 7 [17920/60000 (30%)] lr: 0.00014285714285714287\tLoss: 0.210159\n",
      "Train Epoch: 7 [20480/60000 (34%)] lr: 0.00014285714285714287\tLoss: 0.206940\n",
      "Train Epoch: 7 [23040/60000 (38%)] lr: 0.00014285714285714287\tLoss: 0.134978\n",
      "Train Epoch: 7 [25600/60000 (43%)] lr: 0.00014285714285714287\tLoss: 0.130455\n",
      "Train Epoch: 7 [28160/60000 (47%)] lr: 0.00014285714285714287\tLoss: 0.242010\n",
      "Train Epoch: 7 [30720/60000 (51%)] lr: 0.00014285714285714287\tLoss: 0.179657\n",
      "Train Epoch: 7 [33280/60000 (55%)] lr: 0.00014285714285714287\tLoss: 0.208515\n",
      "Train Epoch: 7 [35840/60000 (60%)] lr: 0.00014285714285714287\tLoss: 0.219826\n",
      "Train Epoch: 7 [38400/60000 (64%)] lr: 0.00014285714285714287\tLoss: 0.246170\n",
      "Train Epoch: 7 [40960/60000 (68%)] lr: 0.00014285714285714287\tLoss: 0.252276\n",
      "Train Epoch: 7 [43520/60000 (72%)] lr: 0.00014285714285714287\tLoss: 0.241599\n",
      "Train Epoch: 7 [46080/60000 (77%)] lr: 0.00014285714285714287\tLoss: 0.139530\n",
      "Train Epoch: 7 [48640/60000 (81%)] lr: 0.00014285714285714287\tLoss: 0.235417\n",
      "Train Epoch: 7 [51200/60000 (85%)] lr: 0.00014285714285714287\tLoss: 0.243631\n",
      "Train Epoch: 7 [53760/60000 (89%)] lr: 0.00014285714285714287\tLoss: 0.203389\n",
      "Train Epoch: 7 [56320/60000 (94%)] lr: 0.00014285714285714287\tLoss: 0.295998\n",
      "Train Epoch: 7 [58880/60000 (98%)] lr: 0.00014285714285714287\tLoss: 0.295072\n",
      "Finish 7 epoch(s). Epoch loss: 0.22494717966368857.\n",
      "Train Epoch: 8 [0/60000 (0%)] lr: 0.000125\tLoss: 0.203032\n",
      "Train Epoch: 8 [2560/60000 (4%)] lr: 0.000125\tLoss: 0.256777\n",
      "Train Epoch: 8 [5120/60000 (9%)] lr: 0.000125\tLoss: 0.238830\n",
      "Train Epoch: 8 [7680/60000 (13%)] lr: 0.000125\tLoss: 0.261173\n",
      "Train Epoch: 8 [10240/60000 (17%)] lr: 0.000125\tLoss: 0.269921\n",
      "Train Epoch: 8 [12800/60000 (21%)] lr: 0.000125\tLoss: 0.221864\n",
      "Train Epoch: 8 [15360/60000 (26%)] lr: 0.000125\tLoss: 0.192008\n",
      "Train Epoch: 8 [17920/60000 (30%)] lr: 0.000125\tLoss: 0.287876\n",
      "Train Epoch: 8 [20480/60000 (34%)] lr: 0.000125\tLoss: 0.221217\n",
      "Train Epoch: 8 [23040/60000 (38%)] lr: 0.000125\tLoss: 0.149738\n",
      "Train Epoch: 8 [25600/60000 (43%)] lr: 0.000125\tLoss: 0.233294\n",
      "Train Epoch: 8 [28160/60000 (47%)] lr: 0.000125\tLoss: 0.133090\n",
      "Train Epoch: 8 [30720/60000 (51%)] lr: 0.000125\tLoss: 0.202214\n",
      "Train Epoch: 8 [33280/60000 (55%)] lr: 0.000125\tLoss: 0.160387\n",
      "Train Epoch: 8 [35840/60000 (60%)] lr: 0.000125\tLoss: 0.265139\n",
      "Train Epoch: 8 [38400/60000 (64%)] lr: 0.000125\tLoss: 0.211341\n",
      "Train Epoch: 8 [40960/60000 (68%)] lr: 0.000125\tLoss: 0.248222\n",
      "Train Epoch: 8 [43520/60000 (72%)] lr: 0.000125\tLoss: 0.276280\n",
      "Train Epoch: 8 [46080/60000 (77%)] lr: 0.000125\tLoss: 0.160632\n",
      "Train Epoch: 8 [48640/60000 (81%)] lr: 0.000125\tLoss: 0.211488\n",
      "Train Epoch: 8 [51200/60000 (85%)] lr: 0.000125\tLoss: 0.253031\n",
      "Train Epoch: 8 [53760/60000 (89%)] lr: 0.000125\tLoss: 0.169618\n",
      "Train Epoch: 8 [56320/60000 (94%)] lr: 0.000125\tLoss: 0.190471\n",
      "Train Epoch: 8 [58880/60000 (98%)] lr: 0.000125\tLoss: 0.242438\n",
      "Finish 8 epoch(s). Epoch loss: 0.2173822655005658.\n",
      "Train Epoch: 9 [0/60000 (0%)] lr: 0.00011111111111111112\tLoss: 0.164959\n",
      "Train Epoch: 9 [2560/60000 (4%)] lr: 0.00011111111111111112\tLoss: 0.301113\n",
      "Train Epoch: 9 [5120/60000 (9%)] lr: 0.00011111111111111112\tLoss: 0.242035\n",
      "Train Epoch: 9 [7680/60000 (13%)] lr: 0.00011111111111111112\tLoss: 0.201826\n",
      "Train Epoch: 9 [10240/60000 (17%)] lr: 0.00011111111111111112\tLoss: 0.277943\n",
      "Train Epoch: 9 [12800/60000 (21%)] lr: 0.00011111111111111112\tLoss: 0.188725\n",
      "Train Epoch: 9 [15360/60000 (26%)] lr: 0.00011111111111111112\tLoss: 0.275000\n",
      "Train Epoch: 9 [17920/60000 (30%)] lr: 0.00011111111111111112\tLoss: 0.291679\n",
      "Train Epoch: 9 [20480/60000 (34%)] lr: 0.00011111111111111112\tLoss: 0.260384\n",
      "Train Epoch: 9 [23040/60000 (38%)] lr: 0.00011111111111111112\tLoss: 0.227966\n",
      "Train Epoch: 9 [25600/60000 (43%)] lr: 0.00011111111111111112\tLoss: 0.133721\n",
      "Train Epoch: 9 [28160/60000 (47%)] lr: 0.00011111111111111112\tLoss: 0.193754\n",
      "Train Epoch: 9 [30720/60000 (51%)] lr: 0.00011111111111111112\tLoss: 0.195034\n",
      "Train Epoch: 9 [33280/60000 (55%)] lr: 0.00011111111111111112\tLoss: 0.274565\n",
      "Train Epoch: 9 [35840/60000 (60%)] lr: 0.00011111111111111112\tLoss: 0.151861\n",
      "Train Epoch: 9 [38400/60000 (64%)] lr: 0.00011111111111111112\tLoss: 0.223639\n",
      "Train Epoch: 9 [40960/60000 (68%)] lr: 0.00011111111111111112\tLoss: 0.218744\n",
      "Train Epoch: 9 [43520/60000 (72%)] lr: 0.00011111111111111112\tLoss: 0.275460\n",
      "Train Epoch: 9 [46080/60000 (77%)] lr: 0.00011111111111111112\tLoss: 0.172246\n",
      "Train Epoch: 9 [48640/60000 (81%)] lr: 0.00011111111111111112\tLoss: 0.247053\n",
      "Train Epoch: 9 [51200/60000 (85%)] lr: 0.00011111111111111112\tLoss: 0.224502\n",
      "Train Epoch: 9 [53760/60000 (89%)] lr: 0.00011111111111111112\tLoss: 0.212915\n",
      "Train Epoch: 9 [56320/60000 (94%)] lr: 0.00011111111111111112\tLoss: 0.186865\n",
      "Train Epoch: 9 [58880/60000 (98%)] lr: 0.00011111111111111112\tLoss: 0.147741\n",
      "Finish 9 epoch(s). Epoch loss: 0.2112475799119219.\n",
      "Train Epoch: 10 [0/60000 (0%)] lr: 0.0001\tLoss: 0.183623\n",
      "Train Epoch: 10 [2560/60000 (4%)] lr: 0.0001\tLoss: 0.149556\n",
      "Train Epoch: 10 [5120/60000 (9%)] lr: 0.0001\tLoss: 0.198448\n",
      "Train Epoch: 10 [7680/60000 (13%)] lr: 0.0001\tLoss: 0.186118\n",
      "Train Epoch: 10 [10240/60000 (17%)] lr: 0.0001\tLoss: 0.227898\n",
      "Train Epoch: 10 [12800/60000 (21%)] lr: 0.0001\tLoss: 0.283804\n",
      "Train Epoch: 10 [15360/60000 (26%)] lr: 0.0001\tLoss: 0.181840\n",
      "Train Epoch: 10 [17920/60000 (30%)] lr: 0.0001\tLoss: 0.206634\n",
      "Train Epoch: 10 [20480/60000 (34%)] lr: 0.0001\tLoss: 0.248787\n",
      "Train Epoch: 10 [23040/60000 (38%)] lr: 0.0001\tLoss: 0.167789\n",
      "Train Epoch: 10 [25600/60000 (43%)] lr: 0.0001\tLoss: 0.150764\n",
      "Train Epoch: 10 [28160/60000 (47%)] lr: 0.0001\tLoss: 0.190229\n",
      "Train Epoch: 10 [30720/60000 (51%)] lr: 0.0001\tLoss: 0.206218\n",
      "Train Epoch: 10 [33280/60000 (55%)] lr: 0.0001\tLoss: 0.279430\n",
      "Train Epoch: 10 [35840/60000 (60%)] lr: 0.0001\tLoss: 0.202586\n",
      "Train Epoch: 10 [38400/60000 (64%)] lr: 0.0001\tLoss: 0.232876\n",
      "Train Epoch: 10 [40960/60000 (68%)] lr: 0.0001\tLoss: 0.131546\n",
      "Train Epoch: 10 [43520/60000 (72%)] lr: 0.0001\tLoss: 0.214113\n",
      "Train Epoch: 10 [46080/60000 (77%)] lr: 0.0001\tLoss: 0.241495\n",
      "Train Epoch: 10 [48640/60000 (81%)] lr: 0.0001\tLoss: 0.259954\n",
      "Train Epoch: 10 [51200/60000 (85%)] lr: 0.0001\tLoss: 0.274718\n",
      "Train Epoch: 10 [53760/60000 (89%)] lr: 0.0001\tLoss: 0.190549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [56320/60000 (94%)] lr: 0.0001\tLoss: 0.196260\n",
      "Train Epoch: 10 [58880/60000 (98%)] lr: 0.0001\tLoss: 0.143535\n",
      "Finish 10 epoch(s). Epoch loss: 0.20880602100428114.\n",
      "Train Epoch: 11 [0/60000 (0%)] lr: 9.090909090909092e-05\tLoss: 0.195491\n",
      "Train Epoch: 11 [2560/60000 (4%)] lr: 9.090909090909092e-05\tLoss: 0.184680\n",
      "Train Epoch: 11 [5120/60000 (9%)] lr: 9.090909090909092e-05\tLoss: 0.243679\n",
      "Train Epoch: 11 [7680/60000 (13%)] lr: 9.090909090909092e-05\tLoss: 0.200317\n",
      "Train Epoch: 11 [10240/60000 (17%)] lr: 9.090909090909092e-05\tLoss: 0.289696\n",
      "Train Epoch: 11 [12800/60000 (21%)] lr: 9.090909090909092e-05\tLoss: 0.156441\n",
      "Train Epoch: 11 [15360/60000 (26%)] lr: 9.090909090909092e-05\tLoss: 0.160888\n",
      "Train Epoch: 11 [17920/60000 (30%)] lr: 9.090909090909092e-05\tLoss: 0.238549\n",
      "Train Epoch: 11 [20480/60000 (34%)] lr: 9.090909090909092e-05\tLoss: 0.154718\n",
      "Train Epoch: 11 [23040/60000 (38%)] lr: 9.090909090909092e-05\tLoss: 0.296370\n",
      "Train Epoch: 11 [25600/60000 (43%)] lr: 9.090909090909092e-05\tLoss: 0.160513\n",
      "Train Epoch: 11 [28160/60000 (47%)] lr: 9.090909090909092e-05\tLoss: 0.151065\n",
      "Train Epoch: 11 [30720/60000 (51%)] lr: 9.090909090909092e-05\tLoss: 0.212874\n",
      "Train Epoch: 11 [33280/60000 (55%)] lr: 9.090909090909092e-05\tLoss: 0.256892\n",
      "Train Epoch: 11 [35840/60000 (60%)] lr: 9.090909090909092e-05\tLoss: 0.238166\n",
      "Train Epoch: 11 [38400/60000 (64%)] lr: 9.090909090909092e-05\tLoss: 0.189243\n",
      "Train Epoch: 11 [40960/60000 (68%)] lr: 9.090909090909092e-05\tLoss: 0.180167\n",
      "Train Epoch: 11 [43520/60000 (72%)] lr: 9.090909090909092e-05\tLoss: 0.228789\n",
      "Train Epoch: 11 [46080/60000 (77%)] lr: 9.090909090909092e-05\tLoss: 0.273156\n",
      "Train Epoch: 11 [48640/60000 (81%)] lr: 9.090909090909092e-05\tLoss: 0.160514\n",
      "Train Epoch: 11 [51200/60000 (85%)] lr: 9.090909090909092e-05\tLoss: 0.177200\n",
      "Train Epoch: 11 [53760/60000 (89%)] lr: 9.090909090909092e-05\tLoss: 0.207155\n",
      "Train Epoch: 11 [56320/60000 (94%)] lr: 9.090909090909092e-05\tLoss: 0.216531\n",
      "Train Epoch: 11 [58880/60000 (98%)] lr: 9.090909090909092e-05\tLoss: 0.211007\n",
      "Finish 11 epoch(s). Epoch loss: 0.20240831555838282.\n",
      "Train Epoch: 12 [0/60000 (0%)] lr: 8.333333333333333e-05\tLoss: 0.164661\n",
      "Train Epoch: 12 [2560/60000 (4%)] lr: 8.333333333333333e-05\tLoss: 0.258282\n",
      "Train Epoch: 12 [5120/60000 (9%)] lr: 8.333333333333333e-05\tLoss: 0.169399\n",
      "Train Epoch: 12 [7680/60000 (13%)] lr: 8.333333333333333e-05\tLoss: 0.208767\n",
      "Train Epoch: 12 [10240/60000 (17%)] lr: 8.333333333333333e-05\tLoss: 0.288669\n",
      "Train Epoch: 12 [12800/60000 (21%)] lr: 8.333333333333333e-05\tLoss: 0.195136\n",
      "Train Epoch: 12 [15360/60000 (26%)] lr: 8.333333333333333e-05\tLoss: 0.162655\n",
      "Train Epoch: 12 [17920/60000 (30%)] lr: 8.333333333333333e-05\tLoss: 0.194710\n",
      "Train Epoch: 12 [20480/60000 (34%)] lr: 8.333333333333333e-05\tLoss: 0.192075\n",
      "Train Epoch: 12 [23040/60000 (38%)] lr: 8.333333333333333e-05\tLoss: 0.241990\n",
      "Train Epoch: 12 [25600/60000 (43%)] lr: 8.333333333333333e-05\tLoss: 0.165867\n",
      "Train Epoch: 12 [28160/60000 (47%)] lr: 8.333333333333333e-05\tLoss: 0.179197\n",
      "Train Epoch: 12 [30720/60000 (51%)] lr: 8.333333333333333e-05\tLoss: 0.209953\n",
      "Train Epoch: 12 [33280/60000 (55%)] lr: 8.333333333333333e-05\tLoss: 0.207034\n",
      "Train Epoch: 12 [35840/60000 (60%)] lr: 8.333333333333333e-05\tLoss: 0.240559\n",
      "Train Epoch: 12 [38400/60000 (64%)] lr: 8.333333333333333e-05\tLoss: 0.180087\n",
      "Train Epoch: 12 [40960/60000 (68%)] lr: 8.333333333333333e-05\tLoss: 0.284189\n",
      "Train Epoch: 12 [43520/60000 (72%)] lr: 8.333333333333333e-05\tLoss: 0.290421\n",
      "Train Epoch: 12 [46080/60000 (77%)] lr: 8.333333333333333e-05\tLoss: 0.173725\n",
      "Train Epoch: 12 [48640/60000 (81%)] lr: 8.333333333333333e-05\tLoss: 0.146487\n",
      "Train Epoch: 12 [51200/60000 (85%)] lr: 8.333333333333333e-05\tLoss: 0.146788\n",
      "Train Epoch: 12 [53760/60000 (89%)] lr: 8.333333333333333e-05\tLoss: 0.216451\n",
      "Train Epoch: 12 [56320/60000 (94%)] lr: 8.333333333333333e-05\tLoss: 0.270831\n",
      "Train Epoch: 12 [58880/60000 (98%)] lr: 8.333333333333333e-05\tLoss: 0.212417\n",
      "Finish 12 epoch(s). Epoch loss: 0.2015342725084183.\n",
      "Train Epoch: 13 [0/60000 (0%)] lr: 7.692307692307693e-05\tLoss: 0.230589\n",
      "Train Epoch: 13 [2560/60000 (4%)] lr: 7.692307692307693e-05\tLoss: 0.182133\n",
      "Train Epoch: 13 [5120/60000 (9%)] lr: 7.692307692307693e-05\tLoss: 0.195574\n",
      "Train Epoch: 13 [7680/60000 (13%)] lr: 7.692307692307693e-05\tLoss: 0.140743\n",
      "Train Epoch: 13 [10240/60000 (17%)] lr: 7.692307692307693e-05\tLoss: 0.186076\n",
      "Train Epoch: 13 [12800/60000 (21%)] lr: 7.692307692307693e-05\tLoss: 0.177135\n",
      "Train Epoch: 13 [15360/60000 (26%)] lr: 7.692307692307693e-05\tLoss: 0.182257\n",
      "Train Epoch: 13 [17920/60000 (30%)] lr: 7.692307692307693e-05\tLoss: 0.267483\n",
      "Train Epoch: 13 [20480/60000 (34%)] lr: 7.692307692307693e-05\tLoss: 0.214811\n",
      "Train Epoch: 13 [23040/60000 (38%)] lr: 7.692307692307693e-05\tLoss: 0.211667\n",
      "Train Epoch: 13 [25600/60000 (43%)] lr: 7.692307692307693e-05\tLoss: 0.096302\n",
      "Train Epoch: 13 [28160/60000 (47%)] lr: 7.692307692307693e-05\tLoss: 0.164449\n",
      "Train Epoch: 13 [30720/60000 (51%)] lr: 7.692307692307693e-05\tLoss: 0.148774\n",
      "Train Epoch: 13 [33280/60000 (55%)] lr: 7.692307692307693e-05\tLoss: 0.157083\n",
      "Train Epoch: 13 [35840/60000 (60%)] lr: 7.692307692307693e-05\tLoss: 0.150659\n",
      "Train Epoch: 13 [38400/60000 (64%)] lr: 7.692307692307693e-05\tLoss: 0.130276\n",
      "Train Epoch: 13 [40960/60000 (68%)] lr: 7.692307692307693e-05\tLoss: 0.226906\n",
      "Train Epoch: 13 [43520/60000 (72%)] lr: 7.692307692307693e-05\tLoss: 0.204668\n",
      "Train Epoch: 13 [46080/60000 (77%)] lr: 7.692307692307693e-05\tLoss: 0.304398\n",
      "Train Epoch: 13 [48640/60000 (81%)] lr: 7.692307692307693e-05\tLoss: 0.185900\n",
      "Train Epoch: 13 [51200/60000 (85%)] lr: 7.692307692307693e-05\tLoss: 0.210056\n",
      "Train Epoch: 13 [53760/60000 (89%)] lr: 7.692307692307693e-05\tLoss: 0.146845\n",
      "Train Epoch: 13 [56320/60000 (94%)] lr: 7.692307692307693e-05\tLoss: 0.274494\n",
      "Train Epoch: 13 [58880/60000 (98%)] lr: 7.692307692307693e-05\tLoss: 0.162956\n",
      "Finish 13 epoch(s). Epoch loss: 0.19670244097075565.\n",
      "Train Epoch: 14 [0/60000 (0%)] lr: 7.142857142857143e-05\tLoss: 0.241484\n",
      "Train Epoch: 14 [2560/60000 (4%)] lr: 7.142857142857143e-05\tLoss: 0.130828\n",
      "Train Epoch: 14 [5120/60000 (9%)] lr: 7.142857142857143e-05\tLoss: 0.141186\n",
      "Train Epoch: 14 [7680/60000 (13%)] lr: 7.142857142857143e-05\tLoss: 0.185442\n",
      "Train Epoch: 14 [10240/60000 (17%)] lr: 7.142857142857143e-05\tLoss: 0.144281\n",
      "Train Epoch: 14 [12800/60000 (21%)] lr: 7.142857142857143e-05\tLoss: 0.144985\n",
      "Train Epoch: 14 [15360/60000 (26%)] lr: 7.142857142857143e-05\tLoss: 0.180706\n",
      "Train Epoch: 14 [17920/60000 (30%)] lr: 7.142857142857143e-05\tLoss: 0.198096\n",
      "Train Epoch: 14 [20480/60000 (34%)] lr: 7.142857142857143e-05\tLoss: 0.130987\n",
      "Train Epoch: 14 [23040/60000 (38%)] lr: 7.142857142857143e-05\tLoss: 0.176551\n",
      "Train Epoch: 14 [25600/60000 (43%)] lr: 7.142857142857143e-05\tLoss: 0.185484\n",
      "Train Epoch: 14 [28160/60000 (47%)] lr: 7.142857142857143e-05\tLoss: 0.219701\n",
      "Train Epoch: 14 [30720/60000 (51%)] lr: 7.142857142857143e-05\tLoss: 0.206177\n",
      "Train Epoch: 14 [33280/60000 (55%)] lr: 7.142857142857143e-05\tLoss: 0.182356\n",
      "Train Epoch: 14 [35840/60000 (60%)] lr: 7.142857142857143e-05\tLoss: 0.228773\n",
      "Train Epoch: 14 [38400/60000 (64%)] lr: 7.142857142857143e-05\tLoss: 0.162914\n",
      "Train Epoch: 14 [40960/60000 (68%)] lr: 7.142857142857143e-05\tLoss: 0.218267\n",
      "Train Epoch: 14 [43520/60000 (72%)] lr: 7.142857142857143e-05\tLoss: 0.202816\n",
      "Train Epoch: 14 [46080/60000 (77%)] lr: 7.142857142857143e-05\tLoss: 0.153375\n",
      "Train Epoch: 14 [48640/60000 (81%)] lr: 7.142857142857143e-05\tLoss: 0.143156\n",
      "Train Epoch: 14 [51200/60000 (85%)] lr: 7.142857142857143e-05\tLoss: 0.147469\n",
      "Train Epoch: 14 [53760/60000 (89%)] lr: 7.142857142857143e-05\tLoss: 0.145392\n",
      "Train Epoch: 14 [56320/60000 (94%)] lr: 7.142857142857143e-05\tLoss: 0.223193\n",
      "Train Epoch: 14 [58880/60000 (98%)] lr: 7.142857142857143e-05\tLoss: 0.136066\n",
      "Finish 14 epoch(s). Epoch loss: 0.19993880292836655.\n",
      "Train Epoch: 15 [0/60000 (0%)] lr: 6.666666666666667e-05\tLoss: 0.145663\n",
      "Train Epoch: 15 [2560/60000 (4%)] lr: 6.666666666666667e-05\tLoss: 0.295687\n",
      "Train Epoch: 15 [5120/60000 (9%)] lr: 6.666666666666667e-05\tLoss: 0.214652\n",
      "Train Epoch: 15 [7680/60000 (13%)] lr: 6.666666666666667e-05\tLoss: 0.189219\n",
      "Train Epoch: 15 [10240/60000 (17%)] lr: 6.666666666666667e-05\tLoss: 0.212881\n",
      "Train Epoch: 15 [12800/60000 (21%)] lr: 6.666666666666667e-05\tLoss: 0.219718\n",
      "Train Epoch: 15 [15360/60000 (26%)] lr: 6.666666666666667e-05\tLoss: 0.147860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [17920/60000 (30%)] lr: 6.666666666666667e-05\tLoss: 0.285183\n",
      "Train Epoch: 15 [20480/60000 (34%)] lr: 6.666666666666667e-05\tLoss: 0.175059\n",
      "Train Epoch: 15 [23040/60000 (38%)] lr: 6.666666666666667e-05\tLoss: 0.195563\n",
      "Train Epoch: 15 [25600/60000 (43%)] lr: 6.666666666666667e-05\tLoss: 0.147180\n",
      "Train Epoch: 15 [28160/60000 (47%)] lr: 6.666666666666667e-05\tLoss: 0.175025\n",
      "Train Epoch: 15 [30720/60000 (51%)] lr: 6.666666666666667e-05\tLoss: 0.194844\n",
      "Train Epoch: 15 [33280/60000 (55%)] lr: 6.666666666666667e-05\tLoss: 0.211051\n",
      "Train Epoch: 15 [35840/60000 (60%)] lr: 6.666666666666667e-05\tLoss: 0.191985\n",
      "Train Epoch: 15 [38400/60000 (64%)] lr: 6.666666666666667e-05\tLoss: 0.199691\n",
      "Train Epoch: 15 [40960/60000 (68%)] lr: 6.666666666666667e-05\tLoss: 0.156797\n",
      "Train Epoch: 15 [43520/60000 (72%)] lr: 6.666666666666667e-05\tLoss: 0.205101\n",
      "Train Epoch: 15 [46080/60000 (77%)] lr: 6.666666666666667e-05\tLoss: 0.165639\n",
      "Train Epoch: 15 [48640/60000 (81%)] lr: 6.666666666666667e-05\tLoss: 0.204617\n",
      "Train Epoch: 15 [51200/60000 (85%)] lr: 6.666666666666667e-05\tLoss: 0.224446\n",
      "Train Epoch: 15 [53760/60000 (89%)] lr: 6.666666666666667e-05\tLoss: 0.203402\n",
      "Train Epoch: 15 [56320/60000 (94%)] lr: 6.666666666666667e-05\tLoss: 0.152727\n",
      "Train Epoch: 15 [58880/60000 (98%)] lr: 6.666666666666667e-05\tLoss: 0.204895\n",
      "Finish 15 epoch(s). Epoch loss: 0.19363845989425132.\n",
      "Train Epoch: 16 [0/60000 (0%)] lr: 6.25e-05\tLoss: 0.211985\n",
      "Train Epoch: 16 [2560/60000 (4%)] lr: 6.25e-05\tLoss: 0.137935\n",
      "Train Epoch: 16 [5120/60000 (9%)] lr: 6.25e-05\tLoss: 0.139601\n",
      "Train Epoch: 16 [7680/60000 (13%)] lr: 6.25e-05\tLoss: 0.113099\n",
      "Train Epoch: 16 [10240/60000 (17%)] lr: 6.25e-05\tLoss: 0.258857\n",
      "Train Epoch: 16 [12800/60000 (21%)] lr: 6.25e-05\tLoss: 0.235836\n",
      "Train Epoch: 16 [15360/60000 (26%)] lr: 6.25e-05\tLoss: 0.145656\n",
      "Train Epoch: 16 [17920/60000 (30%)] lr: 6.25e-05\tLoss: 0.170848\n",
      "Train Epoch: 16 [20480/60000 (34%)] lr: 6.25e-05\tLoss: 0.227466\n",
      "Train Epoch: 16 [23040/60000 (38%)] lr: 6.25e-05\tLoss: 0.194431\n",
      "Train Epoch: 16 [25600/60000 (43%)] lr: 6.25e-05\tLoss: 0.213509\n",
      "Train Epoch: 16 [28160/60000 (47%)] lr: 6.25e-05\tLoss: 0.210829\n",
      "Train Epoch: 16 [30720/60000 (51%)] lr: 6.25e-05\tLoss: 0.144455\n",
      "Train Epoch: 16 [33280/60000 (55%)] lr: 6.25e-05\tLoss: 0.187206\n",
      "Train Epoch: 16 [35840/60000 (60%)] lr: 6.25e-05\tLoss: 0.199512\n",
      "Train Epoch: 16 [38400/60000 (64%)] lr: 6.25e-05\tLoss: 0.222049\n",
      "Train Epoch: 16 [40960/60000 (68%)] lr: 6.25e-05\tLoss: 0.323651\n",
      "Train Epoch: 16 [43520/60000 (72%)] lr: 6.25e-05\tLoss: 0.157745\n",
      "Train Epoch: 16 [46080/60000 (77%)] lr: 6.25e-05\tLoss: 0.206270\n",
      "Train Epoch: 16 [48640/60000 (81%)] lr: 6.25e-05\tLoss: 0.192724\n",
      "Train Epoch: 16 [51200/60000 (85%)] lr: 6.25e-05\tLoss: 0.157238\n",
      "Train Epoch: 16 [53760/60000 (89%)] lr: 6.25e-05\tLoss: 0.118188\n",
      "Train Epoch: 16 [56320/60000 (94%)] lr: 6.25e-05\tLoss: 0.197046\n",
      "Train Epoch: 16 [58880/60000 (98%)] lr: 6.25e-05\tLoss: 0.119173\n",
      "Finish 16 epoch(s). Epoch loss: 0.19057170250948438.\n",
      "Train Epoch: 17 [0/60000 (0%)] lr: 5.882352941176471e-05\tLoss: 0.190608\n",
      "Train Epoch: 17 [2560/60000 (4%)] lr: 5.882352941176471e-05\tLoss: 0.193474\n",
      "Train Epoch: 17 [5120/60000 (9%)] lr: 5.882352941176471e-05\tLoss: 0.189032\n",
      "Train Epoch: 17 [7680/60000 (13%)] lr: 5.882352941176471e-05\tLoss: 0.264690\n",
      "Train Epoch: 17 [10240/60000 (17%)] lr: 5.882352941176471e-05\tLoss: 0.203871\n",
      "Train Epoch: 17 [12800/60000 (21%)] lr: 5.882352941176471e-05\tLoss: 0.142085\n",
      "Train Epoch: 17 [15360/60000 (26%)] lr: 5.882352941176471e-05\tLoss: 0.207050\n",
      "Train Epoch: 17 [17920/60000 (30%)] lr: 5.882352941176471e-05\tLoss: 0.146326\n",
      "Train Epoch: 17 [20480/60000 (34%)] lr: 5.882352941176471e-05\tLoss: 0.262865\n",
      "Train Epoch: 17 [23040/60000 (38%)] lr: 5.882352941176471e-05\tLoss: 0.230555\n",
      "Train Epoch: 17 [25600/60000 (43%)] lr: 5.882352941176471e-05\tLoss: 0.217173\n",
      "Train Epoch: 17 [28160/60000 (47%)] lr: 5.882352941176471e-05\tLoss: 0.173485\n",
      "Train Epoch: 17 [30720/60000 (51%)] lr: 5.882352941176471e-05\tLoss: 0.263297\n",
      "Train Epoch: 17 [33280/60000 (55%)] lr: 5.882352941176471e-05\tLoss: 0.187271\n",
      "Train Epoch: 17 [35840/60000 (60%)] lr: 5.882352941176471e-05\tLoss: 0.159676\n",
      "Train Epoch: 17 [38400/60000 (64%)] lr: 5.882352941176471e-05\tLoss: 0.161804\n",
      "Train Epoch: 17 [40960/60000 (68%)] lr: 5.882352941176471e-05\tLoss: 0.215546\n",
      "Train Epoch: 17 [43520/60000 (72%)] lr: 5.882352941176471e-05\tLoss: 0.207579\n",
      "Train Epoch: 17 [46080/60000 (77%)] lr: 5.882352941176471e-05\tLoss: 0.250121\n",
      "Train Epoch: 17 [48640/60000 (81%)] lr: 5.882352941176471e-05\tLoss: 0.193339\n",
      "Train Epoch: 17 [51200/60000 (85%)] lr: 5.882352941176471e-05\tLoss: 0.192295\n",
      "Train Epoch: 17 [53760/60000 (89%)] lr: 5.882352941176471e-05\tLoss: 0.236389\n",
      "Train Epoch: 17 [56320/60000 (94%)] lr: 5.882352941176471e-05\tLoss: 0.199101\n",
      "Train Epoch: 17 [58880/60000 (98%)] lr: 5.882352941176471e-05\tLoss: 0.279471\n",
      "Finish 17 epoch(s). Epoch loss: 0.192492915785059.\n",
      "Train Epoch: 18 [0/60000 (0%)] lr: 5.555555555555556e-05\tLoss: 0.217266\n",
      "Train Epoch: 18 [2560/60000 (4%)] lr: 5.555555555555556e-05\tLoss: 0.125735\n",
      "Train Epoch: 18 [5120/60000 (9%)] lr: 5.555555555555556e-05\tLoss: 0.282376\n",
      "Train Epoch: 18 [7680/60000 (13%)] lr: 5.555555555555556e-05\tLoss: 0.211961\n",
      "Train Epoch: 18 [10240/60000 (17%)] lr: 5.555555555555556e-05\tLoss: 0.170489\n",
      "Train Epoch: 18 [12800/60000 (21%)] lr: 5.555555555555556e-05\tLoss: 0.230304\n",
      "Train Epoch: 18 [15360/60000 (26%)] lr: 5.555555555555556e-05\tLoss: 0.177306\n",
      "Train Epoch: 18 [17920/60000 (30%)] lr: 5.555555555555556e-05\tLoss: 0.194080\n",
      "Train Epoch: 18 [20480/60000 (34%)] lr: 5.555555555555556e-05\tLoss: 0.208539\n",
      "Train Epoch: 18 [23040/60000 (38%)] lr: 5.555555555555556e-05\tLoss: 0.254024\n",
      "Train Epoch: 18 [25600/60000 (43%)] lr: 5.555555555555556e-05\tLoss: 0.130716\n",
      "Train Epoch: 18 [28160/60000 (47%)] lr: 5.555555555555556e-05\tLoss: 0.148341\n",
      "Train Epoch: 18 [30720/60000 (51%)] lr: 5.555555555555556e-05\tLoss: 0.213132\n",
      "Train Epoch: 18 [33280/60000 (55%)] lr: 5.555555555555556e-05\tLoss: 0.211954\n",
      "Train Epoch: 18 [35840/60000 (60%)] lr: 5.555555555555556e-05\tLoss: 0.251109\n",
      "Train Epoch: 18 [38400/60000 (64%)] lr: 5.555555555555556e-05\tLoss: 0.231874\n",
      "Train Epoch: 18 [40960/60000 (68%)] lr: 5.555555555555556e-05\tLoss: 0.151072\n",
      "Train Epoch: 18 [43520/60000 (72%)] lr: 5.555555555555556e-05\tLoss: 0.220359\n",
      "Train Epoch: 18 [46080/60000 (77%)] lr: 5.555555555555556e-05\tLoss: 0.235077\n",
      "Train Epoch: 18 [48640/60000 (81%)] lr: 5.555555555555556e-05\tLoss: 0.182358\n",
      "Train Epoch: 18 [51200/60000 (85%)] lr: 5.555555555555556e-05\tLoss: 0.203970\n",
      "Train Epoch: 18 [53760/60000 (89%)] lr: 5.555555555555556e-05\tLoss: 0.158670\n",
      "Train Epoch: 18 [56320/60000 (94%)] lr: 5.555555555555556e-05\tLoss: 0.120257\n",
      "Train Epoch: 18 [58880/60000 (98%)] lr: 5.555555555555556e-05\tLoss: 0.190554\n",
      "Finish 18 epoch(s). Epoch loss: 0.18688997975055208.\n",
      "Train Epoch: 19 [0/60000 (0%)] lr: 5.2631578947368424e-05\tLoss: 0.202071\n",
      "Train Epoch: 19 [2560/60000 (4%)] lr: 5.2631578947368424e-05\tLoss: 0.181795\n",
      "Train Epoch: 19 [5120/60000 (9%)] lr: 5.2631578947368424e-05\tLoss: 0.191576\n",
      "Train Epoch: 19 [7680/60000 (13%)] lr: 5.2631578947368424e-05\tLoss: 0.118290\n",
      "Train Epoch: 19 [10240/60000 (17%)] lr: 5.2631578947368424e-05\tLoss: 0.251883\n",
      "Train Epoch: 19 [12800/60000 (21%)] lr: 5.2631578947368424e-05\tLoss: 0.209654\n",
      "Train Epoch: 19 [15360/60000 (26%)] lr: 5.2631578947368424e-05\tLoss: 0.210538\n",
      "Train Epoch: 19 [17920/60000 (30%)] lr: 5.2631578947368424e-05\tLoss: 0.133193\n",
      "Train Epoch: 19 [20480/60000 (34%)] lr: 5.2631578947368424e-05\tLoss: 0.104193\n",
      "Train Epoch: 19 [23040/60000 (38%)] lr: 5.2631578947368424e-05\tLoss: 0.117514\n",
      "Train Epoch: 19 [25600/60000 (43%)] lr: 5.2631578947368424e-05\tLoss: 0.171601\n",
      "Train Epoch: 19 [28160/60000 (47%)] lr: 5.2631578947368424e-05\tLoss: 0.178443\n",
      "Train Epoch: 19 [30720/60000 (51%)] lr: 5.2631578947368424e-05\tLoss: 0.217242\n",
      "Train Epoch: 19 [33280/60000 (55%)] lr: 5.2631578947368424e-05\tLoss: 0.131696\n",
      "Train Epoch: 19 [35840/60000 (60%)] lr: 5.2631578947368424e-05\tLoss: 0.168952\n",
      "Train Epoch: 19 [38400/60000 (64%)] lr: 5.2631578947368424e-05\tLoss: 0.215276\n",
      "Train Epoch: 19 [40960/60000 (68%)] lr: 5.2631578947368424e-05\tLoss: 0.196617\n",
      "Train Epoch: 19 [43520/60000 (72%)] lr: 5.2631578947368424e-05\tLoss: 0.213427\n",
      "Train Epoch: 19 [46080/60000 (77%)] lr: 5.2631578947368424e-05\tLoss: 0.217299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [48640/60000 (81%)] lr: 5.2631578947368424e-05\tLoss: 0.179967\n",
      "Train Epoch: 19 [51200/60000 (85%)] lr: 5.2631578947368424e-05\tLoss: 0.167302\n",
      "Train Epoch: 19 [53760/60000 (89%)] lr: 5.2631578947368424e-05\tLoss: 0.171770\n",
      "Train Epoch: 19 [56320/60000 (94%)] lr: 5.2631578947368424e-05\tLoss: 0.199555\n",
      "Train Epoch: 19 [58880/60000 (98%)] lr: 5.2631578947368424e-05\tLoss: 0.154495\n",
      "Finish 19 epoch(s). Epoch loss: 0.18834407342241166.\n",
      "Train Epoch: 20 [0/60000 (0%)] lr: 5e-05\tLoss: 0.132422\n",
      "Train Epoch: 20 [2560/60000 (4%)] lr: 5e-05\tLoss: 0.198349\n",
      "Train Epoch: 20 [5120/60000 (9%)] lr: 5e-05\tLoss: 0.190815\n",
      "Train Epoch: 20 [7680/60000 (13%)] lr: 5e-05\tLoss: 0.177949\n",
      "Train Epoch: 20 [10240/60000 (17%)] lr: 5e-05\tLoss: 0.181395\n",
      "Train Epoch: 20 [12800/60000 (21%)] lr: 5e-05\tLoss: 0.240568\n",
      "Train Epoch: 20 [15360/60000 (26%)] lr: 5e-05\tLoss: 0.169774\n",
      "Train Epoch: 20 [17920/60000 (30%)] lr: 5e-05\tLoss: 0.191460\n",
      "Train Epoch: 20 [20480/60000 (34%)] lr: 5e-05\tLoss: 0.200626\n",
      "Train Epoch: 20 [23040/60000 (38%)] lr: 5e-05\tLoss: 0.168803\n",
      "Train Epoch: 20 [25600/60000 (43%)] lr: 5e-05\tLoss: 0.169223\n",
      "Train Epoch: 20 [28160/60000 (47%)] lr: 5e-05\tLoss: 0.187842\n",
      "Train Epoch: 20 [30720/60000 (51%)] lr: 5e-05\tLoss: 0.220190\n",
      "Train Epoch: 20 [33280/60000 (55%)] lr: 5e-05\tLoss: 0.247102\n",
      "Train Epoch: 20 [35840/60000 (60%)] lr: 5e-05\tLoss: 0.198646\n",
      "Train Epoch: 20 [38400/60000 (64%)] lr: 5e-05\tLoss: 0.132536\n",
      "Train Epoch: 20 [40960/60000 (68%)] lr: 5e-05\tLoss: 0.244559\n",
      "Train Epoch: 20 [43520/60000 (72%)] lr: 5e-05\tLoss: 0.194132\n",
      "Train Epoch: 20 [46080/60000 (77%)] lr: 5e-05\tLoss: 0.251966\n",
      "Train Epoch: 20 [48640/60000 (81%)] lr: 5e-05\tLoss: 0.137506\n",
      "Train Epoch: 20 [51200/60000 (85%)] lr: 5e-05\tLoss: 0.149110\n",
      "Train Epoch: 20 [53760/60000 (89%)] lr: 5e-05\tLoss: 0.230388\n",
      "Train Epoch: 20 [56320/60000 (94%)] lr: 5e-05\tLoss: 0.171851\n",
      "Train Epoch: 20 [58880/60000 (98%)] lr: 5e-05\tLoss: 0.186896\n",
      "Finish 20 epoch(s). Epoch loss: 0.18685212591861156.\n",
      "Train Epoch: 21 [0/60000 (0%)] lr: 4.761904761904762e-05\tLoss: 0.143783\n",
      "Train Epoch: 21 [2560/60000 (4%)] lr: 4.761904761904762e-05\tLoss: 0.188135\n",
      "Train Epoch: 21 [5120/60000 (9%)] lr: 4.761904761904762e-05\tLoss: 0.171974\n",
      "Train Epoch: 21 [7680/60000 (13%)] lr: 4.761904761904762e-05\tLoss: 0.237983\n",
      "Train Epoch: 21 [10240/60000 (17%)] lr: 4.761904761904762e-05\tLoss: 0.199278\n",
      "Train Epoch: 21 [12800/60000 (21%)] lr: 4.761904761904762e-05\tLoss: 0.140703\n",
      "Train Epoch: 21 [15360/60000 (26%)] lr: 4.761904761904762e-05\tLoss: 0.108875\n",
      "Train Epoch: 21 [17920/60000 (30%)] lr: 4.761904761904762e-05\tLoss: 0.173973\n",
      "Train Epoch: 21 [20480/60000 (34%)] lr: 4.761904761904762e-05\tLoss: 0.164143\n",
      "Train Epoch: 21 [23040/60000 (38%)] lr: 4.761904761904762e-05\tLoss: 0.170095\n",
      "Train Epoch: 21 [25600/60000 (43%)] lr: 4.761904761904762e-05\tLoss: 0.217551\n",
      "Train Epoch: 21 [28160/60000 (47%)] lr: 4.761904761904762e-05\tLoss: 0.160186\n",
      "Train Epoch: 21 [30720/60000 (51%)] lr: 4.761904761904762e-05\tLoss: 0.233918\n",
      "Train Epoch: 21 [33280/60000 (55%)] lr: 4.761904761904762e-05\tLoss: 0.192819\n",
      "Train Epoch: 21 [35840/60000 (60%)] lr: 4.761904761904762e-05\tLoss: 0.158097\n",
      "Train Epoch: 21 [38400/60000 (64%)] lr: 4.761904761904762e-05\tLoss: 0.116786\n",
      "Train Epoch: 21 [40960/60000 (68%)] lr: 4.761904761904762e-05\tLoss: 0.149453\n",
      "Train Epoch: 21 [43520/60000 (72%)] lr: 4.761904761904762e-05\tLoss: 0.127749\n",
      "Train Epoch: 21 [46080/60000 (77%)] lr: 4.761904761904762e-05\tLoss: 0.128461\n",
      "Train Epoch: 21 [48640/60000 (81%)] lr: 4.761904761904762e-05\tLoss: 0.181594\n",
      "Train Epoch: 21 [51200/60000 (85%)] lr: 4.761904761904762e-05\tLoss: 0.200515\n",
      "Train Epoch: 21 [53760/60000 (89%)] lr: 4.761904761904762e-05\tLoss: 0.158147\n",
      "Train Epoch: 21 [56320/60000 (94%)] lr: 4.761904761904762e-05\tLoss: 0.120958\n",
      "Train Epoch: 21 [58880/60000 (98%)] lr: 4.761904761904762e-05\tLoss: 0.233311\n",
      "Finish 21 epoch(s). Epoch loss: 0.18464217515701944.\n",
      "Train Epoch: 22 [0/60000 (0%)] lr: 4.545454545454546e-05\tLoss: 0.131841\n",
      "Train Epoch: 22 [2560/60000 (4%)] lr: 4.545454545454546e-05\tLoss: 0.129694\n",
      "Train Epoch: 22 [5120/60000 (9%)] lr: 4.545454545454546e-05\tLoss: 0.096862\n",
      "Train Epoch: 22 [7680/60000 (13%)] lr: 4.545454545454546e-05\tLoss: 0.206749\n",
      "Train Epoch: 22 [10240/60000 (17%)] lr: 4.545454545454546e-05\tLoss: 0.184945\n",
      "Train Epoch: 22 [12800/60000 (21%)] lr: 4.545454545454546e-05\tLoss: 0.153774\n",
      "Train Epoch: 22 [15360/60000 (26%)] lr: 4.545454545454546e-05\tLoss: 0.193412\n",
      "Train Epoch: 22 [17920/60000 (30%)] lr: 4.545454545454546e-05\tLoss: 0.112236\n",
      "Train Epoch: 22 [20480/60000 (34%)] lr: 4.545454545454546e-05\tLoss: 0.205289\n",
      "Train Epoch: 22 [23040/60000 (38%)] lr: 4.545454545454546e-05\tLoss: 0.138791\n",
      "Train Epoch: 22 [25600/60000 (43%)] lr: 4.545454545454546e-05\tLoss: 0.124230\n",
      "Train Epoch: 22 [28160/60000 (47%)] lr: 4.545454545454546e-05\tLoss: 0.128836\n",
      "Train Epoch: 22 [30720/60000 (51%)] lr: 4.545454545454546e-05\tLoss: 0.156617\n",
      "Train Epoch: 22 [33280/60000 (55%)] lr: 4.545454545454546e-05\tLoss: 0.185610\n",
      "Train Epoch: 22 [35840/60000 (60%)] lr: 4.545454545454546e-05\tLoss: 0.231012\n",
      "Train Epoch: 22 [38400/60000 (64%)] lr: 4.545454545454546e-05\tLoss: 0.220691\n",
      "Train Epoch: 22 [40960/60000 (68%)] lr: 4.545454545454546e-05\tLoss: 0.140254\n",
      "Train Epoch: 22 [43520/60000 (72%)] lr: 4.545454545454546e-05\tLoss: 0.144907\n",
      "Train Epoch: 22 [46080/60000 (77%)] lr: 4.545454545454546e-05\tLoss: 0.204748\n",
      "Train Epoch: 22 [48640/60000 (81%)] lr: 4.545454545454546e-05\tLoss: 0.182565\n",
      "Train Epoch: 22 [51200/60000 (85%)] lr: 4.545454545454546e-05\tLoss: 0.210426\n",
      "Train Epoch: 22 [53760/60000 (89%)] lr: 4.545454545454546e-05\tLoss: 0.200594\n",
      "Train Epoch: 22 [56320/60000 (94%)] lr: 4.545454545454546e-05\tLoss: 0.160894\n",
      "Train Epoch: 22 [58880/60000 (98%)] lr: 4.545454545454546e-05\tLoss: 0.152712\n",
      "Finish 22 epoch(s). Epoch loss: 0.18442692319129375.\n",
      "Train Epoch: 23 [0/60000 (0%)] lr: 4.347826086956522e-05\tLoss: 0.237533\n",
      "Train Epoch: 23 [2560/60000 (4%)] lr: 4.347826086956522e-05\tLoss: 0.191753\n",
      "Train Epoch: 23 [5120/60000 (9%)] lr: 4.347826086956522e-05\tLoss: 0.168530\n",
      "Train Epoch: 23 [7680/60000 (13%)] lr: 4.347826086956522e-05\tLoss: 0.154662\n",
      "Train Epoch: 23 [10240/60000 (17%)] lr: 4.347826086956522e-05\tLoss: 0.151951\n",
      "Train Epoch: 23 [12800/60000 (21%)] lr: 4.347826086956522e-05\tLoss: 0.134699\n",
      "Train Epoch: 23 [15360/60000 (26%)] lr: 4.347826086956522e-05\tLoss: 0.192313\n",
      "Train Epoch: 23 [17920/60000 (30%)] lr: 4.347826086956522e-05\tLoss: 0.187893\n",
      "Train Epoch: 23 [20480/60000 (34%)] lr: 4.347826086956522e-05\tLoss: 0.132291\n",
      "Train Epoch: 23 [23040/60000 (38%)] lr: 4.347826086956522e-05\tLoss: 0.126516\n",
      "Train Epoch: 23 [25600/60000 (43%)] lr: 4.347826086956522e-05\tLoss: 0.202739\n",
      "Train Epoch: 23 [28160/60000 (47%)] lr: 4.347826086956522e-05\tLoss: 0.127127\n",
      "Train Epoch: 23 [30720/60000 (51%)] lr: 4.347826086956522e-05\tLoss: 0.221254\n",
      "Train Epoch: 23 [33280/60000 (55%)] lr: 4.347826086956522e-05\tLoss: 0.139540\n",
      "Train Epoch: 23 [35840/60000 (60%)] lr: 4.347826086956522e-05\tLoss: 0.197092\n",
      "Train Epoch: 23 [38400/60000 (64%)] lr: 4.347826086956522e-05\tLoss: 0.220795\n",
      "Train Epoch: 23 [40960/60000 (68%)] lr: 4.347826086956522e-05\tLoss: 0.175382\n",
      "Train Epoch: 23 [43520/60000 (72%)] lr: 4.347826086956522e-05\tLoss: 0.191797\n",
      "Train Epoch: 23 [46080/60000 (77%)] lr: 4.347826086956522e-05\tLoss: 0.184618\n",
      "Train Epoch: 23 [48640/60000 (81%)] lr: 4.347826086956522e-05\tLoss: 0.143886\n",
      "Train Epoch: 23 [51200/60000 (85%)] lr: 4.347826086956522e-05\tLoss: 0.208707\n",
      "Train Epoch: 23 [53760/60000 (89%)] lr: 4.347826086956522e-05\tLoss: 0.179364\n",
      "Train Epoch: 23 [56320/60000 (94%)] lr: 4.347826086956522e-05\tLoss: 0.229083\n",
      "Train Epoch: 23 [58880/60000 (98%)] lr: 4.347826086956522e-05\tLoss: 0.107777\n",
      "Finish 23 epoch(s). Epoch loss: 0.18032283224958054.\n",
      "Train Epoch: 24 [0/60000 (0%)] lr: 4.1666666666666665e-05\tLoss: 0.211943\n",
      "Train Epoch: 24 [2560/60000 (4%)] lr: 4.1666666666666665e-05\tLoss: 0.145340\n",
      "Train Epoch: 24 [5120/60000 (9%)] lr: 4.1666666666666665e-05\tLoss: 0.203112\n",
      "Train Epoch: 24 [7680/60000 (13%)] lr: 4.1666666666666665e-05\tLoss: 0.180263\n",
      "Train Epoch: 24 [10240/60000 (17%)] lr: 4.1666666666666665e-05\tLoss: 0.188548\n",
      "Train Epoch: 24 [12800/60000 (21%)] lr: 4.1666666666666665e-05\tLoss: 0.257817\n",
      "Train Epoch: 24 [15360/60000 (26%)] lr: 4.1666666666666665e-05\tLoss: 0.239275\n",
      "Train Epoch: 24 [17920/60000 (30%)] lr: 4.1666666666666665e-05\tLoss: 0.284402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [20480/60000 (34%)] lr: 4.1666666666666665e-05\tLoss: 0.216705\n",
      "Train Epoch: 24 [23040/60000 (38%)] lr: 4.1666666666666665e-05\tLoss: 0.142668\n",
      "Train Epoch: 24 [25600/60000 (43%)] lr: 4.1666666666666665e-05\tLoss: 0.210677\n",
      "Train Epoch: 24 [28160/60000 (47%)] lr: 4.1666666666666665e-05\tLoss: 0.175924\n",
      "Train Epoch: 24 [30720/60000 (51%)] lr: 4.1666666666666665e-05\tLoss: 0.179077\n",
      "Train Epoch: 24 [33280/60000 (55%)] lr: 4.1666666666666665e-05\tLoss: 0.160558\n",
      "Train Epoch: 24 [35840/60000 (60%)] lr: 4.1666666666666665e-05\tLoss: 0.170980\n",
      "Train Epoch: 24 [38400/60000 (64%)] lr: 4.1666666666666665e-05\tLoss: 0.200610\n",
      "Train Epoch: 24 [40960/60000 (68%)] lr: 4.1666666666666665e-05\tLoss: 0.118699\n",
      "Train Epoch: 24 [43520/60000 (72%)] lr: 4.1666666666666665e-05\tLoss: 0.155347\n",
      "Train Epoch: 24 [46080/60000 (77%)] lr: 4.1666666666666665e-05\tLoss: 0.234472\n",
      "Train Epoch: 24 [48640/60000 (81%)] lr: 4.1666666666666665e-05\tLoss: 0.161739\n",
      "Train Epoch: 24 [51200/60000 (85%)] lr: 4.1666666666666665e-05\tLoss: 0.159142\n",
      "Train Epoch: 24 [53760/60000 (89%)] lr: 4.1666666666666665e-05\tLoss: 0.182340\n",
      "Train Epoch: 24 [56320/60000 (94%)] lr: 4.1666666666666665e-05\tLoss: 0.194859\n",
      "Train Epoch: 24 [58880/60000 (98%)] lr: 4.1666666666666665e-05\tLoss: 0.218281\n",
      "Finish 24 epoch(s). Epoch loss: 0.17975102092357392.\n",
      "Train Epoch: 25 [0/60000 (0%)] lr: 4e-05\tLoss: 0.201979\n",
      "Train Epoch: 25 [2560/60000 (4%)] lr: 4e-05\tLoss: 0.168123\n",
      "Train Epoch: 25 [5120/60000 (9%)] lr: 4e-05\tLoss: 0.140831\n",
      "Train Epoch: 25 [7680/60000 (13%)] lr: 4e-05\tLoss: 0.167228\n",
      "Train Epoch: 25 [10240/60000 (17%)] lr: 4e-05\tLoss: 0.245326\n",
      "Train Epoch: 25 [12800/60000 (21%)] lr: 4e-05\tLoss: 0.115738\n",
      "Train Epoch: 25 [15360/60000 (26%)] lr: 4e-05\tLoss: 0.166502\n",
      "Train Epoch: 25 [17920/60000 (30%)] lr: 4e-05\tLoss: 0.171478\n",
      "Train Epoch: 25 [20480/60000 (34%)] lr: 4e-05\tLoss: 0.129722\n",
      "Train Epoch: 25 [23040/60000 (38%)] lr: 4e-05\tLoss: 0.216244\n",
      "Train Epoch: 25 [25600/60000 (43%)] lr: 4e-05\tLoss: 0.173821\n",
      "Train Epoch: 25 [28160/60000 (47%)] lr: 4e-05\tLoss: 0.142701\n",
      "Train Epoch: 25 [30720/60000 (51%)] lr: 4e-05\tLoss: 0.179439\n",
      "Train Epoch: 25 [33280/60000 (55%)] lr: 4e-05\tLoss: 0.123271\n",
      "Train Epoch: 25 [35840/60000 (60%)] lr: 4e-05\tLoss: 0.182242\n",
      "Train Epoch: 25 [38400/60000 (64%)] lr: 4e-05\tLoss: 0.214281\n",
      "Train Epoch: 25 [40960/60000 (68%)] lr: 4e-05\tLoss: 0.200459\n",
      "Train Epoch: 25 [43520/60000 (72%)] lr: 4e-05\tLoss: 0.203996\n",
      "Train Epoch: 25 [46080/60000 (77%)] lr: 4e-05\tLoss: 0.187583\n",
      "Train Epoch: 25 [48640/60000 (81%)] lr: 4e-05\tLoss: 0.141960\n",
      "Train Epoch: 25 [51200/60000 (85%)] lr: 4e-05\tLoss: 0.235300\n",
      "Train Epoch: 25 [53760/60000 (89%)] lr: 4e-05\tLoss: 0.129417\n",
      "Train Epoch: 25 [56320/60000 (94%)] lr: 4e-05\tLoss: 0.230431\n",
      "Train Epoch: 25 [58880/60000 (98%)] lr: 4e-05\tLoss: 0.162429\n",
      "Finish 25 epoch(s). Epoch loss: 0.17852803830136643.\n",
      "Train Epoch: 26 [0/60000 (0%)] lr: 3.846153846153846e-05\tLoss: 0.140744\n",
      "Train Epoch: 26 [2560/60000 (4%)] lr: 3.846153846153846e-05\tLoss: 0.215445\n",
      "Train Epoch: 26 [5120/60000 (9%)] lr: 3.846153846153846e-05\tLoss: 0.181480\n",
      "Train Epoch: 26 [7680/60000 (13%)] lr: 3.846153846153846e-05\tLoss: 0.195543\n",
      "Train Epoch: 26 [10240/60000 (17%)] lr: 3.846153846153846e-05\tLoss: 0.182087\n",
      "Train Epoch: 26 [12800/60000 (21%)] lr: 3.846153846153846e-05\tLoss: 0.189786\n",
      "Train Epoch: 26 [15360/60000 (26%)] lr: 3.846153846153846e-05\tLoss: 0.225263\n",
      "Train Epoch: 26 [17920/60000 (30%)] lr: 3.846153846153846e-05\tLoss: 0.143204\n",
      "Train Epoch: 26 [20480/60000 (34%)] lr: 3.846153846153846e-05\tLoss: 0.096122\n",
      "Train Epoch: 26 [23040/60000 (38%)] lr: 3.846153846153846e-05\tLoss: 0.222844\n",
      "Train Epoch: 26 [25600/60000 (43%)] lr: 3.846153846153846e-05\tLoss: 0.175326\n",
      "Train Epoch: 26 [28160/60000 (47%)] lr: 3.846153846153846e-05\tLoss: 0.170590\n",
      "Train Epoch: 26 [30720/60000 (51%)] lr: 3.846153846153846e-05\tLoss: 0.182868\n",
      "Train Epoch: 26 [33280/60000 (55%)] lr: 3.846153846153846e-05\tLoss: 0.135977\n",
      "Train Epoch: 26 [35840/60000 (60%)] lr: 3.846153846153846e-05\tLoss: 0.123595\n",
      "Train Epoch: 26 [38400/60000 (64%)] lr: 3.846153846153846e-05\tLoss: 0.203850\n",
      "Train Epoch: 26 [40960/60000 (68%)] lr: 3.846153846153846e-05\tLoss: 0.228203\n",
      "Train Epoch: 26 [43520/60000 (72%)] lr: 3.846153846153846e-05\tLoss: 0.239265\n",
      "Train Epoch: 26 [46080/60000 (77%)] lr: 3.846153846153846e-05\tLoss: 0.115350\n",
      "Train Epoch: 26 [48640/60000 (81%)] lr: 3.846153846153846e-05\tLoss: 0.152237\n",
      "Train Epoch: 26 [51200/60000 (85%)] lr: 3.846153846153846e-05\tLoss: 0.155514\n",
      "Train Epoch: 26 [53760/60000 (89%)] lr: 3.846153846153846e-05\tLoss: 0.188019\n",
      "Train Epoch: 26 [56320/60000 (94%)] lr: 3.846153846153846e-05\tLoss: 0.182070\n",
      "Train Epoch: 26 [58880/60000 (98%)] lr: 3.846153846153846e-05\tLoss: 0.193466\n",
      "Finish 26 epoch(s). Epoch loss: 0.18109081996248125.\n",
      "Train Epoch: 27 [0/60000 (0%)] lr: 3.7037037037037037e-05\tLoss: 0.090105\n",
      "Train Epoch: 27 [2560/60000 (4%)] lr: 3.7037037037037037e-05\tLoss: 0.179141\n",
      "Train Epoch: 27 [5120/60000 (9%)] lr: 3.7037037037037037e-05\tLoss: 0.183371\n",
      "Train Epoch: 27 [7680/60000 (13%)] lr: 3.7037037037037037e-05\tLoss: 0.148616\n",
      "Train Epoch: 27 [10240/60000 (17%)] lr: 3.7037037037037037e-05\tLoss: 0.247247\n",
      "Train Epoch: 27 [12800/60000 (21%)] lr: 3.7037037037037037e-05\tLoss: 0.133031\n",
      "Train Epoch: 27 [15360/60000 (26%)] lr: 3.7037037037037037e-05\tLoss: 0.077860\n",
      "Train Epoch: 27 [17920/60000 (30%)] lr: 3.7037037037037037e-05\tLoss: 0.261485\n",
      "Train Epoch: 27 [20480/60000 (34%)] lr: 3.7037037037037037e-05\tLoss: 0.149736\n",
      "Train Epoch: 27 [23040/60000 (38%)] lr: 3.7037037037037037e-05\tLoss: 0.262818\n",
      "Train Epoch: 27 [25600/60000 (43%)] lr: 3.7037037037037037e-05\tLoss: 0.186778\n",
      "Train Epoch: 27 [28160/60000 (47%)] lr: 3.7037037037037037e-05\tLoss: 0.255711\n",
      "Train Epoch: 27 [30720/60000 (51%)] lr: 3.7037037037037037e-05\tLoss: 0.169336\n",
      "Train Epoch: 27 [33280/60000 (55%)] lr: 3.7037037037037037e-05\tLoss: 0.259318\n",
      "Train Epoch: 27 [35840/60000 (60%)] lr: 3.7037037037037037e-05\tLoss: 0.169960\n",
      "Train Epoch: 27 [38400/60000 (64%)] lr: 3.7037037037037037e-05\tLoss: 0.179661\n",
      "Train Epoch: 27 [40960/60000 (68%)] lr: 3.7037037037037037e-05\tLoss: 0.209899\n",
      "Train Epoch: 27 [43520/60000 (72%)] lr: 3.7037037037037037e-05\tLoss: 0.162572\n",
      "Train Epoch: 27 [46080/60000 (77%)] lr: 3.7037037037037037e-05\tLoss: 0.102638\n",
      "Train Epoch: 27 [48640/60000 (81%)] lr: 3.7037037037037037e-05\tLoss: 0.133446\n",
      "Train Epoch: 27 [51200/60000 (85%)] lr: 3.7037037037037037e-05\tLoss: 0.123174\n",
      "Train Epoch: 27 [53760/60000 (89%)] lr: 3.7037037037037037e-05\tLoss: 0.208343\n",
      "Train Epoch: 27 [56320/60000 (94%)] lr: 3.7037037037037037e-05\tLoss: 0.202017\n",
      "Train Epoch: 27 [58880/60000 (98%)] lr: 3.7037037037037037e-05\tLoss: 0.203307\n",
      "Finish 27 epoch(s). Epoch loss: 0.17689991875531827.\n",
      "Train Epoch: 28 [0/60000 (0%)] lr: 3.571428571428572e-05\tLoss: 0.173934\n",
      "Train Epoch: 28 [2560/60000 (4%)] lr: 3.571428571428572e-05\tLoss: 0.210265\n",
      "Train Epoch: 28 [5120/60000 (9%)] lr: 3.571428571428572e-05\tLoss: 0.217696\n",
      "Train Epoch: 28 [7680/60000 (13%)] lr: 3.571428571428572e-05\tLoss: 0.143651\n",
      "Train Epoch: 28 [10240/60000 (17%)] lr: 3.571428571428572e-05\tLoss: 0.186632\n",
      "Train Epoch: 28 [12800/60000 (21%)] lr: 3.571428571428572e-05\tLoss: 0.247135\n",
      "Train Epoch: 28 [15360/60000 (26%)] lr: 3.571428571428572e-05\tLoss: 0.324982\n",
      "Train Epoch: 28 [17920/60000 (30%)] lr: 3.571428571428572e-05\tLoss: 0.176197\n",
      "Train Epoch: 28 [20480/60000 (34%)] lr: 3.571428571428572e-05\tLoss: 0.174877\n",
      "Train Epoch: 28 [23040/60000 (38%)] lr: 3.571428571428572e-05\tLoss: 0.150304\n",
      "Train Epoch: 28 [25600/60000 (43%)] lr: 3.571428571428572e-05\tLoss: 0.112400\n",
      "Train Epoch: 28 [28160/60000 (47%)] lr: 3.571428571428572e-05\tLoss: 0.228626\n",
      "Train Epoch: 28 [30720/60000 (51%)] lr: 3.571428571428572e-05\tLoss: 0.132445\n",
      "Train Epoch: 28 [33280/60000 (55%)] lr: 3.571428571428572e-05\tLoss: 0.136802\n",
      "Train Epoch: 28 [35840/60000 (60%)] lr: 3.571428571428572e-05\tLoss: 0.134494\n",
      "Train Epoch: 28 [38400/60000 (64%)] lr: 3.571428571428572e-05\tLoss: 0.222556\n",
      "Train Epoch: 28 [40960/60000 (68%)] lr: 3.571428571428572e-05\tLoss: 0.157351\n",
      "Train Epoch: 28 [43520/60000 (72%)] lr: 3.571428571428572e-05\tLoss: 0.252404\n",
      "Train Epoch: 28 [46080/60000 (77%)] lr: 3.571428571428572e-05\tLoss: 0.119876\n",
      "Train Epoch: 28 [48640/60000 (81%)] lr: 3.571428571428572e-05\tLoss: 0.186369\n",
      "Train Epoch: 28 [51200/60000 (85%)] lr: 3.571428571428572e-05\tLoss: 0.132041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [53760/60000 (89%)] lr: 3.571428571428572e-05\tLoss: 0.156629\n",
      "Train Epoch: 28 [56320/60000 (94%)] lr: 3.571428571428572e-05\tLoss: 0.286672\n",
      "Train Epoch: 28 [58880/60000 (98%)] lr: 3.571428571428572e-05\tLoss: 0.237636\n",
      "Finish 28 epoch(s). Epoch loss: 0.17521285132524814.\n",
      "Train Epoch: 29 [0/60000 (0%)] lr: 3.4482758620689657e-05\tLoss: 0.138124\n",
      "Train Epoch: 29 [2560/60000 (4%)] lr: 3.4482758620689657e-05\tLoss: 0.207500\n",
      "Train Epoch: 29 [5120/60000 (9%)] lr: 3.4482758620689657e-05\tLoss: 0.194497\n",
      "Train Epoch: 29 [7680/60000 (13%)] lr: 3.4482758620689657e-05\tLoss: 0.159786\n",
      "Train Epoch: 29 [10240/60000 (17%)] lr: 3.4482758620689657e-05\tLoss: 0.159967\n",
      "Train Epoch: 29 [12800/60000 (21%)] lr: 3.4482758620689657e-05\tLoss: 0.163789\n",
      "Train Epoch: 29 [15360/60000 (26%)] lr: 3.4482758620689657e-05\tLoss: 0.183106\n",
      "Train Epoch: 29 [17920/60000 (30%)] lr: 3.4482758620689657e-05\tLoss: 0.187423\n",
      "Train Epoch: 29 [20480/60000 (34%)] lr: 3.4482758620689657e-05\tLoss: 0.114919\n",
      "Train Epoch: 29 [23040/60000 (38%)] lr: 3.4482758620689657e-05\tLoss: 0.277584\n",
      "Train Epoch: 29 [25600/60000 (43%)] lr: 3.4482758620689657e-05\tLoss: 0.182514\n",
      "Train Epoch: 29 [28160/60000 (47%)] lr: 3.4482758620689657e-05\tLoss: 0.127717\n",
      "Train Epoch: 29 [30720/60000 (51%)] lr: 3.4482758620689657e-05\tLoss: 0.194325\n",
      "Train Epoch: 29 [33280/60000 (55%)] lr: 3.4482758620689657e-05\tLoss: 0.158657\n",
      "Train Epoch: 29 [35840/60000 (60%)] lr: 3.4482758620689657e-05\tLoss: 0.106132\n",
      "Train Epoch: 29 [38400/60000 (64%)] lr: 3.4482758620689657e-05\tLoss: 0.243677\n",
      "Train Epoch: 29 [40960/60000 (68%)] lr: 3.4482758620689657e-05\tLoss: 0.163672\n",
      "Train Epoch: 29 [43520/60000 (72%)] lr: 3.4482758620689657e-05\tLoss: 0.223412\n",
      "Train Epoch: 29 [46080/60000 (77%)] lr: 3.4482758620689657e-05\tLoss: 0.160029\n",
      "Train Epoch: 29 [48640/60000 (81%)] lr: 3.4482758620689657e-05\tLoss: 0.162932\n",
      "Train Epoch: 29 [51200/60000 (85%)] lr: 3.4482758620689657e-05\tLoss: 0.155161\n",
      "Train Epoch: 29 [53760/60000 (89%)] lr: 3.4482758620689657e-05\tLoss: 0.209147\n",
      "Train Epoch: 29 [56320/60000 (94%)] lr: 3.4482758620689657e-05\tLoss: 0.223124\n",
      "Train Epoch: 29 [58880/60000 (98%)] lr: 3.4482758620689657e-05\tLoss: 0.186562\n",
      "Finish 29 epoch(s). Epoch loss: 0.1780753418486169.\n",
      "Train Epoch: 30 [0/60000 (0%)] lr: 3.3333333333333335e-05\tLoss: 0.175936\n",
      "Train Epoch: 30 [2560/60000 (4%)] lr: 3.3333333333333335e-05\tLoss: 0.180401\n",
      "Train Epoch: 30 [5120/60000 (9%)] lr: 3.3333333333333335e-05\tLoss: 0.132088\n",
      "Train Epoch: 30 [7680/60000 (13%)] lr: 3.3333333333333335e-05\tLoss: 0.191728\n",
      "Train Epoch: 30 [10240/60000 (17%)] lr: 3.3333333333333335e-05\tLoss: 0.103369\n",
      "Train Epoch: 30 [12800/60000 (21%)] lr: 3.3333333333333335e-05\tLoss: 0.213340\n",
      "Train Epoch: 30 [15360/60000 (26%)] lr: 3.3333333333333335e-05\tLoss: 0.109208\n",
      "Train Epoch: 30 [17920/60000 (30%)] lr: 3.3333333333333335e-05\tLoss: 0.141394\n",
      "Train Epoch: 30 [20480/60000 (34%)] lr: 3.3333333333333335e-05\tLoss: 0.157410\n",
      "Train Epoch: 30 [23040/60000 (38%)] lr: 3.3333333333333335e-05\tLoss: 0.186852\n",
      "Train Epoch: 30 [25600/60000 (43%)] lr: 3.3333333333333335e-05\tLoss: 0.185532\n",
      "Train Epoch: 30 [28160/60000 (47%)] lr: 3.3333333333333335e-05\tLoss: 0.154630\n",
      "Train Epoch: 30 [30720/60000 (51%)] lr: 3.3333333333333335e-05\tLoss: 0.201656\n",
      "Train Epoch: 30 [33280/60000 (55%)] lr: 3.3333333333333335e-05\tLoss: 0.139209\n",
      "Train Epoch: 30 [35840/60000 (60%)] lr: 3.3333333333333335e-05\tLoss: 0.182073\n",
      "Train Epoch: 30 [38400/60000 (64%)] lr: 3.3333333333333335e-05\tLoss: 0.159041\n",
      "Train Epoch: 30 [40960/60000 (68%)] lr: 3.3333333333333335e-05\tLoss: 0.218820\n",
      "Train Epoch: 30 [43520/60000 (72%)] lr: 3.3333333333333335e-05\tLoss: 0.192994\n",
      "Train Epoch: 30 [46080/60000 (77%)] lr: 3.3333333333333335e-05\tLoss: 0.127060\n",
      "Train Epoch: 30 [48640/60000 (81%)] lr: 3.3333333333333335e-05\tLoss: 0.126061\n",
      "Train Epoch: 30 [51200/60000 (85%)] lr: 3.3333333333333335e-05\tLoss: 0.153397\n",
      "Train Epoch: 30 [53760/60000 (89%)] lr: 3.3333333333333335e-05\tLoss: 0.154318\n",
      "Train Epoch: 30 [56320/60000 (94%)] lr: 3.3333333333333335e-05\tLoss: 0.146512\n",
      "Train Epoch: 30 [58880/60000 (98%)] lr: 3.3333333333333335e-05\tLoss: 0.205345\n",
      "Finish 30 epoch(s). Epoch loss: 0.1735153538749573.\n",
      "Train Epoch: 31 [0/60000 (0%)] lr: 3.2258064516129034e-05\tLoss: 0.143694\n",
      "Train Epoch: 31 [2560/60000 (4%)] lr: 3.2258064516129034e-05\tLoss: 0.190325\n",
      "Train Epoch: 31 [5120/60000 (9%)] lr: 3.2258064516129034e-05\tLoss: 0.161593\n",
      "Train Epoch: 31 [7680/60000 (13%)] lr: 3.2258064516129034e-05\tLoss: 0.141062\n",
      "Train Epoch: 31 [10240/60000 (17%)] lr: 3.2258064516129034e-05\tLoss: 0.175596\n",
      "Train Epoch: 31 [12800/60000 (21%)] lr: 3.2258064516129034e-05\tLoss: 0.221024\n",
      "Train Epoch: 31 [15360/60000 (26%)] lr: 3.2258064516129034e-05\tLoss: 0.155246\n",
      "Train Epoch: 31 [17920/60000 (30%)] lr: 3.2258064516129034e-05\tLoss: 0.165267\n",
      "Train Epoch: 31 [20480/60000 (34%)] lr: 3.2258064516129034e-05\tLoss: 0.295857\n",
      "Train Epoch: 31 [23040/60000 (38%)] lr: 3.2258064516129034e-05\tLoss: 0.180488\n",
      "Train Epoch: 31 [25600/60000 (43%)] lr: 3.2258064516129034e-05\tLoss: 0.167317\n",
      "Train Epoch: 31 [28160/60000 (47%)] lr: 3.2258064516129034e-05\tLoss: 0.185465\n",
      "Train Epoch: 31 [30720/60000 (51%)] lr: 3.2258064516129034e-05\tLoss: 0.184245\n",
      "Train Epoch: 31 [33280/60000 (55%)] lr: 3.2258064516129034e-05\tLoss: 0.219642\n",
      "Train Epoch: 31 [35840/60000 (60%)] lr: 3.2258064516129034e-05\tLoss: 0.188032\n",
      "Train Epoch: 31 [38400/60000 (64%)] lr: 3.2258064516129034e-05\tLoss: 0.139710\n",
      "Train Epoch: 31 [40960/60000 (68%)] lr: 3.2258064516129034e-05\tLoss: 0.106618\n",
      "Train Epoch: 31 [43520/60000 (72%)] lr: 3.2258064516129034e-05\tLoss: 0.185627\n",
      "Train Epoch: 31 [46080/60000 (77%)] lr: 3.2258064516129034e-05\tLoss: 0.110937\n",
      "Train Epoch: 31 [48640/60000 (81%)] lr: 3.2258064516129034e-05\tLoss: 0.264087\n",
      "Train Epoch: 31 [51200/60000 (85%)] lr: 3.2258064516129034e-05\tLoss: 0.173154\n",
      "Train Epoch: 31 [53760/60000 (89%)] lr: 3.2258064516129034e-05\tLoss: 0.179788\n",
      "Train Epoch: 31 [56320/60000 (94%)] lr: 3.2258064516129034e-05\tLoss: 0.240820\n",
      "Train Epoch: 31 [58880/60000 (98%)] lr: 3.2258064516129034e-05\tLoss: 0.122378\n",
      "Finish 31 epoch(s). Epoch loss: 0.17863906596569304.\n",
      "Train Epoch: 32 [0/60000 (0%)] lr: 3.125e-05\tLoss: 0.209451\n",
      "Train Epoch: 32 [2560/60000 (4%)] lr: 3.125e-05\tLoss: 0.116703\n",
      "Train Epoch: 32 [5120/60000 (9%)] lr: 3.125e-05\tLoss: 0.164241\n",
      "Train Epoch: 32 [7680/60000 (13%)] lr: 3.125e-05\tLoss: 0.145309\n",
      "Train Epoch: 32 [10240/60000 (17%)] lr: 3.125e-05\tLoss: 0.239781\n",
      "Train Epoch: 32 [12800/60000 (21%)] lr: 3.125e-05\tLoss: 0.270923\n",
      "Train Epoch: 32 [15360/60000 (26%)] lr: 3.125e-05\tLoss: 0.168483\n",
      "Train Epoch: 32 [17920/60000 (30%)] lr: 3.125e-05\tLoss: 0.187150\n",
      "Train Epoch: 32 [20480/60000 (34%)] lr: 3.125e-05\tLoss: 0.105188\n",
      "Train Epoch: 32 [23040/60000 (38%)] lr: 3.125e-05\tLoss: 0.146549\n",
      "Train Epoch: 32 [25600/60000 (43%)] lr: 3.125e-05\tLoss: 0.111371\n",
      "Train Epoch: 32 [28160/60000 (47%)] lr: 3.125e-05\tLoss: 0.155145\n",
      "Train Epoch: 32 [30720/60000 (51%)] lr: 3.125e-05\tLoss: 0.169315\n",
      "Train Epoch: 32 [33280/60000 (55%)] lr: 3.125e-05\tLoss: 0.168846\n",
      "Train Epoch: 32 [35840/60000 (60%)] lr: 3.125e-05\tLoss: 0.167848\n",
      "Train Epoch: 32 [38400/60000 (64%)] lr: 3.125e-05\tLoss: 0.098312\n",
      "Train Epoch: 32 [40960/60000 (68%)] lr: 3.125e-05\tLoss: 0.138378\n",
      "Train Epoch: 32 [43520/60000 (72%)] lr: 3.125e-05\tLoss: 0.161670\n",
      "Train Epoch: 32 [46080/60000 (77%)] lr: 3.125e-05\tLoss: 0.201767\n",
      "Train Epoch: 32 [48640/60000 (81%)] lr: 3.125e-05\tLoss: 0.211393\n",
      "Train Epoch: 32 [51200/60000 (85%)] lr: 3.125e-05\tLoss: 0.139847\n",
      "Train Epoch: 32 [53760/60000 (89%)] lr: 3.125e-05\tLoss: 0.143218\n",
      "Train Epoch: 32 [56320/60000 (94%)] lr: 3.125e-05\tLoss: 0.154374\n",
      "Train Epoch: 32 [58880/60000 (98%)] lr: 3.125e-05\tLoss: 0.191122\n",
      "Finish 32 epoch(s). Epoch loss: 0.17289707584584013.\n",
      "Train Epoch: 33 [0/60000 (0%)] lr: 3.0303030303030302e-05\tLoss: 0.172005\n",
      "Train Epoch: 33 [2560/60000 (4%)] lr: 3.0303030303030302e-05\tLoss: 0.221810\n",
      "Train Epoch: 33 [5120/60000 (9%)] lr: 3.0303030303030302e-05\tLoss: 0.209317\n",
      "Train Epoch: 33 [7680/60000 (13%)] lr: 3.0303030303030302e-05\tLoss: 0.206416\n",
      "Train Epoch: 33 [10240/60000 (17%)] lr: 3.0303030303030302e-05\tLoss: 0.147532\n",
      "Train Epoch: 33 [12800/60000 (21%)] lr: 3.0303030303030302e-05\tLoss: 0.096405\n",
      "Train Epoch: 33 [15360/60000 (26%)] lr: 3.0303030303030302e-05\tLoss: 0.152667\n",
      "Train Epoch: 33 [17920/60000 (30%)] lr: 3.0303030303030302e-05\tLoss: 0.192169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [20480/60000 (34%)] lr: 3.0303030303030302e-05\tLoss: 0.136695\n",
      "Train Epoch: 33 [23040/60000 (38%)] lr: 3.0303030303030302e-05\tLoss: 0.158054\n",
      "Train Epoch: 33 [25600/60000 (43%)] lr: 3.0303030303030302e-05\tLoss: 0.171301\n",
      "Train Epoch: 33 [28160/60000 (47%)] lr: 3.0303030303030302e-05\tLoss: 0.151630\n",
      "Train Epoch: 33 [30720/60000 (51%)] lr: 3.0303030303030302e-05\tLoss: 0.239310\n",
      "Train Epoch: 33 [33280/60000 (55%)] lr: 3.0303030303030302e-05\tLoss: 0.101732\n",
      "Train Epoch: 33 [35840/60000 (60%)] lr: 3.0303030303030302e-05\tLoss: 0.192407\n",
      "Train Epoch: 33 [38400/60000 (64%)] lr: 3.0303030303030302e-05\tLoss: 0.210370\n",
      "Train Epoch: 33 [40960/60000 (68%)] lr: 3.0303030303030302e-05\tLoss: 0.201549\n",
      "Train Epoch: 33 [43520/60000 (72%)] lr: 3.0303030303030302e-05\tLoss: 0.188137\n",
      "Train Epoch: 33 [46080/60000 (77%)] lr: 3.0303030303030302e-05\tLoss: 0.190311\n",
      "Train Epoch: 33 [48640/60000 (81%)] lr: 3.0303030303030302e-05\tLoss: 0.201382\n",
      "Train Epoch: 33 [51200/60000 (85%)] lr: 3.0303030303030302e-05\tLoss: 0.131163\n",
      "Train Epoch: 33 [53760/60000 (89%)] lr: 3.0303030303030302e-05\tLoss: 0.284043\n",
      "Train Epoch: 33 [56320/60000 (94%)] lr: 3.0303030303030302e-05\tLoss: 0.179371\n",
      "Train Epoch: 33 [58880/60000 (98%)] lr: 3.0303030303030302e-05\tLoss: 0.173591\n",
      "Finish 33 epoch(s). Epoch loss: 0.17417475295827745.\n",
      "Train Epoch: 34 [0/60000 (0%)] lr: 2.9411764705882354e-05\tLoss: 0.198154\n",
      "Train Epoch: 34 [2560/60000 (4%)] lr: 2.9411764705882354e-05\tLoss: 0.186487\n",
      "Train Epoch: 34 [5120/60000 (9%)] lr: 2.9411764705882354e-05\tLoss: 0.252248\n",
      "Train Epoch: 34 [7680/60000 (13%)] lr: 2.9411764705882354e-05\tLoss: 0.121444\n",
      "Train Epoch: 34 [10240/60000 (17%)] lr: 2.9411764705882354e-05\tLoss: 0.063016\n",
      "Train Epoch: 34 [12800/60000 (21%)] lr: 2.9411764705882354e-05\tLoss: 0.126372\n",
      "Train Epoch: 34 [15360/60000 (26%)] lr: 2.9411764705882354e-05\tLoss: 0.198926\n",
      "Train Epoch: 34 [17920/60000 (30%)] lr: 2.9411764705882354e-05\tLoss: 0.133439\n",
      "Train Epoch: 34 [20480/60000 (34%)] lr: 2.9411764705882354e-05\tLoss: 0.234252\n",
      "Train Epoch: 34 [23040/60000 (38%)] lr: 2.9411764705882354e-05\tLoss: 0.272261\n",
      "Train Epoch: 34 [25600/60000 (43%)] lr: 2.9411764705882354e-05\tLoss: 0.203109\n",
      "Train Epoch: 34 [28160/60000 (47%)] lr: 2.9411764705882354e-05\tLoss: 0.181740\n",
      "Train Epoch: 34 [30720/60000 (51%)] lr: 2.9411764705882354e-05\tLoss: 0.148881\n",
      "Train Epoch: 34 [33280/60000 (55%)] lr: 2.9411764705882354e-05\tLoss: 0.189012\n",
      "Train Epoch: 34 [35840/60000 (60%)] lr: 2.9411764705882354e-05\tLoss: 0.167191\n",
      "Train Epoch: 34 [38400/60000 (64%)] lr: 2.9411764705882354e-05\tLoss: 0.196793\n",
      "Train Epoch: 34 [40960/60000 (68%)] lr: 2.9411764705882354e-05\tLoss: 0.256323\n",
      "Train Epoch: 34 [43520/60000 (72%)] lr: 2.9411764705882354e-05\tLoss: 0.134224\n",
      "Train Epoch: 34 [46080/60000 (77%)] lr: 2.9411764705882354e-05\tLoss: 0.204771\n",
      "Train Epoch: 34 [48640/60000 (81%)] lr: 2.9411764705882354e-05\tLoss: 0.181990\n",
      "Train Epoch: 34 [51200/60000 (85%)] lr: 2.9411764705882354e-05\tLoss: 0.202285\n",
      "Train Epoch: 34 [53760/60000 (89%)] lr: 2.9411764705882354e-05\tLoss: 0.184201\n",
      "Train Epoch: 34 [56320/60000 (94%)] lr: 2.9411764705882354e-05\tLoss: 0.137338\n",
      "Train Epoch: 34 [58880/60000 (98%)] lr: 2.9411764705882354e-05\tLoss: 0.125667\n",
      "Finish 34 epoch(s). Epoch loss: 0.1741206870434132.\n",
      "Train Epoch: 35 [0/60000 (0%)] lr: 2.857142857142857e-05\tLoss: 0.124368\n",
      "Train Epoch: 35 [2560/60000 (4%)] lr: 2.857142857142857e-05\tLoss: 0.177418\n",
      "Train Epoch: 35 [5120/60000 (9%)] lr: 2.857142857142857e-05\tLoss: 0.131768\n",
      "Train Epoch: 35 [7680/60000 (13%)] lr: 2.857142857142857e-05\tLoss: 0.195839\n",
      "Train Epoch: 35 [10240/60000 (17%)] lr: 2.857142857142857e-05\tLoss: 0.146115\n",
      "Train Epoch: 35 [12800/60000 (21%)] lr: 2.857142857142857e-05\tLoss: 0.178032\n",
      "Train Epoch: 35 [15360/60000 (26%)] lr: 2.857142857142857e-05\tLoss: 0.236455\n",
      "Train Epoch: 35 [17920/60000 (30%)] lr: 2.857142857142857e-05\tLoss: 0.210374\n",
      "Train Epoch: 35 [20480/60000 (34%)] lr: 2.857142857142857e-05\tLoss: 0.181137\n",
      "Train Epoch: 35 [23040/60000 (38%)] lr: 2.857142857142857e-05\tLoss: 0.136054\n",
      "Train Epoch: 35 [25600/60000 (43%)] lr: 2.857142857142857e-05\tLoss: 0.243986\n",
      "Train Epoch: 35 [28160/60000 (47%)] lr: 2.857142857142857e-05\tLoss: 0.285344\n",
      "Train Epoch: 35 [30720/60000 (51%)] lr: 2.857142857142857e-05\tLoss: 0.128607\n",
      "Train Epoch: 35 [33280/60000 (55%)] lr: 2.857142857142857e-05\tLoss: 0.181238\n",
      "Train Epoch: 35 [35840/60000 (60%)] lr: 2.857142857142857e-05\tLoss: 0.176494\n",
      "Train Epoch: 35 [38400/60000 (64%)] lr: 2.857142857142857e-05\tLoss: 0.213316\n",
      "Train Epoch: 35 [40960/60000 (68%)] lr: 2.857142857142857e-05\tLoss: 0.156431\n",
      "Train Epoch: 35 [43520/60000 (72%)] lr: 2.857142857142857e-05\tLoss: 0.189505\n",
      "Train Epoch: 35 [46080/60000 (77%)] lr: 2.857142857142857e-05\tLoss: 0.193773\n",
      "Train Epoch: 35 [48640/60000 (81%)] lr: 2.857142857142857e-05\tLoss: 0.121026\n",
      "Train Epoch: 35 [51200/60000 (85%)] lr: 2.857142857142857e-05\tLoss: 0.191833\n",
      "Train Epoch: 35 [53760/60000 (89%)] lr: 2.857142857142857e-05\tLoss: 0.133758\n",
      "Train Epoch: 35 [56320/60000 (94%)] lr: 2.857142857142857e-05\tLoss: 0.187904\n",
      "Train Epoch: 35 [58880/60000 (98%)] lr: 2.857142857142857e-05\tLoss: 0.186293\n",
      "Finish 35 epoch(s). Epoch loss: 0.17256736387597754.\n",
      "Train Epoch: 36 [0/60000 (0%)] lr: 2.777777777777778e-05\tLoss: 0.155703\n",
      "Train Epoch: 36 [2560/60000 (4%)] lr: 2.777777777777778e-05\tLoss: 0.156993\n",
      "Train Epoch: 36 [5120/60000 (9%)] lr: 2.777777777777778e-05\tLoss: 0.143672\n",
      "Train Epoch: 36 [7680/60000 (13%)] lr: 2.777777777777778e-05\tLoss: 0.104731\n",
      "Train Epoch: 36 [10240/60000 (17%)] lr: 2.777777777777778e-05\tLoss: 0.191823\n",
      "Train Epoch: 36 [12800/60000 (21%)] lr: 2.777777777777778e-05\tLoss: 0.166399\n",
      "Train Epoch: 36 [15360/60000 (26%)] lr: 2.777777777777778e-05\tLoss: 0.158977\n",
      "Train Epoch: 36 [17920/60000 (30%)] lr: 2.777777777777778e-05\tLoss: 0.181329\n",
      "Train Epoch: 36 [20480/60000 (34%)] lr: 2.777777777777778e-05\tLoss: 0.192489\n",
      "Train Epoch: 36 [23040/60000 (38%)] lr: 2.777777777777778e-05\tLoss: 0.130764\n",
      "Train Epoch: 36 [25600/60000 (43%)] lr: 2.777777777777778e-05\tLoss: 0.131817\n",
      "Train Epoch: 36 [28160/60000 (47%)] lr: 2.777777777777778e-05\tLoss: 0.171717\n",
      "Train Epoch: 36 [30720/60000 (51%)] lr: 2.777777777777778e-05\tLoss: 0.162383\n",
      "Train Epoch: 36 [33280/60000 (55%)] lr: 2.777777777777778e-05\tLoss: 0.116297\n",
      "Train Epoch: 36 [35840/60000 (60%)] lr: 2.777777777777778e-05\tLoss: 0.228724\n",
      "Train Epoch: 36 [38400/60000 (64%)] lr: 2.777777777777778e-05\tLoss: 0.168902\n",
      "Train Epoch: 36 [40960/60000 (68%)] lr: 2.777777777777778e-05\tLoss: 0.170076\n",
      "Train Epoch: 36 [43520/60000 (72%)] lr: 2.777777777777778e-05\tLoss: 0.127280\n",
      "Train Epoch: 36 [46080/60000 (77%)] lr: 2.777777777777778e-05\tLoss: 0.239862\n",
      "Train Epoch: 36 [48640/60000 (81%)] lr: 2.777777777777778e-05\tLoss: 0.127577\n",
      "Train Epoch: 36 [51200/60000 (85%)] lr: 2.777777777777778e-05\tLoss: 0.137699\n",
      "Train Epoch: 36 [53760/60000 (89%)] lr: 2.777777777777778e-05\tLoss: 0.143867\n",
      "Train Epoch: 36 [56320/60000 (94%)] lr: 2.777777777777778e-05\tLoss: 0.125441\n",
      "Train Epoch: 36 [58880/60000 (98%)] lr: 2.777777777777778e-05\tLoss: 0.194326\n",
      "Finish 36 epoch(s). Epoch loss: 0.17320652651659985.\n",
      "Train Epoch: 37 [0/60000 (0%)] lr: 2.7027027027027027e-05\tLoss: 0.154036\n",
      "Train Epoch: 37 [2560/60000 (4%)] lr: 2.7027027027027027e-05\tLoss: 0.147906\n",
      "Train Epoch: 37 [5120/60000 (9%)] lr: 2.7027027027027027e-05\tLoss: 0.124549\n",
      "Train Epoch: 37 [7680/60000 (13%)] lr: 2.7027027027027027e-05\tLoss: 0.176779\n",
      "Train Epoch: 37 [10240/60000 (17%)] lr: 2.7027027027027027e-05\tLoss: 0.246130\n",
      "Train Epoch: 37 [12800/60000 (21%)] lr: 2.7027027027027027e-05\tLoss: 0.099832\n",
      "Train Epoch: 37 [15360/60000 (26%)] lr: 2.7027027027027027e-05\tLoss: 0.174667\n",
      "Train Epoch: 37 [17920/60000 (30%)] lr: 2.7027027027027027e-05\tLoss: 0.133653\n",
      "Train Epoch: 37 [20480/60000 (34%)] lr: 2.7027027027027027e-05\tLoss: 0.136871\n",
      "Train Epoch: 37 [23040/60000 (38%)] lr: 2.7027027027027027e-05\tLoss: 0.278477\n",
      "Train Epoch: 37 [25600/60000 (43%)] lr: 2.7027027027027027e-05\tLoss: 0.201961\n",
      "Train Epoch: 37 [28160/60000 (47%)] lr: 2.7027027027027027e-05\tLoss: 0.123747\n",
      "Train Epoch: 37 [30720/60000 (51%)] lr: 2.7027027027027027e-05\tLoss: 0.104178\n",
      "Train Epoch: 37 [33280/60000 (55%)] lr: 2.7027027027027027e-05\tLoss: 0.172760\n",
      "Train Epoch: 37 [35840/60000 (60%)] lr: 2.7027027027027027e-05\tLoss: 0.087627\n",
      "Train Epoch: 37 [38400/60000 (64%)] lr: 2.7027027027027027e-05\tLoss: 0.149723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [40960/60000 (68%)] lr: 2.7027027027027027e-05\tLoss: 0.163502\n",
      "Train Epoch: 37 [43520/60000 (72%)] lr: 2.7027027027027027e-05\tLoss: 0.161588\n",
      "Train Epoch: 37 [46080/60000 (77%)] lr: 2.7027027027027027e-05\tLoss: 0.149022\n",
      "Train Epoch: 37 [48640/60000 (81%)] lr: 2.7027027027027027e-05\tLoss: 0.167591\n",
      "Train Epoch: 37 [51200/60000 (85%)] lr: 2.7027027027027027e-05\tLoss: 0.105986\n",
      "Train Epoch: 37 [53760/60000 (89%)] lr: 2.7027027027027027e-05\tLoss: 0.133868\n",
      "Train Epoch: 37 [56320/60000 (94%)] lr: 2.7027027027027027e-05\tLoss: 0.150988\n",
      "Train Epoch: 37 [58880/60000 (98%)] lr: 2.7027027027027027e-05\tLoss: 0.194608\n",
      "Finish 37 epoch(s). Epoch loss: 0.16739078347987316.\n",
      "Train Epoch: 38 [0/60000 (0%)] lr: 2.6315789473684212e-05\tLoss: 0.165887\n",
      "Train Epoch: 38 [2560/60000 (4%)] lr: 2.6315789473684212e-05\tLoss: 0.151517\n",
      "Train Epoch: 38 [5120/60000 (9%)] lr: 2.6315789473684212e-05\tLoss: 0.172784\n",
      "Train Epoch: 38 [7680/60000 (13%)] lr: 2.6315789473684212e-05\tLoss: 0.211830\n",
      "Train Epoch: 38 [10240/60000 (17%)] lr: 2.6315789473684212e-05\tLoss: 0.166882\n",
      "Train Epoch: 38 [12800/60000 (21%)] lr: 2.6315789473684212e-05\tLoss: 0.234297\n",
      "Train Epoch: 38 [15360/60000 (26%)] lr: 2.6315789473684212e-05\tLoss: 0.146475\n",
      "Train Epoch: 38 [17920/60000 (30%)] lr: 2.6315789473684212e-05\tLoss: 0.154816\n",
      "Train Epoch: 38 [20480/60000 (34%)] lr: 2.6315789473684212e-05\tLoss: 0.126815\n",
      "Train Epoch: 38 [23040/60000 (38%)] lr: 2.6315789473684212e-05\tLoss: 0.162899\n",
      "Train Epoch: 38 [25600/60000 (43%)] lr: 2.6315789473684212e-05\tLoss: 0.126322\n",
      "Train Epoch: 38 [28160/60000 (47%)] lr: 2.6315789473684212e-05\tLoss: 0.110924\n",
      "Train Epoch: 38 [30720/60000 (51%)] lr: 2.6315789473684212e-05\tLoss: 0.183296\n",
      "Train Epoch: 38 [33280/60000 (55%)] lr: 2.6315789473684212e-05\tLoss: 0.210847\n",
      "Train Epoch: 38 [35840/60000 (60%)] lr: 2.6315789473684212e-05\tLoss: 0.141021\n",
      "Train Epoch: 38 [38400/60000 (64%)] lr: 2.6315789473684212e-05\tLoss: 0.142250\n",
      "Train Epoch: 38 [40960/60000 (68%)] lr: 2.6315789473684212e-05\tLoss: 0.240859\n",
      "Train Epoch: 38 [43520/60000 (72%)] lr: 2.6315789473684212e-05\tLoss: 0.162218\n",
      "Train Epoch: 38 [46080/60000 (77%)] lr: 2.6315789473684212e-05\tLoss: 0.193586\n",
      "Train Epoch: 38 [48640/60000 (81%)] lr: 2.6315789473684212e-05\tLoss: 0.194321\n",
      "Train Epoch: 38 [51200/60000 (85%)] lr: 2.6315789473684212e-05\tLoss: 0.166703\n",
      "Train Epoch: 38 [53760/60000 (89%)] lr: 2.6315789473684212e-05\tLoss: 0.147560\n",
      "Train Epoch: 38 [56320/60000 (94%)] lr: 2.6315789473684212e-05\tLoss: 0.190865\n",
      "Train Epoch: 38 [58880/60000 (98%)] lr: 2.6315789473684212e-05\tLoss: 0.182589\n",
      "Finish 38 epoch(s). Epoch loss: 0.1674253873051481.\n",
      "Train Epoch: 39 [0/60000 (0%)] lr: 2.5641025641025643e-05\tLoss: 0.177365\n",
      "Train Epoch: 39 [2560/60000 (4%)] lr: 2.5641025641025643e-05\tLoss: 0.120782\n",
      "Train Epoch: 39 [5120/60000 (9%)] lr: 2.5641025641025643e-05\tLoss: 0.147259\n",
      "Train Epoch: 39 [7680/60000 (13%)] lr: 2.5641025641025643e-05\tLoss: 0.166146\n",
      "Train Epoch: 39 [10240/60000 (17%)] lr: 2.5641025641025643e-05\tLoss: 0.211990\n",
      "Train Epoch: 39 [12800/60000 (21%)] lr: 2.5641025641025643e-05\tLoss: 0.153801\n",
      "Train Epoch: 39 [15360/60000 (26%)] lr: 2.5641025641025643e-05\tLoss: 0.202918\n",
      "Train Epoch: 39 [17920/60000 (30%)] lr: 2.5641025641025643e-05\tLoss: 0.166110\n",
      "Train Epoch: 39 [20480/60000 (34%)] lr: 2.5641025641025643e-05\tLoss: 0.161248\n",
      "Train Epoch: 39 [23040/60000 (38%)] lr: 2.5641025641025643e-05\tLoss: 0.161891\n",
      "Train Epoch: 39 [25600/60000 (43%)] lr: 2.5641025641025643e-05\tLoss: 0.138556\n",
      "Train Epoch: 39 [28160/60000 (47%)] lr: 2.5641025641025643e-05\tLoss: 0.108316\n",
      "Train Epoch: 39 [30720/60000 (51%)] lr: 2.5641025641025643e-05\tLoss: 0.151088\n",
      "Train Epoch: 39 [33280/60000 (55%)] lr: 2.5641025641025643e-05\tLoss: 0.202283\n",
      "Train Epoch: 39 [35840/60000 (60%)] lr: 2.5641025641025643e-05\tLoss: 0.100693\n",
      "Train Epoch: 39 [38400/60000 (64%)] lr: 2.5641025641025643e-05\tLoss: 0.161103\n",
      "Train Epoch: 39 [40960/60000 (68%)] lr: 2.5641025641025643e-05\tLoss: 0.211601\n",
      "Train Epoch: 39 [43520/60000 (72%)] lr: 2.5641025641025643e-05\tLoss: 0.220643\n",
      "Train Epoch: 39 [46080/60000 (77%)] lr: 2.5641025641025643e-05\tLoss: 0.192414\n",
      "Train Epoch: 39 [48640/60000 (81%)] lr: 2.5641025641025643e-05\tLoss: 0.273226\n",
      "Train Epoch: 39 [51200/60000 (85%)] lr: 2.5641025641025643e-05\tLoss: 0.253943\n",
      "Train Epoch: 39 [53760/60000 (89%)] lr: 2.5641025641025643e-05\tLoss: 0.113472\n",
      "Train Epoch: 39 [56320/60000 (94%)] lr: 2.5641025641025643e-05\tLoss: 0.203858\n",
      "Train Epoch: 39 [58880/60000 (98%)] lr: 2.5641025641025643e-05\tLoss: 0.170448\n",
      "Finish 39 epoch(s). Epoch loss: 0.17030429427928112.\n",
      "Train Epoch: 40 [0/60000 (0%)] lr: 2.5e-05\tLoss: 0.153359\n",
      "Train Epoch: 40 [2560/60000 (4%)] lr: 2.5e-05\tLoss: 0.182331\n",
      "Train Epoch: 40 [5120/60000 (9%)] lr: 2.5e-05\tLoss: 0.122728\n",
      "Train Epoch: 40 [7680/60000 (13%)] lr: 2.5e-05\tLoss: 0.217693\n",
      "Train Epoch: 40 [10240/60000 (17%)] lr: 2.5e-05\tLoss: 0.149150\n",
      "Train Epoch: 40 [12800/60000 (21%)] lr: 2.5e-05\tLoss: 0.186462\n",
      "Train Epoch: 40 [15360/60000 (26%)] lr: 2.5e-05\tLoss: 0.175511\n",
      "Train Epoch: 40 [17920/60000 (30%)] lr: 2.5e-05\tLoss: 0.115515\n",
      "Train Epoch: 40 [20480/60000 (34%)] lr: 2.5e-05\tLoss: 0.175649\n",
      "Train Epoch: 40 [23040/60000 (38%)] lr: 2.5e-05\tLoss: 0.143323\n",
      "Train Epoch: 40 [25600/60000 (43%)] lr: 2.5e-05\tLoss: 0.154710\n",
      "Train Epoch: 40 [28160/60000 (47%)] lr: 2.5e-05\tLoss: 0.195852\n",
      "Train Epoch: 40 [30720/60000 (51%)] lr: 2.5e-05\tLoss: 0.141808\n",
      "Train Epoch: 40 [33280/60000 (55%)] lr: 2.5e-05\tLoss: 0.234115\n",
      "Train Epoch: 40 [35840/60000 (60%)] lr: 2.5e-05\tLoss: 0.183478\n",
      "Train Epoch: 40 [38400/60000 (64%)] lr: 2.5e-05\tLoss: 0.143378\n",
      "Train Epoch: 40 [40960/60000 (68%)] lr: 2.5e-05\tLoss: 0.188715\n",
      "Train Epoch: 40 [43520/60000 (72%)] lr: 2.5e-05\tLoss: 0.182766\n",
      "Train Epoch: 40 [46080/60000 (77%)] lr: 2.5e-05\tLoss: 0.163413\n",
      "Train Epoch: 40 [48640/60000 (81%)] lr: 2.5e-05\tLoss: 0.126873\n",
      "Train Epoch: 40 [51200/60000 (85%)] lr: 2.5e-05\tLoss: 0.132458\n",
      "Train Epoch: 40 [53760/60000 (89%)] lr: 2.5e-05\tLoss: 0.176307\n",
      "Train Epoch: 40 [56320/60000 (94%)] lr: 2.5e-05\tLoss: 0.183970\n",
      "Train Epoch: 40 [58880/60000 (98%)] lr: 2.5e-05\tLoss: 0.160636\n",
      "Finish 40 epoch(s). Epoch loss: 0.17096403735749266.\n",
      "Train Epoch: 41 [0/60000 (0%)] lr: 2.4390243902439026e-05\tLoss: 0.142071\n",
      "Train Epoch: 41 [2560/60000 (4%)] lr: 2.4390243902439026e-05\tLoss: 0.216185\n",
      "Train Epoch: 41 [5120/60000 (9%)] lr: 2.4390243902439026e-05\tLoss: 0.139664\n",
      "Train Epoch: 41 [7680/60000 (13%)] lr: 2.4390243902439026e-05\tLoss: 0.182865\n",
      "Train Epoch: 41 [10240/60000 (17%)] lr: 2.4390243902439026e-05\tLoss: 0.151698\n",
      "Train Epoch: 41 [12800/60000 (21%)] lr: 2.4390243902439026e-05\tLoss: 0.151029\n",
      "Train Epoch: 41 [15360/60000 (26%)] lr: 2.4390243902439026e-05\tLoss: 0.094440\n",
      "Train Epoch: 41 [17920/60000 (30%)] lr: 2.4390243902439026e-05\tLoss: 0.123323\n",
      "Train Epoch: 41 [20480/60000 (34%)] lr: 2.4390243902439026e-05\tLoss: 0.190547\n",
      "Train Epoch: 41 [23040/60000 (38%)] lr: 2.4390243902439026e-05\tLoss: 0.171006\n",
      "Train Epoch: 41 [25600/60000 (43%)] lr: 2.4390243902439026e-05\tLoss: 0.129076\n",
      "Train Epoch: 41 [28160/60000 (47%)] lr: 2.4390243902439026e-05\tLoss: 0.191956\n",
      "Train Epoch: 41 [30720/60000 (51%)] lr: 2.4390243902439026e-05\tLoss: 0.167576\n",
      "Train Epoch: 41 [33280/60000 (55%)] lr: 2.4390243902439026e-05\tLoss: 0.158518\n",
      "Train Epoch: 41 [35840/60000 (60%)] lr: 2.4390243902439026e-05\tLoss: 0.231311\n",
      "Train Epoch: 41 [38400/60000 (64%)] lr: 2.4390243902439026e-05\tLoss: 0.166726\n",
      "Train Epoch: 41 [40960/60000 (68%)] lr: 2.4390243902439026e-05\tLoss: 0.198643\n",
      "Train Epoch: 41 [43520/60000 (72%)] lr: 2.4390243902439026e-05\tLoss: 0.185550\n",
      "Train Epoch: 41 [46080/60000 (77%)] lr: 2.4390243902439026e-05\tLoss: 0.205856\n",
      "Train Epoch: 41 [48640/60000 (81%)] lr: 2.4390243902439026e-05\tLoss: 0.228127\n",
      "Train Epoch: 41 [51200/60000 (85%)] lr: 2.4390243902439026e-05\tLoss: 0.161081\n",
      "Train Epoch: 41 [53760/60000 (89%)] lr: 2.4390243902439026e-05\tLoss: 0.236809\n",
      "Train Epoch: 41 [56320/60000 (94%)] lr: 2.4390243902439026e-05\tLoss: 0.136728\n",
      "Train Epoch: 41 [58880/60000 (98%)] lr: 2.4390243902439026e-05\tLoss: 0.158157\n",
      "Finish 41 epoch(s). Epoch loss: 0.16907058180646695.\n",
      "Train Epoch: 42 [0/60000 (0%)] lr: 2.380952380952381e-05\tLoss: 0.210717\n",
      "Train Epoch: 42 [2560/60000 (4%)] lr: 2.380952380952381e-05\tLoss: 0.135422\n",
      "Train Epoch: 42 [5120/60000 (9%)] lr: 2.380952380952381e-05\tLoss: 0.134025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42 [7680/60000 (13%)] lr: 2.380952380952381e-05\tLoss: 0.193355\n",
      "Train Epoch: 42 [10240/60000 (17%)] lr: 2.380952380952381e-05\tLoss: 0.153915\n",
      "Train Epoch: 42 [12800/60000 (21%)] lr: 2.380952380952381e-05\tLoss: 0.199441\n",
      "Train Epoch: 42 [15360/60000 (26%)] lr: 2.380952380952381e-05\tLoss: 0.129447\n",
      "Train Epoch: 42 [17920/60000 (30%)] lr: 2.380952380952381e-05\tLoss: 0.145854\n",
      "Train Epoch: 42 [20480/60000 (34%)] lr: 2.380952380952381e-05\tLoss: 0.123414\n",
      "Train Epoch: 42 [23040/60000 (38%)] lr: 2.380952380952381e-05\tLoss: 0.170565\n",
      "Train Epoch: 42 [25600/60000 (43%)] lr: 2.380952380952381e-05\tLoss: 0.078681\n",
      "Train Epoch: 42 [28160/60000 (47%)] lr: 2.380952380952381e-05\tLoss: 0.131439\n",
      "Train Epoch: 42 [30720/60000 (51%)] lr: 2.380952380952381e-05\tLoss: 0.146514\n",
      "Train Epoch: 42 [33280/60000 (55%)] lr: 2.380952380952381e-05\tLoss: 0.122262\n",
      "Train Epoch: 42 [35840/60000 (60%)] lr: 2.380952380952381e-05\tLoss: 0.239674\n",
      "Train Epoch: 42 [38400/60000 (64%)] lr: 2.380952380952381e-05\tLoss: 0.155922\n",
      "Train Epoch: 42 [40960/60000 (68%)] lr: 2.380952380952381e-05\tLoss: 0.160594\n",
      "Train Epoch: 42 [43520/60000 (72%)] lr: 2.380952380952381e-05\tLoss: 0.204292\n",
      "Train Epoch: 42 [46080/60000 (77%)] lr: 2.380952380952381e-05\tLoss: 0.243691\n",
      "Train Epoch: 42 [48640/60000 (81%)] lr: 2.380952380952381e-05\tLoss: 0.138208\n",
      "Train Epoch: 42 [51200/60000 (85%)] lr: 2.380952380952381e-05\tLoss: 0.155728\n",
      "Train Epoch: 42 [53760/60000 (89%)] lr: 2.380952380952381e-05\tLoss: 0.168977\n",
      "Train Epoch: 42 [56320/60000 (94%)] lr: 2.380952380952381e-05\tLoss: 0.118755\n",
      "Train Epoch: 42 [58880/60000 (98%)] lr: 2.380952380952381e-05\tLoss: 0.261274\n",
      "Finish 42 epoch(s). Epoch loss: 0.16913738412425874.\n",
      "Train Epoch: 43 [0/60000 (0%)] lr: 2.3255813953488374e-05\tLoss: 0.134464\n",
      "Train Epoch: 43 [2560/60000 (4%)] lr: 2.3255813953488374e-05\tLoss: 0.159582\n",
      "Train Epoch: 43 [5120/60000 (9%)] lr: 2.3255813953488374e-05\tLoss: 0.140663\n",
      "Train Epoch: 43 [7680/60000 (13%)] lr: 2.3255813953488374e-05\tLoss: 0.214982\n",
      "Train Epoch: 43 [10240/60000 (17%)] lr: 2.3255813953488374e-05\tLoss: 0.160605\n",
      "Train Epoch: 43 [12800/60000 (21%)] lr: 2.3255813953488374e-05\tLoss: 0.161521\n",
      "Train Epoch: 43 [15360/60000 (26%)] lr: 2.3255813953488374e-05\tLoss: 0.262770\n",
      "Train Epoch: 43 [17920/60000 (30%)] lr: 2.3255813953488374e-05\tLoss: 0.152704\n",
      "Train Epoch: 43 [20480/60000 (34%)] lr: 2.3255813953488374e-05\tLoss: 0.180707\n",
      "Train Epoch: 43 [23040/60000 (38%)] lr: 2.3255813953488374e-05\tLoss: 0.165285\n",
      "Train Epoch: 43 [25600/60000 (43%)] lr: 2.3255813953488374e-05\tLoss: 0.213311\n",
      "Train Epoch: 43 [28160/60000 (47%)] lr: 2.3255813953488374e-05\tLoss: 0.168986\n",
      "Train Epoch: 43 [30720/60000 (51%)] lr: 2.3255813953488374e-05\tLoss: 0.187458\n",
      "Train Epoch: 43 [33280/60000 (55%)] lr: 2.3255813953488374e-05\tLoss: 0.217072\n",
      "Train Epoch: 43 [35840/60000 (60%)] lr: 2.3255813953488374e-05\tLoss: 0.106638\n",
      "Train Epoch: 43 [38400/60000 (64%)] lr: 2.3255813953488374e-05\tLoss: 0.182560\n",
      "Train Epoch: 43 [40960/60000 (68%)] lr: 2.3255813953488374e-05\tLoss: 0.159011\n",
      "Train Epoch: 43 [43520/60000 (72%)] lr: 2.3255813953488374e-05\tLoss: 0.247298\n",
      "Train Epoch: 43 [46080/60000 (77%)] lr: 2.3255813953488374e-05\tLoss: 0.215827\n",
      "Train Epoch: 43 [48640/60000 (81%)] lr: 2.3255813953488374e-05\tLoss: 0.249761\n",
      "Train Epoch: 43 [51200/60000 (85%)] lr: 2.3255813953488374e-05\tLoss: 0.112464\n",
      "Train Epoch: 43 [53760/60000 (89%)] lr: 2.3255813953488374e-05\tLoss: 0.091797\n",
      "Train Epoch: 43 [56320/60000 (94%)] lr: 2.3255813953488374e-05\tLoss: 0.132212\n",
      "Train Epoch: 43 [58880/60000 (98%)] lr: 2.3255813953488374e-05\tLoss: 0.096435\n",
      "Finish 43 epoch(s). Epoch loss: 0.16694270508720518.\n",
      "Train Epoch: 44 [0/60000 (0%)] lr: 2.272727272727273e-05\tLoss: 0.199837\n",
      "Train Epoch: 44 [2560/60000 (4%)] lr: 2.272727272727273e-05\tLoss: 0.173636\n",
      "Train Epoch: 44 [5120/60000 (9%)] lr: 2.272727272727273e-05\tLoss: 0.179333\n",
      "Train Epoch: 44 [7680/60000 (13%)] lr: 2.272727272727273e-05\tLoss: 0.175079\n",
      "Train Epoch: 44 [10240/60000 (17%)] lr: 2.272727272727273e-05\tLoss: 0.128183\n",
      "Train Epoch: 44 [12800/60000 (21%)] lr: 2.272727272727273e-05\tLoss: 0.170250\n",
      "Train Epoch: 44 [15360/60000 (26%)] lr: 2.272727272727273e-05\tLoss: 0.189940\n",
      "Train Epoch: 44 [17920/60000 (30%)] lr: 2.272727272727273e-05\tLoss: 0.251635\n",
      "Train Epoch: 44 [20480/60000 (34%)] lr: 2.272727272727273e-05\tLoss: 0.151322\n",
      "Train Epoch: 44 [23040/60000 (38%)] lr: 2.272727272727273e-05\tLoss: 0.174412\n",
      "Train Epoch: 44 [25600/60000 (43%)] lr: 2.272727272727273e-05\tLoss: 0.174882\n",
      "Train Epoch: 44 [28160/60000 (47%)] lr: 2.272727272727273e-05\tLoss: 0.147376\n",
      "Train Epoch: 44 [30720/60000 (51%)] lr: 2.272727272727273e-05\tLoss: 0.197813\n",
      "Train Epoch: 44 [33280/60000 (55%)] lr: 2.272727272727273e-05\tLoss: 0.114832\n",
      "Train Epoch: 44 [35840/60000 (60%)] lr: 2.272727272727273e-05\tLoss: 0.184024\n",
      "Train Epoch: 44 [38400/60000 (64%)] lr: 2.272727272727273e-05\tLoss: 0.194519\n",
      "Train Epoch: 44 [40960/60000 (68%)] lr: 2.272727272727273e-05\tLoss: 0.212173\n",
      "Train Epoch: 44 [43520/60000 (72%)] lr: 2.272727272727273e-05\tLoss: 0.122195\n",
      "Train Epoch: 44 [46080/60000 (77%)] lr: 2.272727272727273e-05\tLoss: 0.163443\n",
      "Train Epoch: 44 [48640/60000 (81%)] lr: 2.272727272727273e-05\tLoss: 0.151577\n",
      "Train Epoch: 44 [51200/60000 (85%)] lr: 2.272727272727273e-05\tLoss: 0.149294\n",
      "Train Epoch: 44 [53760/60000 (89%)] lr: 2.272727272727273e-05\tLoss: 0.131113\n",
      "Train Epoch: 44 [56320/60000 (94%)] lr: 2.272727272727273e-05\tLoss: 0.165356\n",
      "Train Epoch: 44 [58880/60000 (98%)] lr: 2.272727272727273e-05\tLoss: 0.193950\n",
      "Finish 44 epoch(s). Epoch loss: 0.16535195221609258.\n",
      "Train Epoch: 45 [0/60000 (0%)] lr: 2.2222222222222223e-05\tLoss: 0.204636\n",
      "Train Epoch: 45 [2560/60000 (4%)] lr: 2.2222222222222223e-05\tLoss: 0.232685\n",
      "Train Epoch: 45 [5120/60000 (9%)] lr: 2.2222222222222223e-05\tLoss: 0.148816\n",
      "Train Epoch: 45 [7680/60000 (13%)] lr: 2.2222222222222223e-05\tLoss: 0.223383\n",
      "Train Epoch: 45 [10240/60000 (17%)] lr: 2.2222222222222223e-05\tLoss: 0.162000\n",
      "Train Epoch: 45 [12800/60000 (21%)] lr: 2.2222222222222223e-05\tLoss: 0.139001\n",
      "Train Epoch: 45 [15360/60000 (26%)] lr: 2.2222222222222223e-05\tLoss: 0.165618\n",
      "Train Epoch: 45 [17920/60000 (30%)] lr: 2.2222222222222223e-05\tLoss: 0.136571\n",
      "Train Epoch: 45 [20480/60000 (34%)] lr: 2.2222222222222223e-05\tLoss: 0.158542\n",
      "Train Epoch: 45 [23040/60000 (38%)] lr: 2.2222222222222223e-05\tLoss: 0.134852\n",
      "Train Epoch: 45 [25600/60000 (43%)] lr: 2.2222222222222223e-05\tLoss: 0.178198\n",
      "Train Epoch: 45 [28160/60000 (47%)] lr: 2.2222222222222223e-05\tLoss: 0.112642\n",
      "Train Epoch: 45 [30720/60000 (51%)] lr: 2.2222222222222223e-05\tLoss: 0.219668\n",
      "Train Epoch: 45 [33280/60000 (55%)] lr: 2.2222222222222223e-05\tLoss: 0.155392\n",
      "Train Epoch: 45 [35840/60000 (60%)] lr: 2.2222222222222223e-05\tLoss: 0.138042\n",
      "Train Epoch: 45 [38400/60000 (64%)] lr: 2.2222222222222223e-05\tLoss: 0.220946\n",
      "Train Epoch: 45 [40960/60000 (68%)] lr: 2.2222222222222223e-05\tLoss: 0.119098\n",
      "Train Epoch: 45 [43520/60000 (72%)] lr: 2.2222222222222223e-05\tLoss: 0.194500\n",
      "Train Epoch: 45 [46080/60000 (77%)] lr: 2.2222222222222223e-05\tLoss: 0.176342\n",
      "Train Epoch: 45 [48640/60000 (81%)] lr: 2.2222222222222223e-05\tLoss: 0.192090\n",
      "Train Epoch: 45 [51200/60000 (85%)] lr: 2.2222222222222223e-05\tLoss: 0.205404\n",
      "Train Epoch: 45 [53760/60000 (89%)] lr: 2.2222222222222223e-05\tLoss: 0.087586\n",
      "Train Epoch: 45 [56320/60000 (94%)] lr: 2.2222222222222223e-05\tLoss: 0.141988\n",
      "Train Epoch: 45 [58880/60000 (98%)] lr: 2.2222222222222223e-05\tLoss: 0.196148\n",
      "Finish 45 epoch(s). Epoch loss: 0.16727739223774443.\n",
      "Train Epoch: 46 [0/60000 (0%)] lr: 2.173913043478261e-05\tLoss: 0.133976\n",
      "Train Epoch: 46 [2560/60000 (4%)] lr: 2.173913043478261e-05\tLoss: 0.151053\n",
      "Train Epoch: 46 [5120/60000 (9%)] lr: 2.173913043478261e-05\tLoss: 0.218141\n",
      "Train Epoch: 46 [7680/60000 (13%)] lr: 2.173913043478261e-05\tLoss: 0.143762\n",
      "Train Epoch: 46 [10240/60000 (17%)] lr: 2.173913043478261e-05\tLoss: 0.236508\n",
      "Train Epoch: 46 [12800/60000 (21%)] lr: 2.173913043478261e-05\tLoss: 0.176917\n",
      "Train Epoch: 46 [15360/60000 (26%)] lr: 2.173913043478261e-05\tLoss: 0.180514\n",
      "Train Epoch: 46 [17920/60000 (30%)] lr: 2.173913043478261e-05\tLoss: 0.149593\n",
      "Train Epoch: 46 [20480/60000 (34%)] lr: 2.173913043478261e-05\tLoss: 0.139662\n",
      "Train Epoch: 46 [23040/60000 (38%)] lr: 2.173913043478261e-05\tLoss: 0.150024\n",
      "Train Epoch: 46 [25600/60000 (43%)] lr: 2.173913043478261e-05\tLoss: 0.231426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [28160/60000 (47%)] lr: 2.173913043478261e-05\tLoss: 0.256208\n",
      "Train Epoch: 46 [30720/60000 (51%)] lr: 2.173913043478261e-05\tLoss: 0.159035\n",
      "Train Epoch: 46 [33280/60000 (55%)] lr: 2.173913043478261e-05\tLoss: 0.173686\n",
      "Train Epoch: 46 [35840/60000 (60%)] lr: 2.173913043478261e-05\tLoss: 0.158871\n",
      "Train Epoch: 46 [38400/60000 (64%)] lr: 2.173913043478261e-05\tLoss: 0.144038\n",
      "Train Epoch: 46 [40960/60000 (68%)] lr: 2.173913043478261e-05\tLoss: 0.221868\n",
      "Train Epoch: 46 [43520/60000 (72%)] lr: 2.173913043478261e-05\tLoss: 0.127164\n",
      "Train Epoch: 46 [46080/60000 (77%)] lr: 2.173913043478261e-05\tLoss: 0.224199\n",
      "Train Epoch: 46 [48640/60000 (81%)] lr: 2.173913043478261e-05\tLoss: 0.116917\n",
      "Train Epoch: 46 [51200/60000 (85%)] lr: 2.173913043478261e-05\tLoss: 0.185570\n",
      "Train Epoch: 46 [53760/60000 (89%)] lr: 2.173913043478261e-05\tLoss: 0.207947\n",
      "Train Epoch: 46 [56320/60000 (94%)] lr: 2.173913043478261e-05\tLoss: 0.149405\n",
      "Train Epoch: 46 [58880/60000 (98%)] lr: 2.173913043478261e-05\tLoss: 0.150271\n",
      "Finish 46 epoch(s). Epoch loss: 0.1675485074520111.\n",
      "Train Epoch: 47 [0/60000 (0%)] lr: 2.1276595744680852e-05\tLoss: 0.255453\n",
      "Train Epoch: 47 [2560/60000 (4%)] lr: 2.1276595744680852e-05\tLoss: 0.179676\n",
      "Train Epoch: 47 [5120/60000 (9%)] lr: 2.1276595744680852e-05\tLoss: 0.179725\n",
      "Train Epoch: 47 [7680/60000 (13%)] lr: 2.1276595744680852e-05\tLoss: 0.154915\n",
      "Train Epoch: 47 [10240/60000 (17%)] lr: 2.1276595744680852e-05\tLoss: 0.155059\n",
      "Train Epoch: 47 [12800/60000 (21%)] lr: 2.1276595744680852e-05\tLoss: 0.154234\n",
      "Train Epoch: 47 [15360/60000 (26%)] lr: 2.1276595744680852e-05\tLoss: 0.148146\n",
      "Train Epoch: 47 [17920/60000 (30%)] lr: 2.1276595744680852e-05\tLoss: 0.220089\n",
      "Train Epoch: 47 [20480/60000 (34%)] lr: 2.1276595744680852e-05\tLoss: 0.209453\n",
      "Train Epoch: 47 [23040/60000 (38%)] lr: 2.1276595744680852e-05\tLoss: 0.216445\n",
      "Train Epoch: 47 [25600/60000 (43%)] lr: 2.1276595744680852e-05\tLoss: 0.171986\n",
      "Train Epoch: 47 [28160/60000 (47%)] lr: 2.1276595744680852e-05\tLoss: 0.118190\n",
      "Train Epoch: 47 [30720/60000 (51%)] lr: 2.1276595744680852e-05\tLoss: 0.198429\n",
      "Train Epoch: 47 [33280/60000 (55%)] lr: 2.1276595744680852e-05\tLoss: 0.110540\n",
      "Train Epoch: 47 [35840/60000 (60%)] lr: 2.1276595744680852e-05\tLoss: 0.112033\n",
      "Train Epoch: 47 [38400/60000 (64%)] lr: 2.1276595744680852e-05\tLoss: 0.177397\n",
      "Train Epoch: 47 [40960/60000 (68%)] lr: 2.1276595744680852e-05\tLoss: 0.235365\n",
      "Train Epoch: 47 [43520/60000 (72%)] lr: 2.1276595744680852e-05\tLoss: 0.145590\n",
      "Train Epoch: 47 [46080/60000 (77%)] lr: 2.1276595744680852e-05\tLoss: 0.216408\n",
      "Train Epoch: 47 [48640/60000 (81%)] lr: 2.1276595744680852e-05\tLoss: 0.117337\n",
      "Train Epoch: 47 [51200/60000 (85%)] lr: 2.1276595744680852e-05\tLoss: 0.207289\n",
      "Train Epoch: 47 [53760/60000 (89%)] lr: 2.1276595744680852e-05\tLoss: 0.157117\n",
      "Train Epoch: 47 [56320/60000 (94%)] lr: 2.1276595744680852e-05\tLoss: 0.204069\n",
      "Train Epoch: 47 [58880/60000 (98%)] lr: 2.1276595744680852e-05\tLoss: 0.161009\n",
      "Finish 47 epoch(s). Epoch loss: 0.16539679331348298.\n",
      "Train Epoch: 48 [0/60000 (0%)] lr: 2.0833333333333333e-05\tLoss: 0.160835\n",
      "Train Epoch: 48 [2560/60000 (4%)] lr: 2.0833333333333333e-05\tLoss: 0.186050\n",
      "Train Epoch: 48 [5120/60000 (9%)] lr: 2.0833333333333333e-05\tLoss: 0.127869\n",
      "Train Epoch: 48 [7680/60000 (13%)] lr: 2.0833333333333333e-05\tLoss: 0.180561\n",
      "Train Epoch: 48 [10240/60000 (17%)] lr: 2.0833333333333333e-05\tLoss: 0.201915\n",
      "Train Epoch: 48 [12800/60000 (21%)] lr: 2.0833333333333333e-05\tLoss: 0.217612\n",
      "Train Epoch: 48 [15360/60000 (26%)] lr: 2.0833333333333333e-05\tLoss: 0.188535\n",
      "Train Epoch: 48 [17920/60000 (30%)] lr: 2.0833333333333333e-05\tLoss: 0.069912\n",
      "Train Epoch: 48 [20480/60000 (34%)] lr: 2.0833333333333333e-05\tLoss: 0.095290\n",
      "Train Epoch: 48 [23040/60000 (38%)] lr: 2.0833333333333333e-05\tLoss: 0.100704\n",
      "Train Epoch: 48 [25600/60000 (43%)] lr: 2.0833333333333333e-05\tLoss: 0.174377\n",
      "Train Epoch: 48 [28160/60000 (47%)] lr: 2.0833333333333333e-05\tLoss: 0.203854\n",
      "Train Epoch: 48 [30720/60000 (51%)] lr: 2.0833333333333333e-05\tLoss: 0.226627\n",
      "Train Epoch: 48 [33280/60000 (55%)] lr: 2.0833333333333333e-05\tLoss: 0.104259\n",
      "Train Epoch: 48 [35840/60000 (60%)] lr: 2.0833333333333333e-05\tLoss: 0.195468\n",
      "Train Epoch: 48 [38400/60000 (64%)] lr: 2.0833333333333333e-05\tLoss: 0.193505\n",
      "Train Epoch: 48 [40960/60000 (68%)] lr: 2.0833333333333333e-05\tLoss: 0.113768\n",
      "Train Epoch: 48 [43520/60000 (72%)] lr: 2.0833333333333333e-05\tLoss: 0.168750\n",
      "Train Epoch: 48 [46080/60000 (77%)] lr: 2.0833333333333333e-05\tLoss: 0.156264\n",
      "Train Epoch: 48 [48640/60000 (81%)] lr: 2.0833333333333333e-05\tLoss: 0.123779\n",
      "Train Epoch: 48 [51200/60000 (85%)] lr: 2.0833333333333333e-05\tLoss: 0.157874\n",
      "Train Epoch: 48 [53760/60000 (89%)] lr: 2.0833333333333333e-05\tLoss: 0.073660\n",
      "Train Epoch: 48 [56320/60000 (94%)] lr: 2.0833333333333333e-05\tLoss: 0.147297\n",
      "Train Epoch: 48 [58880/60000 (98%)] lr: 2.0833333333333333e-05\tLoss: 0.256076\n",
      "Finish 48 epoch(s). Epoch loss: 0.16551633832302498.\n",
      "Train Epoch: 49 [0/60000 (0%)] lr: 2.0408163265306123e-05\tLoss: 0.185904\n",
      "Train Epoch: 49 [2560/60000 (4%)] lr: 2.0408163265306123e-05\tLoss: 0.126773\n",
      "Train Epoch: 49 [5120/60000 (9%)] lr: 2.0408163265306123e-05\tLoss: 0.125050\n",
      "Train Epoch: 49 [7680/60000 (13%)] lr: 2.0408163265306123e-05\tLoss: 0.171094\n",
      "Train Epoch: 49 [10240/60000 (17%)] lr: 2.0408163265306123e-05\tLoss: 0.167378\n",
      "Train Epoch: 49 [12800/60000 (21%)] lr: 2.0408163265306123e-05\tLoss: 0.185063\n",
      "Train Epoch: 49 [15360/60000 (26%)] lr: 2.0408163265306123e-05\tLoss: 0.248990\n",
      "Train Epoch: 49 [17920/60000 (30%)] lr: 2.0408163265306123e-05\tLoss: 0.187349\n",
      "Train Epoch: 49 [20480/60000 (34%)] lr: 2.0408163265306123e-05\tLoss: 0.124824\n",
      "Train Epoch: 49 [23040/60000 (38%)] lr: 2.0408163265306123e-05\tLoss: 0.222835\n",
      "Train Epoch: 49 [25600/60000 (43%)] lr: 2.0408163265306123e-05\tLoss: 0.277013\n",
      "Train Epoch: 49 [28160/60000 (47%)] lr: 2.0408163265306123e-05\tLoss: 0.191090\n",
      "Train Epoch: 49 [30720/60000 (51%)] lr: 2.0408163265306123e-05\tLoss: 0.160045\n",
      "Train Epoch: 49 [33280/60000 (55%)] lr: 2.0408163265306123e-05\tLoss: 0.177306\n",
      "Train Epoch: 49 [35840/60000 (60%)] lr: 2.0408163265306123e-05\tLoss: 0.163611\n",
      "Train Epoch: 49 [38400/60000 (64%)] lr: 2.0408163265306123e-05\tLoss: 0.157788\n",
      "Train Epoch: 49 [40960/60000 (68%)] lr: 2.0408163265306123e-05\tLoss: 0.142896\n",
      "Train Epoch: 49 [43520/60000 (72%)] lr: 2.0408163265306123e-05\tLoss: 0.153091\n",
      "Train Epoch: 49 [46080/60000 (77%)] lr: 2.0408163265306123e-05\tLoss: 0.160509\n",
      "Train Epoch: 49 [48640/60000 (81%)] lr: 2.0408163265306123e-05\tLoss: 0.201100\n",
      "Train Epoch: 49 [51200/60000 (85%)] lr: 2.0408163265306123e-05\tLoss: 0.158203\n",
      "Train Epoch: 49 [53760/60000 (89%)] lr: 2.0408163265306123e-05\tLoss: 0.181913\n",
      "Train Epoch: 49 [56320/60000 (94%)] lr: 2.0408163265306123e-05\tLoss: 0.179026\n",
      "Train Epoch: 49 [58880/60000 (98%)] lr: 2.0408163265306123e-05\tLoss: 0.178348\n",
      "Finish 49 epoch(s). Epoch loss: 0.1687031683452586.\n",
      "Train Epoch: 50 [0/60000 (0%)] lr: 2e-05\tLoss: 0.164659\n",
      "Train Epoch: 50 [2560/60000 (4%)] lr: 2e-05\tLoss: 0.235564\n",
      "Train Epoch: 50 [5120/60000 (9%)] lr: 2e-05\tLoss: 0.156446\n",
      "Train Epoch: 50 [7680/60000 (13%)] lr: 2e-05\tLoss: 0.147991\n",
      "Train Epoch: 50 [10240/60000 (17%)] lr: 2e-05\tLoss: 0.261787\n",
      "Train Epoch: 50 [12800/60000 (21%)] lr: 2e-05\tLoss: 0.217876\n",
      "Train Epoch: 50 [15360/60000 (26%)] lr: 2e-05\tLoss: 0.162248\n",
      "Train Epoch: 50 [17920/60000 (30%)] lr: 2e-05\tLoss: 0.179770\n",
      "Train Epoch: 50 [20480/60000 (34%)] lr: 2e-05\tLoss: 0.140816\n",
      "Train Epoch: 50 [23040/60000 (38%)] lr: 2e-05\tLoss: 0.178554\n",
      "Train Epoch: 50 [25600/60000 (43%)] lr: 2e-05\tLoss: 0.179574\n",
      "Train Epoch: 50 [28160/60000 (47%)] lr: 2e-05\tLoss: 0.104493\n",
      "Train Epoch: 50 [30720/60000 (51%)] lr: 2e-05\tLoss: 0.251805\n",
      "Train Epoch: 50 [33280/60000 (55%)] lr: 2e-05\tLoss: 0.218425\n",
      "Train Epoch: 50 [35840/60000 (60%)] lr: 2e-05\tLoss: 0.194343\n",
      "Train Epoch: 50 [38400/60000 (64%)] lr: 2e-05\tLoss: 0.078986\n",
      "Train Epoch: 50 [40960/60000 (68%)] lr: 2e-05\tLoss: 0.226020\n",
      "Train Epoch: 50 [43520/60000 (72%)] lr: 2e-05\tLoss: 0.155817\n",
      "Train Epoch: 50 [46080/60000 (77%)] lr: 2e-05\tLoss: 0.119040\n",
      "Train Epoch: 50 [48640/60000 (81%)] lr: 2e-05\tLoss: 0.140873\n",
      "Train Epoch: 50 [51200/60000 (85%)] lr: 2e-05\tLoss: 0.122422\n",
      "Train Epoch: 50 [53760/60000 (89%)] lr: 2e-05\tLoss: 0.138112\n",
      "Train Epoch: 50 [56320/60000 (94%)] lr: 2e-05\tLoss: 0.218729\n",
      "Train Epoch: 50 [58880/60000 (98%)] lr: 2e-05\tLoss: 0.149758\n",
      "Finish 50 epoch(s). Epoch loss: 0.16640818807038854.\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_sobol.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_drop_Eoutput(T=100,criterion = nn.CrossEntropyLoss()):\n",
    "    model.train()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    for batch_idx, (xdata,ydata) in enumerate(test_loader):\n",
    "        batch_size=xdata.shape[0]\n",
    "        xdata=xdata.to(device).view(batch_size,-1)\n",
    "        ydata=ydata.to(device)\n",
    "        outputlist=[]\n",
    "        for i in range(T):\n",
    "            outputlist.append(torch.unsqueeze(model(xdata),0))\n",
    "        output_mean=torch.cat(outputlist,dim=0).mean(dim=0)\n",
    "        test_loss+=criterion(output_mean,ydata)\n",
    "        pred=output_mean.max(dim=1,keepdim=True)[1]\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(ydata)\n",
    "        correct+=pred.eq(ydata.view_as(pred)).cpu().sum()\n",
    "        if (batch_idx+1) % (int(len(test_loader)/10))==0:\n",
    "            print(\"Test finished {:.0f}%\".format(100. * (batch_idx+1) / len(test_loader)))\n",
    "    test_loss/=len(test_loader)\n",
    "    print('\\nMC Dropout Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    y_pred=torch.cat(y_pred,dim=0)\n",
    "    y_true=torch.cat(y_true,dim=0)\n",
    "    print(confusion_matrix(y_true,y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test finished 10%\n",
      "Test finished 20%\n",
      "Test finished 30%\n",
      "Test finished 40%\n",
      "Test finished 50%\n",
      "Test finished 60%\n",
      "Test finished 70%\n",
      "Test finished 80%\n",
      "Test finished 90%\n",
      "Test finished 100%\n",
      "\n",
      "MC Dropout Test set: Average loss: 0.0882, Accuracy: 9733/10000 (97.00%)\n",
      "\n",
      "[[ 972    0    1    1    0    0    2    1    3    0]\n",
      " [   0 1120    3    0    0    0    4    0    8    0]\n",
      " [   4    0 1004    2    3    0    2    9    8    0]\n",
      " [   0    0    5  981    0    7    0    7    7    3]\n",
      " [   1    0    2    0  950    0    7    0    2   20]\n",
      " [   2    0    0    9    2  864    7    2    4    2]\n",
      " [   5    3    0    1    4    3  936    0    6    0]\n",
      " [   2    8   13    2    1    0    0  989    0   13]\n",
      " [   4    0    2    8    3    3    3    2  943    6]\n",
      " [   4    6    0    6    7    2    0    4    6  974]]\n"
     ]
    }
   ],
   "source": [
    "test_drop_Eoutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices de sobol de premier ordre sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DropoutNetwork(input_dim,nclasses,p_drop)\n",
    "model.load_state_dict(torch.load(\"model_sobol.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset=datasets.MNIST('data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h86479/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f31c29d8e48>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# i-ème image et label\n",
    "i=10\n",
    "X=testDataset[i][0]\n",
    "y=torch.tensor(testDataset[i][1])\n",
    "plt.imshow(X.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul V[E[y\\z]]\n",
    "def i_Sobol_numerateur(X,y,i,monte_num=100,p=0.5):\n",
    "    model.eval()\n",
    "    X=X.view(-1)\n",
    "    X_expand=X.expand(monte_num,784)\n",
    "    \n",
    "    #calculate E[y\\z_i=0]\n",
    "    \n",
    "    Z=torch.bernoulli(p*torch.ones(monte_num,784))\n",
    "    Z[:,i]=0\n",
    "    X1=X_expand*Z\n",
    "    E_y_zi0=F.softmax(model(X1),dim=1).mean(dim=0)[y]\n",
    "    \n",
    "    #calculate E[y\\z_i=1]\n",
    "    Z=torch.bernoulli(p*torch.ones(monte_num,784))\n",
    "    Z[:,i]=1\n",
    "    X1=X_expand*Z\n",
    "    E_y_zi1=F.softmax(model(X1),dim=1).mean(dim=0)[y]\n",
    "    \n",
    "    \n",
    "    return p*(1-p)*E_y_zi0.item()**2+p*(1-p)*E_y_zi1.item()**2-2*p*(1-p)*E_y_zi1.item()*E_y_zi0.item() #\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sobol(X,y,monte_num=100,p=0.5):\n",
    "    X=X.view(-1)\n",
    "    X_expand=X.expand(monte_num,784)\n",
    "    \n",
    "    #calculate var(y)\n",
    "    Z=torch.bernoulli(p*torch.ones(monte_num,784))\n",
    "    \n",
    "    V_y=F.softmax(model(X_expand*Z),dim=1).var(dim=0)[y]\n",
    "    \n",
    "    if V_y==0:\n",
    "        print(\"V[y]=0\")\n",
    "        V_y=1\n",
    "    #calculate indice de sobol for each pixel\n",
    "    l_res=[]\n",
    "    for i in range(784):\n",
    "        print(\"pixel:\",i+1)\n",
    "        l_res.append(i_Sobol_numerateur(X,y,i,monte_num,p)/V_y)\n",
    "    return l_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel: 1\n",
      "pixel: 2\n",
      "pixel: 3\n",
      "pixel: 4\n",
      "pixel: 5\n",
      "pixel: 6\n",
      "pixel: 7\n",
      "pixel: 8\n",
      "pixel: 9\n",
      "pixel: 10\n",
      "pixel: 11\n",
      "pixel: 12\n",
      "pixel: 13\n",
      "pixel: 14\n",
      "pixel: 15\n",
      "pixel: 16\n",
      "pixel: 17\n",
      "pixel: 18\n",
      "pixel: 19\n",
      "pixel: 20\n",
      "pixel: 21\n",
      "pixel: 22\n",
      "pixel: 23\n",
      "pixel: 24\n",
      "pixel: 25\n",
      "pixel: 26\n",
      "pixel: 27\n",
      "pixel: 28\n",
      "pixel: 29\n",
      "pixel: 30\n",
      "pixel: 31\n",
      "pixel: 32\n",
      "pixel: 33\n",
      "pixel: 34\n",
      "pixel: 35\n",
      "pixel: 36\n",
      "pixel: 37\n",
      "pixel: 38\n",
      "pixel: 39\n",
      "pixel: 40\n",
      "pixel: 41\n",
      "pixel: 42\n",
      "pixel: 43\n",
      "pixel: 44\n",
      "pixel: 45\n",
      "pixel: 46\n",
      "pixel: 47\n",
      "pixel: 48\n",
      "pixel: 49\n",
      "pixel: 50\n",
      "pixel: 51\n",
      "pixel: 52\n",
      "pixel: 53\n",
      "pixel: 54\n",
      "pixel: 55\n",
      "pixel: 56\n",
      "pixel: 57\n",
      "pixel: 58\n",
      "pixel: 59\n",
      "pixel: 60\n",
      "pixel: 61\n",
      "pixel: 62\n",
      "pixel: 63\n",
      "pixel: 64\n",
      "pixel: 65\n",
      "pixel: 66\n",
      "pixel: 67\n",
      "pixel: 68\n",
      "pixel: 69\n",
      "pixel: 70\n",
      "pixel: 71\n",
      "pixel: 72\n",
      "pixel: 73\n",
      "pixel: 74\n",
      "pixel: 75\n",
      "pixel: 76\n",
      "pixel: 77\n",
      "pixel: 78\n",
      "pixel: 79\n",
      "pixel: 80\n",
      "pixel: 81\n",
      "pixel: 82\n",
      "pixel: 83\n",
      "pixel: 84\n",
      "pixel: 85\n",
      "pixel: 86\n",
      "pixel: 87\n",
      "pixel: 88\n",
      "pixel: 89\n",
      "pixel: 90\n",
      "pixel: 91\n",
      "pixel: 92\n",
      "pixel: 93\n",
      "pixel: 94\n",
      "pixel: 95\n",
      "pixel: 96\n",
      "pixel: 97\n",
      "pixel: 98\n",
      "pixel: 99\n",
      "pixel: 100\n",
      "pixel: 101\n",
      "pixel: 102\n",
      "pixel: 103\n",
      "pixel: 104\n",
      "pixel: 105\n",
      "pixel: 106\n",
      "pixel: 107\n",
      "pixel: 108\n",
      "pixel: 109\n",
      "pixel: 110\n",
      "pixel: 111\n",
      "pixel: 112\n",
      "pixel: 113\n",
      "pixel: 114\n",
      "pixel: 115\n",
      "pixel: 116\n",
      "pixel: 117\n",
      "pixel: 118\n",
      "pixel: 119\n",
      "pixel: 120\n",
      "pixel: 121\n",
      "pixel: 122\n",
      "pixel: 123\n",
      "pixel: 124\n",
      "pixel: 125\n",
      "pixel: 126\n",
      "pixel: 127\n",
      "pixel: 128\n",
      "pixel: 129\n",
      "pixel: 130\n",
      "pixel: 131\n",
      "pixel: 132\n",
      "pixel: 133\n",
      "pixel: 134\n",
      "pixel: 135\n",
      "pixel: 136\n",
      "pixel: 137\n",
      "pixel: 138\n",
      "pixel: 139\n",
      "pixel: 140\n",
      "pixel: 141\n",
      "pixel: 142\n",
      "pixel: 143\n",
      "pixel: 144\n",
      "pixel: 145\n",
      "pixel: 146\n",
      "pixel: 147\n",
      "pixel: 148\n",
      "pixel: 149\n",
      "pixel: 150\n",
      "pixel: 151\n",
      "pixel: 152\n",
      "pixel: 153\n",
      "pixel: 154\n",
      "pixel: 155\n",
      "pixel: 156\n",
      "pixel: 157\n",
      "pixel: 158\n",
      "pixel: 159\n",
      "pixel: 160\n",
      "pixel: 161\n",
      "pixel: 162\n",
      "pixel: 163\n",
      "pixel: 164\n",
      "pixel: 165\n",
      "pixel: 166\n",
      "pixel: 167\n",
      "pixel: 168\n",
      "pixel: 169\n",
      "pixel: 170\n",
      "pixel: 171\n",
      "pixel: 172\n",
      "pixel: 173\n",
      "pixel: 174\n",
      "pixel: 175\n",
      "pixel: 176\n",
      "pixel: 177\n",
      "pixel: 178\n",
      "pixel: 179\n",
      "pixel: 180\n",
      "pixel: 181\n",
      "pixel: 182\n",
      "pixel: 183\n",
      "pixel: 184\n",
      "pixel: 185\n",
      "pixel: 186\n",
      "pixel: 187\n",
      "pixel: 188\n",
      "pixel: 189\n",
      "pixel: 190\n",
      "pixel: 191\n",
      "pixel: 192\n",
      "pixel: 193\n",
      "pixel: 194\n",
      "pixel: 195\n",
      "pixel: 196\n",
      "pixel: 197\n",
      "pixel: 198\n",
      "pixel: 199\n",
      "pixel: 200\n",
      "pixel: 201\n",
      "pixel: 202\n",
      "pixel: 203\n",
      "pixel: 204\n",
      "pixel: 205\n",
      "pixel: 206\n",
      "pixel: 207\n",
      "pixel: 208\n",
      "pixel: 209\n",
      "pixel: 210\n",
      "pixel: 211\n",
      "pixel: 212\n",
      "pixel: 213\n",
      "pixel: 214\n",
      "pixel: 215\n",
      "pixel: 216\n",
      "pixel: 217\n",
      "pixel: 218\n",
      "pixel: 219\n",
      "pixel: 220\n",
      "pixel: 221\n",
      "pixel: 222\n",
      "pixel: 223\n",
      "pixel: 224\n",
      "pixel: 225\n",
      "pixel: 226\n",
      "pixel: 227\n",
      "pixel: 228\n",
      "pixel: 229\n",
      "pixel: 230\n",
      "pixel: 231\n",
      "pixel: 232\n",
      "pixel: 233\n",
      "pixel: 234\n",
      "pixel: 235\n",
      "pixel: 236\n",
      "pixel: 237\n",
      "pixel: 238\n",
      "pixel: 239\n",
      "pixel: 240\n",
      "pixel: 241\n",
      "pixel: 242\n",
      "pixel: 243\n",
      "pixel: 244\n",
      "pixel: 245\n",
      "pixel: 246\n",
      "pixel: 247\n",
      "pixel: 248\n",
      "pixel: 249\n",
      "pixel: 250\n",
      "pixel: 251\n",
      "pixel: 252\n",
      "pixel: 253\n",
      "pixel: 254\n",
      "pixel: 255\n",
      "pixel: 256\n",
      "pixel: 257\n",
      "pixel: 258\n",
      "pixel: 259\n",
      "pixel: 260\n",
      "pixel: 261\n",
      "pixel: 262\n",
      "pixel: 263\n",
      "pixel: 264\n",
      "pixel: 265\n",
      "pixel: 266\n",
      "pixel: 267\n",
      "pixel: 268\n",
      "pixel: 269\n",
      "pixel: 270\n",
      "pixel: 271\n",
      "pixel: 272\n",
      "pixel: 273\n",
      "pixel: 274\n",
      "pixel: 275\n",
      "pixel: 276\n",
      "pixel: 277\n",
      "pixel: 278\n",
      "pixel: 279\n",
      "pixel: 280\n",
      "pixel: 281\n",
      "pixel: 282\n",
      "pixel: 283\n",
      "pixel: 284\n",
      "pixel: 285\n",
      "pixel: 286\n",
      "pixel: 287\n",
      "pixel: 288\n",
      "pixel: 289\n",
      "pixel: 290\n",
      "pixel: 291\n",
      "pixel: 292\n",
      "pixel: 293\n",
      "pixel: 294\n",
      "pixel: 295\n",
      "pixel: 296\n",
      "pixel: 297\n",
      "pixel: 298\n",
      "pixel: 299\n",
      "pixel: 300\n",
      "pixel: 301\n",
      "pixel: 302\n",
      "pixel: 303\n",
      "pixel: 304\n",
      "pixel: 305\n",
      "pixel: 306\n",
      "pixel: 307\n",
      "pixel: 308\n",
      "pixel: 309\n",
      "pixel: 310\n",
      "pixel: 311\n",
      "pixel: 312\n",
      "pixel: 313\n",
      "pixel: 314\n",
      "pixel: 315\n",
      "pixel: 316\n",
      "pixel: 317\n",
      "pixel: 318\n",
      "pixel: 319\n",
      "pixel: 320\n",
      "pixel: 321\n",
      "pixel: 322\n",
      "pixel: 323\n",
      "pixel: 324\n",
      "pixel: 325\n",
      "pixel: 326\n",
      "pixel: 327\n",
      "pixel: 328\n",
      "pixel: 329\n",
      "pixel: 330\n",
      "pixel: 331\n",
      "pixel: 332\n",
      "pixel: 333\n",
      "pixel: 334\n",
      "pixel: 335\n",
      "pixel: 336\n",
      "pixel: 337\n",
      "pixel: 338\n",
      "pixel: 339\n",
      "pixel: 340\n",
      "pixel: 341\n",
      "pixel: 342\n",
      "pixel: 343\n",
      "pixel: 344\n",
      "pixel: 345\n",
      "pixel: 346\n",
      "pixel: 347\n",
      "pixel: 348\n",
      "pixel: 349\n",
      "pixel: 350\n",
      "pixel: 351\n",
      "pixel: 352\n",
      "pixel: 353\n",
      "pixel: 354\n",
      "pixel: 355\n",
      "pixel: 356\n",
      "pixel: 357\n",
      "pixel: 358\n",
      "pixel: 359\n",
      "pixel: 360\n",
      "pixel: 361\n",
      "pixel: 362\n",
      "pixel: 363\n",
      "pixel: 364\n",
      "pixel: 365\n",
      "pixel: 366\n",
      "pixel: 367\n",
      "pixel: 368\n",
      "pixel: 369\n",
      "pixel: 370\n",
      "pixel: 371\n",
      "pixel: 372\n",
      "pixel: 373\n",
      "pixel: 374\n",
      "pixel: 375\n",
      "pixel: 376\n",
      "pixel: 377\n",
      "pixel: 378\n",
      "pixel: 379\n",
      "pixel: 380\n",
      "pixel: 381\n",
      "pixel: 382\n",
      "pixel: 383\n",
      "pixel: 384\n",
      "pixel: 385\n",
      "pixel: 386\n",
      "pixel: 387\n",
      "pixel: 388\n",
      "pixel: 389\n",
      "pixel: 390\n",
      "pixel: 391\n",
      "pixel: 392\n",
      "pixel: 393\n",
      "pixel: 394\n",
      "pixel: 395\n",
      "pixel: 396\n",
      "pixel: 397\n",
      "pixel: 398\n",
      "pixel: 399\n",
      "pixel: 400\n",
      "pixel: 401\n",
      "pixel: 402\n",
      "pixel: 403\n",
      "pixel: 404\n",
      "pixel: 405\n",
      "pixel: 406\n",
      "pixel: 407\n",
      "pixel: 408\n",
      "pixel: 409\n",
      "pixel: 410\n",
      "pixel: 411\n",
      "pixel: 412\n",
      "pixel: 413\n",
      "pixel: 414\n",
      "pixel: 415\n",
      "pixel: 416\n",
      "pixel: 417\n",
      "pixel: 418\n",
      "pixel: 419\n",
      "pixel: 420\n",
      "pixel: 421\n",
      "pixel: 422\n",
      "pixel: 423\n",
      "pixel: 424\n",
      "pixel: 425\n",
      "pixel: 426\n",
      "pixel: 427\n",
      "pixel: 428\n",
      "pixel: 429\n",
      "pixel: 430\n",
      "pixel: 431\n",
      "pixel: 432\n",
      "pixel: 433\n",
      "pixel: 434\n",
      "pixel: 435\n",
      "pixel: 436\n",
      "pixel: 437\n",
      "pixel: 438\n",
      "pixel: 439\n",
      "pixel: 440\n",
      "pixel: 441\n",
      "pixel: 442\n",
      "pixel: 443\n",
      "pixel: 444\n",
      "pixel: 445\n",
      "pixel: 446\n",
      "pixel: 447\n",
      "pixel: 448\n",
      "pixel: 449\n",
      "pixel: 450\n",
      "pixel: 451\n",
      "pixel: 452\n",
      "pixel: 453\n",
      "pixel: 454\n",
      "pixel: 455\n",
      "pixel: 456\n",
      "pixel: 457\n",
      "pixel: 458\n",
      "pixel: 459\n",
      "pixel: 460\n",
      "pixel: 461\n",
      "pixel: 462\n",
      "pixel: 463\n",
      "pixel: 464\n",
      "pixel: 465\n",
      "pixel: 466\n",
      "pixel: 467\n",
      "pixel: 468\n",
      "pixel: 469\n",
      "pixel: 470\n",
      "pixel: 471\n",
      "pixel: 472\n",
      "pixel: 473\n",
      "pixel: 474\n",
      "pixel: 475\n",
      "pixel: 476\n",
      "pixel: 477\n",
      "pixel: 478\n",
      "pixel: 479\n",
      "pixel: 480\n",
      "pixel: 481\n",
      "pixel: 482\n",
      "pixel: 483\n",
      "pixel: 484\n",
      "pixel: 485\n",
      "pixel: 486\n",
      "pixel: 487\n",
      "pixel: 488\n",
      "pixel: 489\n",
      "pixel: 490\n",
      "pixel: 491\n",
      "pixel: 492\n",
      "pixel: 493\n",
      "pixel: 494\n",
      "pixel: 495\n",
      "pixel: 496\n",
      "pixel: 497\n",
      "pixel: 498\n",
      "pixel: 499\n",
      "pixel: 500\n",
      "pixel: 501\n",
      "pixel: 502\n",
      "pixel: 503\n",
      "pixel: 504\n",
      "pixel: 505\n",
      "pixel: 506\n",
      "pixel: 507\n",
      "pixel: 508\n",
      "pixel: 509\n",
      "pixel: 510\n",
      "pixel: 511\n",
      "pixel: 512\n",
      "pixel: 513\n",
      "pixel: 514\n",
      "pixel: 515\n",
      "pixel: 516\n",
      "pixel: 517\n",
      "pixel: 518\n",
      "pixel: 519\n",
      "pixel: 520\n",
      "pixel: 521\n",
      "pixel: 522\n",
      "pixel: 523\n",
      "pixel: 524\n",
      "pixel: 525\n",
      "pixel: 526\n",
      "pixel: 527\n",
      "pixel: 528\n",
      "pixel: 529\n",
      "pixel: 530\n",
      "pixel: 531\n",
      "pixel: 532\n",
      "pixel: 533\n",
      "pixel: 534\n",
      "pixel: 535\n",
      "pixel: 536\n",
      "pixel: 537\n",
      "pixel: 538\n",
      "pixel: 539\n",
      "pixel: 540\n",
      "pixel: 541\n",
      "pixel: 542\n",
      "pixel: 543\n",
      "pixel: 544\n",
      "pixel: 545\n",
      "pixel: 546\n",
      "pixel: 547\n",
      "pixel: 548\n",
      "pixel: 549\n",
      "pixel: 550\n",
      "pixel: 551\n",
      "pixel: 552\n",
      "pixel: 553\n",
      "pixel: 554\n",
      "pixel: 555\n",
      "pixel: 556\n",
      "pixel: 557\n",
      "pixel: 558\n",
      "pixel: 559\n",
      "pixel: 560\n",
      "pixel: 561\n",
      "pixel: 562\n",
      "pixel: 563\n",
      "pixel: 564\n",
      "pixel: 565\n",
      "pixel: 566\n",
      "pixel: 567\n",
      "pixel: 568\n",
      "pixel: 569\n",
      "pixel: 570\n",
      "pixel: 571\n",
      "pixel: 572\n",
      "pixel: 573\n",
      "pixel: 574\n",
      "pixel: 575\n",
      "pixel: 576\n",
      "pixel: 577\n",
      "pixel: 578\n",
      "pixel: 579\n",
      "pixel: 580\n",
      "pixel: 581\n",
      "pixel: 582\n",
      "pixel: 583\n",
      "pixel: 584\n",
      "pixel: 585\n",
      "pixel: 586\n",
      "pixel: 587\n",
      "pixel: 588\n",
      "pixel: 589\n",
      "pixel: 590\n",
      "pixel: 591\n",
      "pixel: 592\n",
      "pixel: 593\n",
      "pixel: 594\n",
      "pixel: 595\n",
      "pixel: 596\n",
      "pixel: 597\n",
      "pixel: 598\n",
      "pixel: 599\n",
      "pixel: 600\n",
      "pixel: 601\n",
      "pixel: 602\n",
      "pixel: 603\n",
      "pixel: 604\n",
      "pixel: 605\n",
      "pixel: 606\n",
      "pixel: 607\n",
      "pixel: 608\n",
      "pixel: 609\n",
      "pixel: 610\n",
      "pixel: 611\n",
      "pixel: 612\n",
      "pixel: 613\n",
      "pixel: 614\n",
      "pixel: 615\n",
      "pixel: 616\n",
      "pixel: 617\n",
      "pixel: 618\n",
      "pixel: 619\n",
      "pixel: 620\n",
      "pixel: 621\n",
      "pixel: 622\n",
      "pixel: 623\n",
      "pixel: 624\n",
      "pixel: 625\n",
      "pixel: 626\n",
      "pixel: 627\n",
      "pixel: 628\n",
      "pixel: 629\n",
      "pixel: 630\n",
      "pixel: 631\n",
      "pixel: 632\n",
      "pixel: 633\n",
      "pixel: 634\n",
      "pixel: 635\n",
      "pixel: 636\n",
      "pixel: 637\n",
      "pixel: 638\n",
      "pixel: 639\n",
      "pixel: 640\n",
      "pixel: 641\n",
      "pixel: 642\n",
      "pixel: 643\n",
      "pixel: 644\n",
      "pixel: 645\n",
      "pixel: 646\n",
      "pixel: 647\n",
      "pixel: 648\n",
      "pixel: 649\n",
      "pixel: 650\n",
      "pixel: 651\n",
      "pixel: 652\n",
      "pixel: 653\n",
      "pixel: 654\n",
      "pixel: 655\n",
      "pixel: 656\n",
      "pixel: 657\n",
      "pixel: 658\n",
      "pixel: 659\n",
      "pixel: 660\n",
      "pixel: 661\n",
      "pixel: 662\n",
      "pixel: 663\n",
      "pixel: 664\n",
      "pixel: 665\n",
      "pixel: 666\n",
      "pixel: 667\n",
      "pixel: 668\n",
      "pixel: 669\n",
      "pixel: 670\n",
      "pixel: 671\n",
      "pixel: 672\n",
      "pixel: 673\n",
      "pixel: 674\n",
      "pixel: 675\n",
      "pixel: 676\n",
      "pixel: 677\n",
      "pixel: 678\n",
      "pixel: 679\n",
      "pixel: 680\n",
      "pixel: 681\n",
      "pixel: 682\n",
      "pixel: 683\n",
      "pixel: 684\n",
      "pixel: 685\n",
      "pixel: 686\n",
      "pixel: 687\n",
      "pixel: 688\n",
      "pixel: 689\n",
      "pixel: 690\n",
      "pixel: 691\n",
      "pixel: 692\n",
      "pixel: 693\n",
      "pixel: 694\n",
      "pixel: 695\n",
      "pixel: 696\n",
      "pixel: 697\n",
      "pixel: 698\n",
      "pixel: 699\n",
      "pixel: 700\n",
      "pixel: 701\n",
      "pixel: 702\n",
      "pixel: 703\n",
      "pixel: 704\n",
      "pixel: 705\n",
      "pixel: 706\n",
      "pixel: 707\n",
      "pixel: 708\n",
      "pixel: 709\n",
      "pixel: 710\n",
      "pixel: 711\n",
      "pixel: 712\n",
      "pixel: 713\n",
      "pixel: 714\n",
      "pixel: 715\n",
      "pixel: 716\n",
      "pixel: 717\n",
      "pixel: 718\n",
      "pixel: 719\n",
      "pixel: 720\n",
      "pixel: 721\n",
      "pixel: 722\n",
      "pixel: 723\n",
      "pixel: 724\n",
      "pixel: 725\n",
      "pixel: 726\n",
      "pixel: 727\n",
      "pixel: 728\n",
      "pixel: 729\n",
      "pixel: 730\n",
      "pixel: 731\n",
      "pixel: 732\n",
      "pixel: 733\n",
      "pixel: 734\n",
      "pixel: 735\n",
      "pixel: 736\n",
      "pixel: 737\n",
      "pixel: 738\n",
      "pixel: 739\n",
      "pixel: 740\n",
      "pixel: 741\n",
      "pixel: 742\n",
      "pixel: 743\n",
      "pixel: 744\n",
      "pixel: 745\n",
      "pixel: 746\n",
      "pixel: 747\n",
      "pixel: 748\n",
      "pixel: 749\n",
      "pixel: 750\n",
      "pixel: 751\n",
      "pixel: 752\n",
      "pixel: 753\n",
      "pixel: 754\n",
      "pixel: 755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel: 756\n",
      "pixel: 757\n",
      "pixel: 758\n",
      "pixel: 759\n",
      "pixel: 760\n",
      "pixel: 761\n",
      "pixel: 762\n",
      "pixel: 763\n",
      "pixel: 764\n",
      "pixel: 765\n",
      "pixel: 766\n",
      "pixel: 767\n",
      "pixel: 768\n",
      "pixel: 769\n",
      "pixel: 770\n",
      "pixel: 771\n",
      "pixel: 772\n",
      "pixel: 773\n",
      "pixel: 774\n",
      "pixel: 775\n",
      "pixel: 776\n",
      "pixel: 777\n",
      "pixel: 778\n",
      "pixel: 779\n",
      "pixel: 780\n",
      "pixel: 781\n",
      "pixel: 782\n",
      "pixel: 783\n",
      "pixel: 784\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "l_res=Sobol(X,2,monte_num=10000,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f31c27ad7f0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_res_t=torch.tensor(l_res).view(28,28)\n",
    "plt.imshow(l_res_t.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel: 1\n",
      "pixel: 2\n",
      "pixel: 3\n",
      "pixel: 4\n",
      "pixel: 5\n",
      "pixel: 6\n",
      "pixel: 7\n",
      "pixel: 8\n",
      "pixel: 9\n",
      "pixel: 10\n",
      "pixel: 11\n",
      "pixel: 12\n",
      "pixel: 13\n",
      "pixel: 14\n",
      "pixel: 15\n",
      "pixel: 16\n",
      "pixel: 17\n",
      "pixel: 18\n",
      "pixel: 19\n",
      "pixel: 20\n",
      "pixel: 21\n",
      "pixel: 22\n",
      "pixel: 23\n",
      "pixel: 24\n",
      "pixel: 25\n",
      "pixel: 26\n",
      "pixel: 27\n",
      "pixel: 28\n",
      "pixel: 29\n",
      "pixel: 30\n",
      "pixel: 31\n",
      "pixel: 32\n",
      "pixel: 33\n",
      "pixel: 34\n",
      "pixel: 35\n",
      "pixel: 36\n",
      "pixel: 37\n",
      "pixel: 38\n",
      "pixel: 39\n",
      "pixel: 40\n",
      "pixel: 41\n",
      "pixel: 42\n",
      "pixel: 43\n",
      "pixel: 44\n",
      "pixel: 45\n",
      "pixel: 46\n",
      "pixel: 47\n",
      "pixel: 48\n",
      "pixel: 49\n",
      "pixel: 50\n",
      "pixel: 51\n",
      "pixel: 52\n",
      "pixel: 53\n",
      "pixel: 54\n",
      "pixel: 55\n",
      "pixel: 56\n",
      "pixel: 57\n",
      "pixel: 58\n",
      "pixel: 59\n",
      "pixel: 60\n",
      "pixel: 61\n",
      "pixel: 62\n",
      "pixel: 63\n",
      "pixel: 64\n",
      "pixel: 65\n",
      "pixel: 66\n",
      "pixel: 67\n",
      "pixel: 68\n",
      "pixel: 69\n",
      "pixel: 70\n",
      "pixel: 71\n",
      "pixel: 72\n",
      "pixel: 73\n",
      "pixel: 74\n",
      "pixel: 75\n",
      "pixel: 76\n",
      "pixel: 77\n",
      "pixel: 78\n",
      "pixel: 79\n",
      "pixel: 80\n",
      "pixel: 81\n",
      "pixel: 82\n",
      "pixel: 83\n",
      "pixel: 84\n",
      "pixel: 85\n",
      "pixel: 86\n",
      "pixel: 87\n",
      "pixel: 88\n",
      "pixel: 89\n",
      "pixel: 90\n",
      "pixel: 91\n",
      "pixel: 92\n",
      "pixel: 93\n",
      "pixel: 94\n",
      "pixel: 95\n",
      "pixel: 96\n",
      "pixel: 97\n",
      "pixel: 98\n",
      "pixel: 99\n",
      "pixel: 100\n",
      "pixel: 101\n",
      "pixel: 102\n",
      "pixel: 103\n",
      "pixel: 104\n",
      "pixel: 105\n",
      "pixel: 106\n",
      "pixel: 107\n",
      "pixel: 108\n",
      "pixel: 109\n",
      "pixel: 110\n",
      "pixel: 111\n",
      "pixel: 112\n",
      "pixel: 113\n",
      "pixel: 114\n",
      "pixel: 115\n",
      "pixel: 116\n",
      "pixel: 117\n",
      "pixel: 118\n",
      "pixel: 119\n",
      "pixel: 120\n",
      "pixel: 121\n",
      "pixel: 122\n",
      "pixel: 123\n",
      "pixel: 124\n",
      "pixel: 125\n",
      "pixel: 126\n",
      "pixel: 127\n",
      "pixel: 128\n",
      "pixel: 129\n",
      "pixel: 130\n",
      "pixel: 131\n",
      "pixel: 132\n",
      "pixel: 133\n",
      "pixel: 134\n",
      "pixel: 135\n",
      "pixel: 136\n",
      "pixel: 137\n",
      "pixel: 138\n",
      "pixel: 139\n",
      "pixel: 140\n",
      "pixel: 141\n",
      "pixel: 142\n",
      "pixel: 143\n",
      "pixel: 144\n",
      "pixel: 145\n",
      "pixel: 146\n",
      "pixel: 147\n",
      "pixel: 148\n",
      "pixel: 149\n",
      "pixel: 150\n",
      "pixel: 151\n",
      "pixel: 152\n",
      "pixel: 153\n",
      "pixel: 154\n",
      "pixel: 155\n",
      "pixel: 156\n",
      "pixel: 157\n",
      "pixel: 158\n",
      "pixel: 159\n",
      "pixel: 160\n",
      "pixel: 161\n",
      "pixel: 162\n",
      "pixel: 163\n",
      "pixel: 164\n",
      "pixel: 165\n",
      "pixel: 166\n",
      "pixel: 167\n",
      "pixel: 168\n",
      "pixel: 169\n",
      "pixel: 170\n",
      "pixel: 171\n",
      "pixel: 172\n",
      "pixel: 173\n",
      "pixel: 174\n",
      "pixel: 175\n",
      "pixel: 176\n",
      "pixel: 177\n",
      "pixel: 178\n",
      "pixel: 179\n",
      "pixel: 180\n",
      "pixel: 181\n",
      "pixel: 182\n",
      "pixel: 183\n",
      "pixel: 184\n",
      "pixel: 185\n",
      "pixel: 186\n",
      "pixel: 187\n",
      "pixel: 188\n",
      "pixel: 189\n",
      "pixel: 190\n",
      "pixel: 191\n",
      "pixel: 192\n",
      "pixel: 193\n",
      "pixel: 194\n",
      "pixel: 195\n",
      "pixel: 196\n",
      "pixel: 197\n",
      "pixel: 198\n",
      "pixel: 199\n",
      "pixel: 200\n",
      "pixel: 201\n",
      "pixel: 202\n",
      "pixel: 203\n",
      "pixel: 204\n",
      "pixel: 205\n",
      "pixel: 206\n",
      "pixel: 207\n",
      "pixel: 208\n",
      "pixel: 209\n",
      "pixel: 210\n",
      "pixel: 211\n",
      "pixel: 212\n",
      "pixel: 213\n",
      "pixel: 214\n",
      "pixel: 215\n",
      "pixel: 216\n",
      "pixel: 217\n",
      "pixel: 218\n",
      "pixel: 219\n",
      "pixel: 220\n",
      "pixel: 221\n",
      "pixel: 222\n",
      "pixel: 223\n",
      "pixel: 224\n",
      "pixel: 225\n",
      "pixel: 226\n",
      "pixel: 227\n",
      "pixel: 228\n",
      "pixel: 229\n",
      "pixel: 230\n",
      "pixel: 231\n",
      "pixel: 232\n",
      "pixel: 233\n",
      "pixel: 234\n",
      "pixel: 235\n",
      "pixel: 236\n",
      "pixel: 237\n",
      "pixel: 238\n",
      "pixel: 239\n",
      "pixel: 240\n",
      "pixel: 241\n",
      "pixel: 242\n",
      "pixel: 243\n",
      "pixel: 244\n",
      "pixel: 245\n",
      "pixel: 246\n",
      "pixel: 247\n",
      "pixel: 248\n",
      "pixel: 249\n",
      "pixel: 250\n",
      "pixel: 251\n",
      "pixel: 252\n",
      "pixel: 253\n",
      "pixel: 254\n",
      "pixel: 255\n",
      "pixel: 256\n",
      "pixel: 257\n",
      "pixel: 258\n",
      "pixel: 259\n",
      "pixel: 260\n",
      "pixel: 261\n",
      "pixel: 262\n",
      "pixel: 263\n",
      "pixel: 264\n",
      "pixel: 265\n",
      "pixel: 266\n",
      "pixel: 267\n",
      "pixel: 268\n",
      "pixel: 269\n",
      "pixel: 270\n",
      "pixel: 271\n",
      "pixel: 272\n",
      "pixel: 273\n",
      "pixel: 274\n",
      "pixel: 275\n",
      "pixel: 276\n",
      "pixel: 277\n",
      "pixel: 278\n",
      "pixel: 279\n",
      "pixel: 280\n",
      "pixel: 281\n",
      "pixel: 282\n",
      "pixel: 283\n",
      "pixel: 284\n",
      "pixel: 285\n",
      "pixel: 286\n",
      "pixel: 287\n",
      "pixel: 288\n",
      "pixel: 289\n",
      "pixel: 290\n",
      "pixel: 291\n",
      "pixel: 292\n",
      "pixel: 293\n",
      "pixel: 294\n",
      "pixel: 295\n",
      "pixel: 296\n",
      "pixel: 297\n",
      "pixel: 298\n",
      "pixel: 299\n",
      "pixel: 300\n",
      "pixel: 301\n",
      "pixel: 302\n",
      "pixel: 303\n",
      "pixel: 304\n",
      "pixel: 305\n",
      "pixel: 306\n",
      "pixel: 307\n",
      "pixel: 308\n",
      "pixel: 309\n",
      "pixel: 310\n",
      "pixel: 311\n",
      "pixel: 312\n",
      "pixel: 313\n",
      "pixel: 314\n",
      "pixel: 315\n",
      "pixel: 316\n",
      "pixel: 317\n",
      "pixel: 318\n",
      "pixel: 319\n",
      "pixel: 320\n",
      "pixel: 321\n",
      "pixel: 322\n",
      "pixel: 323\n",
      "pixel: 324\n",
      "pixel: 325\n",
      "pixel: 326\n",
      "pixel: 327\n",
      "pixel: 328\n",
      "pixel: 329\n",
      "pixel: 330\n",
      "pixel: 331\n",
      "pixel: 332\n",
      "pixel: 333\n",
      "pixel: 334\n",
      "pixel: 335\n",
      "pixel: 336\n",
      "pixel: 337\n",
      "pixel: 338\n",
      "pixel: 339\n",
      "pixel: 340\n",
      "pixel: 341\n",
      "pixel: 342\n",
      "pixel: 343\n",
      "pixel: 344\n",
      "pixel: 345\n",
      "pixel: 346\n",
      "pixel: 347\n",
      "pixel: 348\n",
      "pixel: 349\n",
      "pixel: 350\n",
      "pixel: 351\n",
      "pixel: 352\n",
      "pixel: 353\n",
      "pixel: 354\n",
      "pixel: 355\n",
      "pixel: 356\n",
      "pixel: 357\n",
      "pixel: 358\n",
      "pixel: 359\n",
      "pixel: 360\n",
      "pixel: 361\n",
      "pixel: 362\n",
      "pixel: 363\n",
      "pixel: 364\n",
      "pixel: 365\n",
      "pixel: 366\n",
      "pixel: 367\n",
      "pixel: 368\n",
      "pixel: 369\n",
      "pixel: 370\n",
      "pixel: 371\n",
      "pixel: 372\n",
      "pixel: 373\n",
      "pixel: 374\n",
      "pixel: 375\n",
      "pixel: 376\n",
      "pixel: 377\n",
      "pixel: 378\n",
      "pixel: 379\n",
      "pixel: 380\n",
      "pixel: 381\n",
      "pixel: 382\n",
      "pixel: 383\n",
      "pixel: 384\n",
      "pixel: 385\n",
      "pixel: 386\n",
      "pixel: 387\n",
      "pixel: 388\n",
      "pixel: 389\n",
      "pixel: 390\n",
      "pixel: 391\n",
      "pixel: 392\n",
      "pixel: 393\n",
      "pixel: 394\n",
      "pixel: 395\n",
      "pixel: 396\n",
      "pixel: 397\n",
      "pixel: 398\n",
      "pixel: 399\n",
      "pixel: 400\n",
      "pixel: 401\n",
      "pixel: 402\n",
      "pixel: 403\n",
      "pixel: 404\n",
      "pixel: 405\n",
      "pixel: 406\n",
      "pixel: 407\n",
      "pixel: 408\n",
      "pixel: 409\n",
      "pixel: 410\n",
      "pixel: 411\n",
      "pixel: 412\n",
      "pixel: 413\n",
      "pixel: 414\n",
      "pixel: 415\n",
      "pixel: 416\n",
      "pixel: 417\n",
      "pixel: 418\n",
      "pixel: 419\n",
      "pixel: 420\n",
      "pixel: 421\n",
      "pixel: 422\n",
      "pixel: 423\n",
      "pixel: 424\n",
      "pixel: 425\n",
      "pixel: 426\n",
      "pixel: 427\n",
      "pixel: 428\n",
      "pixel: 429\n",
      "pixel: 430\n",
      "pixel: 431\n",
      "pixel: 432\n",
      "pixel: 433\n",
      "pixel: 434\n",
      "pixel: 435\n",
      "pixel: 436\n",
      "pixel: 437\n",
      "pixel: 438\n",
      "pixel: 439\n",
      "pixel: 440\n",
      "pixel: 441\n",
      "pixel: 442\n",
      "pixel: 443\n",
      "pixel: 444\n",
      "pixel: 445\n",
      "pixel: 446\n",
      "pixel: 447\n",
      "pixel: 448\n",
      "pixel: 449\n",
      "pixel: 450\n",
      "pixel: 451\n",
      "pixel: 452\n",
      "pixel: 453\n",
      "pixel: 454\n",
      "pixel: 455\n",
      "pixel: 456\n",
      "pixel: 457\n",
      "pixel: 458\n",
      "pixel: 459\n",
      "pixel: 460\n",
      "pixel: 461\n",
      "pixel: 462\n",
      "pixel: 463\n",
      "pixel: 464\n",
      "pixel: 465\n",
      "pixel: 466\n",
      "pixel: 467\n",
      "pixel: 468\n",
      "pixel: 469\n",
      "pixel: 470\n",
      "pixel: 471\n",
      "pixel: 472\n",
      "pixel: 473\n",
      "pixel: 474\n",
      "pixel: 475\n",
      "pixel: 476\n",
      "pixel: 477\n",
      "pixel: 478\n",
      "pixel: 479\n",
      "pixel: 480\n",
      "pixel: 481\n",
      "pixel: 482\n",
      "pixel: 483\n",
      "pixel: 484\n",
      "pixel: 485\n",
      "pixel: 486\n",
      "pixel: 487\n",
      "pixel: 488\n",
      "pixel: 489\n",
      "pixel: 490\n",
      "pixel: 491\n",
      "pixel: 492\n",
      "pixel: 493\n",
      "pixel: 494\n",
      "pixel: 495\n",
      "pixel: 496\n",
      "pixel: 497\n",
      "pixel: 498\n",
      "pixel: 499\n",
      "pixel: 500\n",
      "pixel: 501\n",
      "pixel: 502\n",
      "pixel: 503\n",
      "pixel: 504\n",
      "pixel: 505\n",
      "pixel: 506\n",
      "pixel: 507\n",
      "pixel: 508\n",
      "pixel: 509\n",
      "pixel: 510\n",
      "pixel: 511\n",
      "pixel: 512\n",
      "pixel: 513\n",
      "pixel: 514\n",
      "pixel: 515\n",
      "pixel: 516\n",
      "pixel: 517\n",
      "pixel: 518\n",
      "pixel: 519\n",
      "pixel: 520\n",
      "pixel: 521\n",
      "pixel: 522\n",
      "pixel: 523\n",
      "pixel: 524\n",
      "pixel: 525\n",
      "pixel: 526\n",
      "pixel: 527\n",
      "pixel: 528\n",
      "pixel: 529\n",
      "pixel: 530\n",
      "pixel: 531\n",
      "pixel: 532\n",
      "pixel: 533\n",
      "pixel: 534\n",
      "pixel: 535\n",
      "pixel: 536\n",
      "pixel: 537\n",
      "pixel: 538\n",
      "pixel: 539\n",
      "pixel: 540\n",
      "pixel: 541\n",
      "pixel: 542\n",
      "pixel: 543\n",
      "pixel: 544\n",
      "pixel: 545\n",
      "pixel: 546\n",
      "pixel: 547\n",
      "pixel: 548\n",
      "pixel: 549\n",
      "pixel: 550\n",
      "pixel: 551\n",
      "pixel: 552\n",
      "pixel: 553\n",
      "pixel: 554\n",
      "pixel: 555\n",
      "pixel: 556\n",
      "pixel: 557\n",
      "pixel: 558\n",
      "pixel: 559\n",
      "pixel: 560\n",
      "pixel: 561\n",
      "pixel: 562\n",
      "pixel: 563\n",
      "pixel: 564\n",
      "pixel: 565\n",
      "pixel: 566\n",
      "pixel: 567\n",
      "pixel: 568\n",
      "pixel: 569\n",
      "pixel: 570\n",
      "pixel: 571\n",
      "pixel: 572\n",
      "pixel: 573\n",
      "pixel: 574\n",
      "pixel: 575\n",
      "pixel: 576\n",
      "pixel: 577\n",
      "pixel: 578\n",
      "pixel: 579\n",
      "pixel: 580\n",
      "pixel: 581\n",
      "pixel: 582\n",
      "pixel: 583\n",
      "pixel: 584\n",
      "pixel: 585\n",
      "pixel: 586\n",
      "pixel: 587\n",
      "pixel: 588\n",
      "pixel: 589\n",
      "pixel: 590\n",
      "pixel: 591\n",
      "pixel: 592\n",
      "pixel: 593\n",
      "pixel: 594\n",
      "pixel: 595\n",
      "pixel: 596\n",
      "pixel: 597\n",
      "pixel: 598\n",
      "pixel: 599\n",
      "pixel: 600\n",
      "pixel: 601\n",
      "pixel: 602\n",
      "pixel: 603\n",
      "pixel: 604\n",
      "pixel: 605\n",
      "pixel: 606\n",
      "pixel: 607\n",
      "pixel: 608\n",
      "pixel: 609\n",
      "pixel: 610\n",
      "pixel: 611\n",
      "pixel: 612\n",
      "pixel: 613\n",
      "pixel: 614\n",
      "pixel: 615\n",
      "pixel: 616\n",
      "pixel: 617\n",
      "pixel: 618\n",
      "pixel: 619\n",
      "pixel: 620\n",
      "pixel: 621\n",
      "pixel: 622\n",
      "pixel: 623\n",
      "pixel: 624\n",
      "pixel: 625\n",
      "pixel: 626\n",
      "pixel: 627\n",
      "pixel: 628\n",
      "pixel: 629\n",
      "pixel: 630\n",
      "pixel: 631\n",
      "pixel: 632\n",
      "pixel: 633\n",
      "pixel: 634\n",
      "pixel: 635\n",
      "pixel: 636\n",
      "pixel: 637\n",
      "pixel: 638\n",
      "pixel: 639\n",
      "pixel: 640\n",
      "pixel: 641\n",
      "pixel: 642\n",
      "pixel: 643\n",
      "pixel: 644\n",
      "pixel: 645\n",
      "pixel: 646\n",
      "pixel: 647\n",
      "pixel: 648\n",
      "pixel: 649\n",
      "pixel: 650\n",
      "pixel: 651\n",
      "pixel: 652\n",
      "pixel: 653\n",
      "pixel: 654\n",
      "pixel: 655\n",
      "pixel: 656\n",
      "pixel: 657\n",
      "pixel: 658\n",
      "pixel: 659\n",
      "pixel: 660\n",
      "pixel: 661\n",
      "pixel: 662\n",
      "pixel: 663\n",
      "pixel: 664\n",
      "pixel: 665\n",
      "pixel: 666\n",
      "pixel: 667\n",
      "pixel: 668\n",
      "pixel: 669\n",
      "pixel: 670\n",
      "pixel: 671\n",
      "pixel: 672\n",
      "pixel: 673\n",
      "pixel: 674\n",
      "pixel: 675\n",
      "pixel: 676\n",
      "pixel: 677\n",
      "pixel: 678\n",
      "pixel: 679\n",
      "pixel: 680\n",
      "pixel: 681\n",
      "pixel: 682\n",
      "pixel: 683\n",
      "pixel: 684\n",
      "pixel: 685\n",
      "pixel: 686\n",
      "pixel: 687\n",
      "pixel: 688\n",
      "pixel: 689\n",
      "pixel: 690\n",
      "pixel: 691\n",
      "pixel: 692\n",
      "pixel: 693\n",
      "pixel: 694\n",
      "pixel: 695\n",
      "pixel: 696\n",
      "pixel: 697\n",
      "pixel: 698\n",
      "pixel: 699\n",
      "pixel: 700\n",
      "pixel: 701\n",
      "pixel: 702\n",
      "pixel: 703\n",
      "pixel: 704\n",
      "pixel: 705\n",
      "pixel: 706\n",
      "pixel: 707\n",
      "pixel: 708\n",
      "pixel: 709\n",
      "pixel: 710\n",
      "pixel: 711\n",
      "pixel: 712\n",
      "pixel: 713\n",
      "pixel: 714\n",
      "pixel: 715\n",
      "pixel: 716\n",
      "pixel: 717\n",
      "pixel: 718\n",
      "pixel: 719\n",
      "pixel: 720\n",
      "pixel: 721\n",
      "pixel: 722\n",
      "pixel: 723\n",
      "pixel: 724\n",
      "pixel: 725\n",
      "pixel: 726\n",
      "pixel: 727\n",
      "pixel: 728\n",
      "pixel: 729\n",
      "pixel: 730\n",
      "pixel: 731\n",
      "pixel: 732\n",
      "pixel: 733\n",
      "pixel: 734\n",
      "pixel: 735\n",
      "pixel: 736\n",
      "pixel: 737\n",
      "pixel: 738\n",
      "pixel: 739\n",
      "pixel: 740\n",
      "pixel: 741\n",
      "pixel: 742\n",
      "pixel: 743\n",
      "pixel: 744\n",
      "pixel: 745\n",
      "pixel: 746\n",
      "pixel: 747\n",
      "pixel: 748\n",
      "pixel: 749\n",
      "pixel: 750\n",
      "pixel: 751\n",
      "pixel: 752\n",
      "pixel: 753\n",
      "pixel: 754\n",
      "pixel: 755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel: 756\n",
      "pixel: 757\n",
      "pixel: 758\n",
      "pixel: 759\n",
      "pixel: 760\n",
      "pixel: 761\n",
      "pixel: 762\n",
      "pixel: 763\n",
      "pixel: 764\n",
      "pixel: 765\n",
      "pixel: 766\n",
      "pixel: 767\n",
      "pixel: 768\n",
      "pixel: 769\n",
      "pixel: 770\n",
      "pixel: 771\n",
      "pixel: 772\n",
      "pixel: 773\n",
      "pixel: 774\n",
      "pixel: 775\n",
      "pixel: 776\n",
      "pixel: 777\n",
      "pixel: 778\n",
      "pixel: 779\n",
      "pixel: 780\n",
      "pixel: 781\n",
      "pixel: 782\n",
      "pixel: 783\n",
      "pixel: 784\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "l_res=Sobol(X,y,monte_num=10000,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f31c26ff6d8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPs0lEQVR4nO3dbYwd5XnG8eva9dqAsTF+WeMaCAQhNahqAa1IKqoKhIoIEjFpRRU+RG6Faj4EKZGQWpR+CFK/oKpJWlUVkhOsuFVKFDUgaEVTqEVKo6qUhRJscHiNg9devKaGYGKw9+Xuhz20C+w8c3xm5szZff4/aXV259k5c+94L885e8/M44gQgOVvqO0CAPQHYQcyQdiBTBB2IBOEHcjEin5ubKVXxRla3c9NAll5X7/UqTjpxcYqhd32DZL+StKwpG9HxD2p7z9Dq/VpX1dlk3nyov92/y/VPh0aTq87N3v69aBclX+zCp6MPYVjPb+Mtz0s6W8kfVbSZZJutX1Zr88HoFlV3rNfJemViHgtIk5J+p6kbfWUBaBuVcK+VdLBBV9PdJZ9iO0dtsdtj0/rZIXNAaiiStgXe1PysTciEbEzIsYiYmxEqypsDkAVVcI+IemCBV+fL+lwtXIANKVK2J+SdKnti22vlPQFSQ/XUxaAuvXceouIGdt3SPoXzbfedkXE87VVVrcmWyFlz12mbNtVahvk1lpL7am+GMDaK/XZI+IRSY/UVAuABnG6LJAJwg5kgrADmSDsQCYIO5AJwg5koq/Xs7eqyb7nAPZUl4Q299ty7vEX4MgOZIKwA5kg7EAmCDuQCcIOZIKwA5lYPq23Nu+i2nYbJ7X9ZdhC6hr75UM4sgOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kInl02dv85bJVXu2Vfv0TvyfHdX2y4qtv5Icn7zpE8nxLf/0euHY3C/eSa47d/x4crxUlf3S9rkTDeDIDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJpZPn71Mm9e7lynp2XpkZXr1menCsaGzzkquO3TeaHJcs+n9snHve8nxN268sHBsw94TyXWHnkrPAD68cUNyfOaNI8nxpFSPXqp8/kJ62830+CuF3fYBScclzUqaiYixKs8HoDl1HNmvjYg3a3geAA3iPTuQiaphD0mP2n7a9o7FvsH2DtvjtsendbLi5gD0qurL+Ksj4rDtUUmP2f5pRDyx8BsiYqeknZK01uuX3tUDwDJR6cgeEYc7j1OSHpR0VR1FAahfz2G3vdr2mg8+l3S9pH11FQagXlVexm+W9KDne4IrJP19RPywlqqa0GAfvbQPPn0qOT59fbpjOfJOen3953OFQ3Mn0r1sHTmaHI5TxT18SRp6Yyo5vum/it+5Da07J7nuieuvSI6v/smh5HglMZceb/J694aule857BHxmqTfqLEWAA2i9QZkgrADmSDsQCYIO5AJwg5kIp9LXBsUJZeBlhl5dDw5Prx2bXL81LVXFq/7+DPJdeNk+hTmmJlJjpdKtKhmj6bbfmf9bF1y/NDvXpQc3/zXh5PjSUvwVtFlOLIDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5CJ5dNnb3OK3aqXz5bc5trr0/3m4R/9d+/bLrtlcun6ze332f0vJ8c3jZ6dHP/l7326cGzt/rfS237hpeT4UsSRHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTCyfPntZP7dqPzjVCy+77XDJcw+deUZyfK7kds1Vetllt7kuf4L2rvse+rf0+QXv3f6bhWPvX52e7nnz8fOT4zMHJ5LjjUr9Lif+OTiyA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQieXTZy9TtR+c6qVX6dGrfFrkmEmPD7TUz97gNNqStOrt4n+Xqc+l75c/d9Pq5Pimz/VUUj16/F0uPbLb3mV7yva+BcvW237M9sudx3N72jqAvunmZfx3JN3wkWV3SdoTEZdK2tP5GsAAKw17RDwh6dhHFm+TtLvz+W5JN9dcF4Ca9foHus0RMSlJncfRom+0vcP2uO3xaaXfJwFoTuN/jY+InRExFhFjI1rV9OYAFOg17Edsb5GkzmPJZVkA2tZr2B+WtL3z+XZJD9VTDoCmlPbZbd8v6RpJG21PSPqapHskfd/2bZJel3RLLdW0ee/3Mqltl9VdYnhDunM5c6TCC6e292nDvfSUdf/8QuHY8KlPJdedXdXwW84Wzj8oDXtE3FowdF3NtQBoEKfLApkg7EAmCDuQCcIOZIKwA5nI5xLXJpVMe1x2q+g4WfF2zsknb7Fd2bK5k8WnZ5/92jvJdQ9sS7dD1/RU0QIttCQ5sgOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kInB6rM32BP2ivSPGjMzPT/3gT+7Kjl+ye70Japx6I30BprslZfc5rrqdNSNKrl8NxJ99uG3302u+/4FZ/dU0iDjyA5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCYGq8/eoCp99DIv/uG9yfFr//2PkuMrX3q1znJOT9Xrqlu8VfVQye2e595/v3hsTXpK5jX7R3qqaZBxZAcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBPZ9NmbdOfklcnxY7+a7tme98M6q+mzsj56qg9fsQef6qOX8cRkcvz8f0zfy7/07IS2p8peROmR3fYu21O29y1YdrftQ7af7Xzc2GyZAKrq5mX8dyTdsMjyb0bE5Z2PR+otC0DdSsMeEU9IOtaHWgA0qMof6O6w/VznZX7hxFi2d9getz0+reJ7ggFoVq9hv1fSJZIulzQp6etF3xgROyNiLCLGRpS+cAFAc3oKe0QciYjZiJiT9C1J6durAmhdT2G3vWXBl5+XtK/oewEMhtI+u+37JV0jaaPtCUlfk3SN7cslhaQDkm5vsMaBMLxxQ+HYvx4cTa47etPB9JP/ZS8VDYiy+863MA/5B5y43n3qlsuS647+6Ei1jbd4/kGR0rBHxK2LLL6vgVoANIjTZYFMEHYgE4QdyARhBzJB2IFMcIlrl2bf/J/CsemZLYVjknR819bk+Dkqac0NshZba2WGzjqrcGz0gReT68Z7vV8+25VBvMQVwPJA2IFMEHYgE4QdyARhBzJB2IFMEHYgE/TZa3DxH7+bHH91+9rk+Dl1FtNnXpH+FWpyquwys2+/XTg2vG5dct25EyfqLqd1HNmBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEffY6lFz7fMm3X0+OV+5EV7ktccWphdvsow9v2pQcn3vrrcKx2cTYcsWRHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTNBnr8Hsm8eS48Pnpad0rqzKPchbuH95t8qulZ89ejT9BGXTSTep7PyFlIb+TUqP7LYvsP247f22n7f95c7y9bYfs/1y5/HcRioEUItuXsbPSLozIj4l6TOSvmT7Mkl3SdoTEZdK2tP5GsCAKg17RExGxDOdz49L2i9pq6RtknZ3vm23pJubKhJAdaf1BzrbF0m6QtKTkjZHxKQ0/x+CpEXfmNreYXvc9vi0TlarFkDPug677bMl/UDSVyLinW7Xi4idETEWEWMjWtVLjQBq0FXYbY9oPujfjYgHOouP2N7SGd8iaaqZEgHUobT1ZtuS7pO0PyK+sWDoYUnbJd3TeXyokQqXgJg+lRyfmTiUHB9el76ZtNesSReworjFNHswve3SS1RL2lceTo/HzHTh2PA56Vtsx+xcclyn0vs9Trb4tnEAW5rd9NmvlvRFSXttP9tZ9lXNh/z7tm+T9LqkW5opEUAdSsMeET+WVHSGwHX1lgOgKZwuC2SCsAOZIOxAJgg7kAnCDmRiaV3iWuWWyS1aceH5yfFYfWb6CU6kb1U9N/Vm4djQxRemn3sk/Stw9DMbkuOjj6Zvkx1nFp81OXdgIr1uyfkLy1bZpblzs709bU9rAVhyCDuQCcIOZIKwA5kg7EAmCDuQCcIOZGJp9dmb7KWX9TYjcW11SV0zPz+YHPfIyuT4kX/4ZHI89hT38X/x6+le9aq16Wu+Z19LDuucn21Ojq/4j+cLxxrvoy/R8zJ67aOX4cgOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmllafvUkN9Ta7UdZvHt32056fO90F70LZ1MMl/epWu9lVeukVf+5BxJEdyARhBzJB2IFMEHYgE4QdyARhBzJB2IFMlIbd9gW2H7e93/bztr/cWX637UO2n+183Nh8uThtdvqjTET6Y7lahj93NyfVzEi6MyKesb1G0tO2H+uMfTMi/qK58gDUpZv52SclTXY+P257v6StTRcGoF6n9Z7d9kWSrpD0ZGfRHbafs73L9rkF6+ywPW57fFrpWyABaE7XYbd9tqQfSPpKRLwj6V5Jl0i6XPNH/q8vtl5E7IyIsYgYG1HxvF8AmtVV2G2PaD7o342IByQpIo5ExGxEzEn6lqSrmisTQFXd/DXeku6TtD8ivrFg+ZYF3/Z5SfvqLw9AXbr5a/zVkr4oaa/tZzvLvirpVtuXa/4qxgOSbm+kQlRT1iZahpdyYnHd/DX+x5IW+414pP5yADSFM+iATBB2IBOEHcgEYQcyQdiBTBB2IBODdSvp5drzHeSfa6nuU5w2juxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmTC0cc+q+2jkn6+YNFGSW/2rYDTM6i1DWpdErX1qs7aPhERmxYb6GvYP7ZxezwixlorIGFQaxvUuiRq61W/auNlPJAJwg5kou2w72x5+ymDWtug1iVRW6/6Ulur79kB9E/bR3YAfULYgUy0EnbbN9h+0fYrtu9qo4Yitg/Y3tuZhnq85Vp22Z6yvW/BsvW2H7P9cudx0Tn2WqptIKbxTkwz3uq+a3v6876/Z7c9LOklSb8jaULSU5JujYgX+lpIAdsHJI1FROsnYNj+bUnvSvrbiPi1zrI/l3QsIu7p/Ed5bkT8yYDUdrekd9uexrszW9GWhdOMS7pZ0h+oxX2XqOv31Yf91saR/SpJr0TEaxFxStL3JG1roY6BFxFPSDr2kcXbJO3ufL5b878sfVdQ20CIiMmIeKbz+XFJH0wz3uq+S9TVF22Efaukgwu+ntBgzfcekh61/bTtHW0Xs4jNETEpzf/ySBptuZ6PKp3Gu58+Ms34wOy7XqY/r6qNsC92Q7ZB6v9dHRFXSvqspC91Xq6iO11N490vi0wzPhB6nf68qjbCPiHpggVfny/pcAt1LCoiDncepyQ9qMGbivrIBzPodh6nWq7n/wzSNN6LTTOuAdh3bU5/3kbYn5J0qe2Lba+U9AVJD7dQx8fYXt35w4lsr5Z0vQZvKuqHJW3vfL5d0kMt1vIhgzKNd9E042p537U+/XlE9P1D0o2a/4v8q5L+tI0aCur6pKSfdD6eb7s2Sfdr/mXdtOZfEd0maYOkPZJe7jyuH6Da/k7SXknPaT5YW1qq7bc0/9bwOUnPdj5ubHvfJerqy37jdFkgE5xBB2SCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJv4XLOsdBLHOoDUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_res_t=torch.tensor(l_res).view(28,28)\n",
    "plt.imshow(l_res_t.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
